<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]>
<us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230225594A1-20230720.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20230704" date-publ="20230720">
<us-bibliographic-data-application lang="EN" country="US">
<publication-reference>
<document-id>
<country>US</country>
<doc-number>20230225594</doc-number>
<kind>A1</kind>
<date>20230720</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>18189303</doc-number>
<date>20230324</date>
</document-id>
</application-reference>
<us-application-series-code>18</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>00</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>34</main-group>
<subgroup>35</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>90</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>00188</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20160201</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>34</main-group>
<subgroup>35</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20160201</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>90</main-group>
<subgroup>361</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>1</main-group>
<subgroup>00193</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<invention-title id="d2e43">SYSTEMS AND METHODS FOR CONTROLLING AUTOFOCUS OPERATIONS</invention-title>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>16655779</doc-number>
<date>20191017</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>11627868</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>18189303</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>SYNAPTIVE MEDICAL INC.</orgname>
<address>
<city>Toronto</city>
<country>CA</country>
</address>
</addressbook>
<residence>
<country>CA</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="00" designation="us-only">
<addressbook>
<last-name>GANE</last-name>
<first-name>Luke William</first-name>
<address>
<city>Toronto</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
<inventor sequence="01" designation="us-only">
<addressbook>
<last-name>GALLOP</last-name>
<first-name>David Bruce</first-name>
<address>
<city>Toronto</city>
<country>CA</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>SYNAPTIVE MEDICAL INC.</orgname>
<role>03</role>
<address>
<city>Toronto</city>
<country>CA</country>
</address>
</addressbook>
</assignee>
</assignees>
</us-bibliographic-data-application>
<abstract id="abstract">
<p id="p-0001" num="0000">A method for performing auto-focus in a camera is disclosed. The method includes: receiving, from a tracking system for tracking a position of a medical instrument, a signal; determining, based on the received signal, that the medical instrument is removed from a field of view of the camera; in response to determining that a continuous auto-focus mode for the camera is enabled: retrieving, from a database, a first focus distance value representing a focus distance that was most recently set with intent for the camera; and automatically updating a focus distance of the camera to the first focus distance value.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="102.45mm" wi="158.75mm" file="US20230225594A1-20230720-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="175.34mm" wi="140.55mm" orientation="landscape" file="US20230225594A1-20230720-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="196.43mm" wi="165.95mm" orientation="landscape" file="US20230225594A1-20230720-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="240.11mm" wi="153.33mm" file="US20230225594A1-20230720-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="177.97mm" wi="161.04mm" orientation="landscape" file="US20230225594A1-20230720-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="165.35mm" wi="165.18mm" file="US20230225594A1-20230720-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="186.27mm" wi="88.31mm" file="US20230225594A1-20230720-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="183.56mm" wi="108.80mm" file="US20230225594A1-20230720-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="212.60mm" wi="110.83mm" file="US20230225594A1-20230720-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="226.57mm" wi="169.08mm" file="US20230225594A1-20230720-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?>
<heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The application claims priority to and the benefit of U.S. Utility patent application Ser. No. 16/655,779, entitled &#x201c;SYSTEMS AND METHODS FOR CONTROLLING AUTO-FOCUS OPERATIONS&#x201d;, filed on Oct. 17, 2019, the disclosure of which are incorporated herein by reference in their entirety.</p>
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?>
<?summary-of-invention description="Summary of Invention" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present disclosure relates to medical imaging and, in particular, to optical imaging systems suitable for use in image-guided medical procedures.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">Digital microscopes support advanced visualization during medical procedures. For example, digital surgical microscopes provide magnified views of anatomical structures during a surgery. Digital microscopes use optics and digital (e.g., CCD-based) cameras to capture images in real-time and output the images to displays for viewing by a surgeon, operator, etc.</p>
<p id="p-0005" num="0004">In image-guided medical applications, such as surgery or diagnostic imaging, accurate three-dimensional (3-D) visualization of patient anatomy and surgical tools is crucial. A medical navigation system is often used to support image-guided surgery. In an exemplary medical navigation system, an optical imaging system may be provided for generating 3-D views of a surgical site. A positioning system, such as a mechanical arm, may support the optical imaging system and facilitate maneuvering the optical imaging system to an appropriate position and orientation to maintain alignment with a viewing target.</p>
<p id="p-0006" num="0005">The optical imaging system may be adapted to perform auto-focus, which enables a camera of the optical imaging system to automatically focus on a defined viewing target, such as a tracked medical instrument. Continuous auto-focus maintains the viewing target constantly in focus. In particular, auto-focus operations dynamically focus the image on the viewing target, enabling an operator (e.g., surgeon) to control focus during a medical procedure without having to manually adjust the focus optics. By virtue of the auto-focus functionality, the operator may observe a surgical site of interest using the camera by moving the viewing target over, or in proximity of, the surgical site.</p>
<p id="p-0007" num="0006">Continuous auto-focus may be disrupted, for example, by actions of the operator or unintended changes to the viewing target. For example, if the viewing target leaves a field of view of the camera, an out-of-focus image may be produced by the camera. Even a momentary loss of focus of the optical imaging system may lead to serious consequences in a medical procedure. It is desirable to mitigate the disruptive effects resulting from loss of continuous auto-focus during a medical procedure.</p>
<?summary-of-invention description="Summary of Invention" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0008" num="0007">Reference will now be made, by way of example, to the accompanying drawings which show example embodiments of the present application and in which:</p>
<p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example medical navigation system to support image-guided surgery;</p>
<p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates components of an example medical navigation system;</p>
<p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example control and processing system which may be used in the example medical navigation system of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>;</p>
<p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows the use of an example optical imaging system during a medical procedure;</p>
<p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a perspective view of an example embodiment of an optical imaging system and a plurality of tracking markers;</p>
<p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating components of an example optical imaging system <b>500</b>;</p>
<p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows, in flowchart form, an example method for performing auto-focus in a camera of the optical imaging system of <figref idref="DRAWINGS">FIG. <b>5</b></figref>;</p>
<p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows, in flowchart form, another example method for performing auto-focus in a camera of the optical imaging system of <figref idref="DRAWINGS">FIG. <b>5</b></figref>; and</p>
<p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>B</figref> illustrate the effects on focus distance of a camera which result from movement of a tracked viewing target.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?detailed-description description="Detailed Description" end="lead"?>
<p id="p-0018" num="0017">Like reference numerals are used in the drawings to denote like elements and features.</p>
<heading id="h-0005" level="1">DETAILED DESCRIPTION OF EXAMPLE EMBODIMENTS</heading>
<p id="p-0019" num="0018">In one aspect, the present disclosure describes a processor-implemented method for performing auto-focus in a camera. The method includes: receiving, from a tracking system for tracking a position of a medical instrument, a signal; determining, based on the received signal, that the medical instrument is removed from a field of view of the camera; in response to determining that a continuous auto-focus mode for the camera is enabled: retrieving, from a database, a first focus distance value representing a focus distance that was most recently set with intent for the camera; and automatically updating a focus distance of the camera to the first focus distance value.</p>
<p id="p-0020" num="0019">In some implementations, the method may further comprise storing, in the database, a predetermined number of most recent focus distance values for the camera and timestamps associated with the focus distance values.</p>
<p id="p-0021" num="0020">In some implementations, retrieving the first focus distance value may comprise: obtaining, for each focus distance value stored in the database, an associated speed value, the associated speed value representing an approximate speed of a tracked point of the medical instrument at the focus distance; retrieving, from the database, a most recently stored focus distance value having an associated speed that is below a predefined threshold speed.</p>
<p id="p-0022" num="0021">In some implementations, the associated speed may be obtained based on a finite difference computation using focus distance values stored in the database.</p>
<p id="p-0023" num="0022">In some implementations, the predefined threshold speed may be 0.1 meter per second.</p>
<p id="p-0024" num="0023">In some implementations, the camera may be configured to automatically focus to a predetermined point relative to the medical instrument.</p>
<p id="p-0025" num="0024">In some implementations, the continuous auto-focus mode may be activated via a voice input or activation of a foot pedal.</p>
<p id="p-0026" num="0025">In some implementations, detecting that the medical instrument is removed from the field of view of the camera may comprise determining that the predetermined point is no longer within the field of view of the camera.</p>
<p id="p-0027" num="0026">In some implementations, detecting that the medical instrument is removed from the field of view of the camera may comprise determining that an approximate speed of the medical instrument exceeds a predefined threshold speed.</p>
<p id="p-0028" num="0027">In some implementations, the method may further comprise computing a speed curve representing approximate speeds of the medical instrument which are associated with the stored focus distance values.</p>
<p id="p-0029" num="0028">In another aspect, the present disclosure describes a medical navigation system to support a medical procedure. The medical navigation system includes a tracking system for tracking a position of a medical instrument, a surgical camera for imaging a target surgical site, and a processor coupled to the tracking system and the surgical camera. The processor is configured to: determine, based on a signal from the tracking system, that the medical instrument is removed from a field of view of the surgical camera; in response to determining that a continuous auto-focus mode for the surgical camera is enabled: retrieve, from a database, a first focus distance value representing a focus distance that was most recently set with intent for the surgical camera; and automatically update a focus distance of the surgical camera to the first focus distance value.</p>
<p id="p-0030" num="0029">In yet another aspect, the present disclosure describes an optical imaging system for imaging a target during a medical procedure. The optical imaging system includes a movable arm, a camera mounted on the movable arm, the camera capturing images of a target surgical site, and a processor for calibrating the camera. The processor is configured to: receive, from a tracking system for tracking a position of a medical instrument, a signal; determine, based on the received signal, that the medical instrument is removed from a field of view of the camera; in response to determining that a continuous auto-focus mode for the camera is enabled: retrieve, from a database, a first focus distance value representing a focus distance that was most recently set with intent for the camera; and automatically update a focus distance of the camera to the first focus distance value.</p>
<p id="p-0031" num="0030">Other example embodiments of the present disclosure will be apparent to those of ordinary skill in the art from a review of the following detailed descriptions in conjunction with the drawings.</p>
<p id="p-0032" num="0031">In the present application, the phrase &#x201c;access port&#x201d; is intended to refer to a cannula, a conduit, sheath, port, tube, or other structure that is insertable into a subject, in order to provide access to internal tissue, organs, or other biological substances. In some embodiments, an access port may directly expose internal tissue, for example, via an opening or aperture at a distal end thereof, and/or via an opening or aperture at an intermediate location along a length thereof. In other embodiments, an access port may provide indirect access, via one or more surfaces that are transparent, or partially transparent, to one or more forms of energy or radiation, such as, but not limited to, electromagnetic waves and acoustic waves.</p>
<p id="p-0033" num="0032">In the present application, the term &#x201c;intraoperative&#x201d; is intended to refer to an action, process, method, event, or step that occurs or is carried out during at least a portion of a medical procedure. Intraoperative, as defined herein, is not limited to surgical procedures, and may refer to other types of medical procedures, such as diagnostic and therapeutic procedures.</p>
<p id="p-0034" num="0033">In the present application, the term &#x201c;and/or&#x201d; is intended to cover all possible combinations and sub-combinations of the listed elements, including any one of the listed elements alone, any sub-combination, or all of the elements, and without necessarily excluding additional elements.</p>
<p id="p-0035" num="0034">In the present application, the phrase &#x201c;at least one of . . . or . . . &#x201d; is intended to cover any one or more of the listed elements, including any one of the listed elements alone, any sub-combination, or all of the elements, without necessarily excluding any additional elements, and without necessarily requiring all of the elements.</p>
<p id="p-0036" num="0035">A medical navigation system may be configured to support image-guided medical procedures. The medical navigation system may include a tracking system for tracking one or more medical instruments and an optical imaging system. The optical imaging system includes a camera for imaging a surgical site of interest during a medical procedure. The optical imaging system may be configured to perform auto-focusing relative to a tracked tool that is used in a medical procedure. The position and orientation of a tracked tool may be determined by the tracking system, and a controller of the optical imaging system may perform auto-focusing to focus the captured image on a point defined relative to the tracked tool. By moving the tracked tool over, or in proximity to, a target surgical site, the operator of the medical navigation system (e.g. a surgeon) may be able to observe the surgical site without manually controlling focus optics of the optical imaging system.</p>
<p id="p-0037" num="0036">&#x201c;Continuous&#x201d; auto-focus maintains a viewing target, such as a moving object, constantly in focus. A continuous auto-focus mode for a medical navigation system may allow a tracked tool (e.g. a medical instrument) to be automatically kept in focus of a camera of the optical imaging system. This auto-focus behavior may be disrupted, for example, when the tracked tool is removed from a field of view of the camera. If the camera loses focus, an out-of-focus image may be produced by the camera, and the operator's ability to observe the target surgical site using the camera may be impeded.</p>
<p id="p-0038" num="0037">The present application discloses improved auto-focusing capabilities of an optical imaging system of a medical navigation system. A controller of the optical imaging system is configured to perform operations for recovering continuous auto-focus for a camera of the optical imaging system. The controller receives a signal from the tracking system indicating a position and/or orientation of the target tool relative to the camera. The controller determines, based on the received signal, that the target tool is removed from a field of view of the camera. If continuous auto-focus mode is enabled for the camera, the controller is configured to retrieve, from a database, a first focus distance value representing a focus distance that was most recently set with intent for the camera. The focus distance of the camera may then be automatically updated to the retrieved first focus distance value.</p>
<p id="p-0039" num="0038">Reference is first made to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, which shows an example medical navigation system <b>200</b>. The example medical navigation system <b>200</b> may be used to support image-guided surgery. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a surgeon <b>201</b> performs surgery on a patient <b>202</b> in an operating room environment. A medical navigation system <b>205</b> may include an equipment tower, tracking system, displays, and tracked instruments to assist the surgeon <b>201</b> during a procedure. An operator <b>203</b> may also be present to operate, control, and provide assistance for the medical navigation system <b>205</b>.</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows components of an example medical navigation system <b>205</b>. The disclosed optical imaging system may be used in the context of the medical navigation system <b>205</b>. The medical navigation system <b>205</b> may include one or more displays <b>206</b>, <b>211</b> for displaying video images, an equipment tower <b>207</b>, and a positioning system <b>208</b>, such as a medical arm, which may support an optical imaging system <b>500</b>. One or more of the displays <b>206</b>, <b>211</b> may include a touch-sensitive display for receiving touch input. The equipment tower <b>207</b> may be mounted on a frame, such as a rack or cart, and may contain a power supply and a computer/controller that may execute planning software, navigation software, and/or other software to manage the positioning system <b>208</b>. In some embodiments, the equipment tower <b>207</b> may be a single tower configuration operating with dual displays <b>206</b>, <b>211</b>; however, other configurations (e.g. dual tower, single display etc.) may be possible. The equipment tower <b>207</b> may also be configured with a universal power supply (UPS) to provide for emergency power, in addition to a regular AC adapter power supply.</p>
<p id="p-0041" num="0040">A portion of the patient's anatomy may be held in place by a holder. For example, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the patient's head and brain may be held in place by a head holder <b>217</b>. An access port <b>12</b> and associated introducer <b>210</b> may be inserted into the head, to provide access to a surgical site in the head. The optical imaging system <b>500</b> may be used to view down the access port <b>12</b> at a sufficient magnification to allow for enhanced visibility. The output of the optical imaging system <b>500</b> may be received by one or more computers or controllers to generate a view that may be depicted on a visual display (e.g. one or more of displays <b>206</b>, <b>211</b>).</p>
<p id="p-0042" num="0041">In some embodiments, the medical navigation system <b>205</b> may include a tracked pointer <b>222</b>. The tracked pointer <b>222</b>, which may include markers <b>212</b> to enable tracking by a tracking camera <b>213</b>, may be used to identify points (e.g. fiducial points) on a patient. An operator, typically a nurse or the surgeon <b>201</b>, may use the tracked pointer <b>222</b> to identify the location of points on the patient <b>202</b>, in order to register the location of selected points on the patient <b>202</b> in the medical navigation system <b>205</b>. In some embodiments, a guided robotic system with closed loop control may be used as a proxy for human interaction. Guidance to the robotic system may be provided by any combination of input sources such as image analysis, tracking of objects in the operating room using markers placed on various objects of interest, or any other suitable robotic system guidance techniques.</p>
<p id="p-0043" num="0042">Fiducial markers <b>212</b> may be connected to the introducer <b>210</b> for tracking by the tracking camera <b>213</b>, which may provide positional information of the introducer <b>210</b> from the medical navigation system <b>205</b>. In some embodiments, the fiducial markers <b>212</b> may be alternatively or additionally attached to the access port <b>12</b>. In some embodiments, the tracking camera <b>213</b> may be a 3-D infrared optical tracking stereo camera. In some other examples, the tracking camera <b>213</b> may be an electromagnetic system (not shown), such as a field transmitter, that is configured to use at least one receiver coil disposed in relation to the tool(s) intended for tracking. A known profile of the electromagnetic field and known position of receiver coil(s) relative to each other may be used to infer the location of the tracked tool(s) using the induced signals and their phases in each of the receiver coils.</p>
<p id="p-0044" num="0043">Location data of the positioning system <b>208</b> and/or access port <b>12</b> may be determined by the tracking camera <b>213</b> by detection of the fiducial markers <b>212</b> placed on or otherwise in fixed relation (e.g. in rigid connection) to any of the positioning system <b>208</b>, the access port <b>12</b>, the introducer <b>210</b>, the tracked pointer <b>222</b> and/or other tracked instruments. The fiducial marker(s) <b>212</b> may be active or passive markers. The displays <b>206</b>, <b>2011</b> may provide an output of the computed data of the medical navigation system <b>205</b>. In some embodiments, the output provided by the displays <b>206</b>, <b>211</b> may include at least one of axial, sagittal, or coronal views of patient anatomy as part of a multi-view output.</p>
<p id="p-0045" num="0044">The active or passive fiducial markers <b>212</b> may be placed on tools (e.g. the access port <b>12</b> and/or the optical imaging system <b>500</b>) to be tracked, to determine the location and orientation of these tools using the tracking camera <b>213</b> and medical navigation system <b>205</b>. A stereo camera of the tracking system may be configured to detect the fiducial markers <b>212</b> and to capture images thereof for providing identifiable points for tracking the tools. A tracked tool may be defined by a grouping of markers <b>212</b>, whereby a rigid body may be defined and identified by the tracking system. This may, in turn, be used to determine the position and/or orientation in three dimensions of a tracked tool in a virtual space. The position and orientation of the tracked tool in 3-D may be tracked in six degrees of freedom (e.g. x, y, z coordinates and pitch, yaw, roll rotations), in five degrees of freedom (e.g. x, y, z coordinates and two degrees of free rotation), and preferably tracked in at least three degrees of freedom (e.g. tracking the position of the tip of a tool in at least x, y, z coordinates). In typical use with medical navigation systems, at least three markers <b>212</b> are provided on a tracked tool to define the tool in virtual space; however, it is known to be advantageous for four or more markers <b>212</b> to be used.</p>
<p id="p-0046" num="0045">Camera images capturing the markers <b>212</b> may be logged and tracked by, for example, a closed-circuit television (CCTV) camera. The markers <b>212</b> may be selectable to enable, assist or facilitate in segmentation of the captured images. For example, infrared (IR)-reflecting markers and an IR light source from the direction of the camera may be used. In some embodiments, the spatial position and orientation of the tracked tool and/or the actual and desired position and orientation of the positioning system <b>208</b> may be determined by optical detection using a camera. The optical detection may be performed using an optical camera, rendering the markers <b>212</b> optically visible.</p>
<p id="p-0047" num="0046">In some embodiments, the markers <b>212</b> (e.g. reflectospheres) may be used in combination with a suitable tracking system, to determine the spatial positioning position of the tracked tools within the operating theatre. Different tools and/or targets may be provided with respect to sets of markers <b>212</b> in different configurations. Differentiation of the different tools and/or targets and their corresponding virtual volumes may be possible based on the specification configuration and/or orientation of the different sets of markers <b>212</b> relative to one another, enabling each such tool and/or target to have a distinct individual identity within the medical navigation system <b>205</b>. The individual identifiers may provide information to the medical navigation system <b>205</b>, such as information relating to the size and/or shape of the tool within the medical navigation system <b>205</b>. The identifier may also provide additional information, such as the tool's central point or the tool's central axis, among other information. The virtual tool may also be determined from a database of tools stored in, or provided to, the medical navigation system <b>205</b>. The markers <b>212</b> may be tracked relative to a reference point, or a reference object, in the operating room, such as the patient <b>202</b>.</p>
<p id="p-0048" num="0047">Various types of fiducial markers may be used. The markers <b>212</b> may comprise the same type or a combination of at least two different types. Possible types of markers include reflective markers, radiofrequency (RF) markers, electromagnetic (EM) markers, pulsed or un-pulsed light-emitting diode (LED) markers, glass markers, reflective adhesives, or reflective unique structures or patterns, among others. RF and EM markers may have specific signatures for the specific tools to which such markers are attached. Reflective adhesives, structures and patterns, glass markers, and LED markers may be detectable using optical detectors, while RF and EM markers may be detectable using antennas. Different marker types may be selected to suit different operating conditions.</p>
<p id="p-0049" num="0048">In some embodiments, the markers <b>212</b> may include printed or 3-D designs that may be used for detection by an auxiliary camera, such as a wide-field camera (not shown) and/or the optical imaging system <b>500</b>. Printed markers may also be used as a calibration pattern, for example, to provide distance information (e.g. 3-D distance information) to an optical detector. Printed identification markers may include designs such as concentric circles with different ring spacing and/or different types of bar codes, among other designs. In some embodiments, in addition to or in place of using markers <b>212</b>, the contours of known objects (e.g. the side of the access port <b>12</b>) could be captured by and identified using optical imaging devices and the tracking system.</p>
<p id="p-0050" num="0049">A guide clamp <b>218</b> (or more generally a guide) for holding the access port <b>12</b> may be provided in the medical navigation system <b>205</b>. The guide clamp <b>218</b> may allow the access port <b>12</b> to be held at a fixed position and orientation while freeing up the surgeon's hands. An articulated arm <b>219</b> may be provided to hold the guide clamp <b>218</b>. The articulated arm <b>219</b> may have up to six degrees of freedom to position the guide clamp <b>218</b>. The articulated arm <b>219</b> may be lockable to fix its position and orientation, once a desired position is achieved. The articulated arm <b>219</b> may be attached or attachable to a point based on the patient head holder <b>217</b>, or another suitable point (e.g. on another patient support, such as on the surgical bed), to ensure that when locked in place, the guide clamp <b>218</b> does not move relative to the patient's head.</p>
<p id="p-0051" num="0050">In a surgical operating room/theatre, setup of a medical navigation system may be complicated; numerous pieces of equipment associated with the surgical procedure, as well as various elements of the medical navigation system <b>205</b>, may need to be arranged and prepared. Setup time typically increases as more equipment is added. To assist in addressing this, the medical navigation system <b>205</b> may include two additional wide-field cameras to enable video overlay information. Video overlay information can be inserted into displayed images, such as images displayed on one or more of the displays <b>206</b>, <b>211</b>. The overlay information may illustrate the physical space where accuracy of the 3-D tracking system (which is typically part of the medical navigation system <b>205</b>) is greater, may illustrate the available range of motion of the positioning system <b>208</b> and/or the optical imaging system <b>500</b>, and may help to guide head and/or patient positioning.</p>
<p id="p-0052" num="0051">The medical navigation system <b>205</b> may provide tools to the surgeon that may help to provide more relevant information to the surgeon, and may assist in improving performance and accuracy of port-based surgical operations. Although described in the present disclosure in the context of port-based neurosurgery (e.g. for removal of brain tumors and/or for treatment of intracranial hemorrhages (ICH)), the medical navigation system <b>205</b> may also be suitable for one or more of: brain biopsy, functional/deep-brain stimulation, catheter/shunt placement (in the brain or elsewhere), open craniotomies, and/or endonasal/skull-based/ear-nose-throat (ENT) procedures, among others. The same medical navigation system <b>205</b> may be used for carrying out any or all of these procedures, with or without modification as appropriate.</p>
<p id="p-0053" num="0052">In some embodiments, the tracking camera <b>213</b> may be part of a suitable tracking system. In some embodiments, the tracking camera <b>213</b> (and any associated tracking system that uses the tracking camera <b>213</b>) may be replaced with a suitable tracking system which may or may not use camera-based tracking techniques. For example, a tracking system that does not use the tracking camera <b>213</b>, such as a radiofrequency tracking system, may be used with the medical navigation system <b>205</b>.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating an example control and processing system <b>300</b> that may be used as part of the medical navigation system <b>205</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> (e.g. as part of the equipment tower <b>207</b>). As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the control and processing system <b>300</b> may include one or more processors <b>302</b>, a memory <b>304</b>, a system bus <b>306</b>, one or more input/output interfaces <b>308</b>, a communications interface <b>310</b>, and storage device <b>312</b>. The control and processing system <b>300</b> may interface with other external devices, such as a tracking system <b>321</b>, data storage <b>342</b>, and external user input and output devices <b>344</b>, which may include, for example, one or more of a display, keyboard, mouse, sensors attached to medical equipment, foot pedal, and microphone and speaker. Data storage <b>342</b> may be any suitable data storage device, such as a local or remote computing device (e.g. a computer, hard drive, digital media device, or server) having a database stored thereon. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, data storage device <b>342</b> includes identification data <b>350</b> for identifying one or more medical instruments <b>360</b> and configuration data <b>352</b> that associates customized configuration parameters with one or more medical instruments <b>360</b>. The data storage device <b>342</b> may also include preoperative image data <b>354</b> and/or medical procedure planning data <b>356</b>. Although the data storage device <b>342</b> is shown as a single device in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, it will be understood that in other embodiments, the data storage device <b>342</b> may be provided as multiple storage devices.</p>
<p id="p-0055" num="0054">The medical instruments <b>360</b> may be identifiable by the control and processing unit <b>300</b>. The medical instruments <b>360</b> may be connected to and controlled by the control and processing unit <b>300</b>, or the medical instruments <b>360</b> may be operated or otherwise employed independent of the control and processing unit <b>300</b>. The tracking system <b>321</b> may be employed to track one or more medical instruments <b>360</b> and spatially register the one or more tracked medical instruments to an intraoperative reference frame. For example, a medical instrument <b>360</b> may include tracking markers such as tracking spheres that may be recognizable by the tracking camera <b>213</b>. In one example, the tracking camera <b>213</b> may be an infrared (IR) tracking camera. In another example, a sheath placed over a medical instrument <b>360</b> may be connected to and controlled by the control and processing unit <b>300</b>.</p>
<p id="p-0056" num="0055">The control and processing unit <b>300</b> may also interface with a number of configurable devices, and may intraoperatively reconfigure one or more of such devices based on configuration parameters obtained from configuration data <b>352</b>. Examples of devices <b>320</b>, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, include one or more external imaging devices <b>322</b>, one or more illumination devices <b>324</b>, the positioning system <b>208</b>, the tracking camera <b>213</b>, one or more projection devices <b>328</b>, and one or more displays <b>206</b>, <b>211</b>.</p>
<p id="p-0057" num="0056">Exemplary aspects of the disclosure can be implemented via the processor(s) <b>302</b> and/or memory <b>304</b>. For example, the functionalities described herein can be partially implemented via hardware logic in the processor <b>302</b> and partially using the instructions stored in the memory <b>304</b>, as at least one processing module or engine <b>370</b>. Example processing modules include, but are not limited to, a user interface engine <b>372</b>, a tracking module <b>374</b>, a motor controller <b>376</b>, an image processing engine <b>378</b>, an image registration engine <b>380</b>, a procedure planning engine <b>382</b>, a navigation engine <b>384</b>, and a context analysis module <b>386</b>. While the example processing modules are shown separately in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in some embodiments, the processing modules <b>370</b> may be stored in the memory <b>304</b> and the processing modules <b>370</b> may be collectively referred to as processing modules <b>370</b>. In some embodiments, two or more modules <b>370</b> may be used together to perform a function. Although depicted as separate modules <b>370</b>, the modules <b>370</b> may be embodied as a unified set of computer-readable instructions (e.g. stored in the memory <b>304</b>) rather than distinct sets of instructions.</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates use of an example optical imaging system <b>500</b>, described further below, in a medical procedure. Although <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows the optical imaging system <b>500</b> being used in the context of a navigation system environment <b>200</b> (e.g. using a medical navigation system as described above), the optical imaging system <b>500</b> may also be used outside of a navigation system environment.</p>
<p id="p-0059" num="0058">An operator, typically a surgeon <b>201</b>, may use the optical imaging system <b>500</b> to observe a surgical site (e.g. to look down an access port). The optical imaging system <b>500</b> may be attached to a positioning system <b>208</b>, such as a controllable and adjustable robotic arm. The position and orientation of the positioning system <b>208</b>, imaging system <b>500</b>, and/or access port may be tracked using a tracking system, such as described above for the medical navigation system <b>205</b>. The distance between the optical imaging system <b>500</b> (more specifically, the aperture of the optical imaging system <b>500</b>) and the viewing target may be referred to as the working distance. The optical imaging system <b>500</b> may be designed to be used in a predefined range of working distance (e.g. in the range of between 15 and 75 centimeters). It should be noted that, if the optical imaging system <b>500</b> is mounted on the positioning system <b>208</b>, the actual available range of working distance may be dependent on both the working distance of the optical imaging system <b>500</b> as well as the workspace and kinematics of the positioning system <b>208</b>. In some embodiments, the optical imaging system <b>500</b> may include a manual release button that, when actuated, enables the optical imaging system to be positioned manually. For example, the controller of the optical imaging system <b>500</b> may be responsive to manual control input received via a user interface.</p>
<p id="p-0060" num="0059">Reference is made to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, which illustrates components of an example optical imaging system <b>500</b>. The optical imaging system <b>500</b> includes an optical assembly <b>505</b> (which may also be referred to as an optical train). The optical assembly <b>505</b> includes optics, e.g. lenses, optical fibers, etc., for focusing and zooming on a viewing target. Specifically, the optical assembly <b>505</b> includes zoom optics <b>510</b> (which may include one or more zoom lenses) and focus optics <b>515</b> (which may include one or more focus lenses). Each of the zoom optics <b>510</b> and the focus optics <b>515</b> may be independently movable within the optical assembly <b>505</b> for respectively adjusting the zoom and focus. Where the zoom optics <b>510</b> and/or the focus optics <b>515</b> include more than one lens, each individual lens may be independently movable. The optical assembly <b>505</b> may comprise an aperture (not shown) which is adjustable.</p>
<p id="p-0061" num="0060">The optical imaging system <b>500</b> may also include a zoom actuator <b>520</b> and a focus actuator <b>525</b> for respectively positioning the zoom optics <b>510</b> and the focus optics <b>515</b>. The zoom actuator <b>520</b> and/or the focus actuator <b>525</b> may comprise electric motors or other types of actuators, such as pneumatic actuators, hydraulic actuators, shape-changing materials, e.g., piezoelectric materials or other smart materials, or engines, among other possibilities. Although the zoom actuator <b>520</b> and the focus actuator <b>525</b> are shown outside of the optical assembly <b>505</b>, in some embodiments, the zoom actuator <b>520</b> and the focus actuator <b>525</b> are components of, or are integrated with, the optical assembly <b>505</b>. The zoom actuator <b>520</b> and the focus actuator <b>525</b> may operate independently, to respectively control positioning of the zoom optics <b>510</b> and the focus optics <b>515</b>. The lens(es) of the zoom optics <b>510</b> and/or the focus optics <b>515</b> may each be mounted on a linear stage, e.g. a motion system that restricts an object to move in a single axis, which may include a linear guide and an actuator, or a conveyor system such as a conveyor belt mechanism that is respectively moved by the zoom actuator <b>520</b> and/or the focus actuator <b>525</b> to control positioning of the zoom optics <b>510</b> and/or the focus optics <b>515</b>. In some embodiments, the zoom optics <b>510</b> may be mounted on a linear stage that is driven, via a belt drive, by the zoom actuator <b>520</b>, while the focus optics <b>515</b> may be geared to the focus actuator <b>525</b>. The independent operation of the zoom actuator <b>520</b> and the focus actuator <b>525</b> may enable the zoom and focus to be adjusted independently. Thus, when an image is in focus, the zoom may be adjusted without requiring further adjustments to the focus optics <b>515</b> to produce a focused image.</p>
<p id="p-0062" num="0061">Operation of the zoom actuator <b>520</b> and the focus actuator <b>525</b> may be controlled by a controller <b>530</b> (e.g. a microprocessor) of the optical imaging system <b>500</b>. The controller <b>530</b> may receive control input from an external system, such as an external processor or an input device. The control input may indicate a desired zoom/focus, and the controller <b>530</b> may, in response, cause the zoom actuator <b>520</b> or the focus actuator <b>525</b> to move the zoom optics <b>510</b> or the focus optics <b>515</b> accordingly, to achieve the desired zoom/focus. In some embodiments, the zoom optics <b>510</b> and/or the focus optics <b>515</b> may be moved or actuated without the use of the zoom actuator <b>520</b> and/or the focus actuator <b>525</b>. For example, the focus optics <b>515</b> may use electrically-tunable lenses or other deformable material that is directly controlled by the controller <b>530</b>.</p>
<p id="p-0063" num="0062">The optical imaging system <b>500</b> may enable an operator (e.g. a surgeon) of the medical navigation system <b>205</b> to control zoom or focus during a medical procedure without having to manually adjust the zoom optics <b>510</b> or focus optics <b>515</b>. For example, the operator may provide control input to the controller <b>530</b> verbally, e.g. via a voice recognition input system, by instructing an assistant to enter control input into an external input device, via a user interface provided by a workstation, using a foot pedal, or by other such means. In some embodiments, the controller <b>530</b> may execute preset instructions to maintain the zoom and/or focus at preset values, for example, to perform auto-focusing, without requiring further control input during a medical procedure.</p>
<p id="p-0064" num="0063">An external processor (e.g. a processor of a workstation or the medical navigation system <b>205</b>) in communication with the controller <b>530</b> may be used to provide control input to the controller <b>530</b>. For example, the external processor may provide a graphical user interface for receiving input instructions to control zoom and/or focus of the optical imaging system <b>500</b>. The controller <b>530</b> may alternatively or additionally be in communication with an external input system, e.g., a voice-recognition input system or a foot pedal. The optical assembly <b>505</b> includes at least one auxiliary optic <b>540</b>, e.g., an adjustable aperture, which is static or dynamic. Where the auxiliary optics <b>540</b> is dynamic, the auxiliary optics <b>540</b> may be moved using an auxiliary actuator (not shown) which is controlled by the controller <b>530</b>.</p>
<p id="p-0065" num="0064">The optical imaging system <b>500</b> includes a camera <b>535</b> (or video-scope) that is configured to capture image data from the optical assembly <b>505</b>. Operation of the camera may be controlled by the controller <b>530</b>. The camera <b>535</b> may also output data to an external system, such as an external workstation or external output device, to view the captured image data. In some embodiments, the camera <b>535</b> outputs data to the controller <b>530</b>, which, in turn, transmits the data to an external system for viewing. The captured images may be viewable on a larger display and may he displayed together with other information relevant to a medical procedure, e.g. a wide-field view of the surgical site, navigation markers, 3D images, etc. Image data captured by the camera <b>535</b> may be displayed on a display together with a wide-field view of the surgical site, for example, in a multiple-view user interface. The portion of the surgical site that is captured by the camera <b>535</b> may be visually indicated in the wide-field view of the surgical site.</p>
<p id="p-0066" num="0065">The optical imaging system <b>500</b> may include a three-dimensional (3-D) scanner <b>545</b> or 3-D camera for obtaining 3-D information of a viewing target. 3-D Information from the 3-D scanner <b>545</b> may be captured by the camera <b>535</b>, or captured by the 3D scanner <b>545</b> itself. Operation of the 3-D scanner <b>545</b> may be controlled by the controller <b>530</b>, and the 3-D scanner <b>545</b> may transmit data to the controller <b>530</b>. 3-D information from the 3-D scanner <b>545</b> may be used to generate a 3-D image of a viewing target (e.g. a 3-D image of a target tumor to be re-seated). 3-D information may also be useful in an augmented reality (AR) display provided by an external system. For example, an AR display may, using information from a navigation system to register 3-D information with optical images, overlay a 3-D image of a target specimen on a real-time optical image captured by the camera <b>535</b>.</p>
<p id="p-0067" num="0066">The controller <b>530</b> is coupled to a memory <b>550</b>. The memory <b>550</b> may be internal or external in relation to the optical imaging system <b>500</b>. Data received by the controller <b>530</b> (e.g. image data from the camera <b>535</b>, 3-D data from the 3D scanner, etc.) may be stored in the memory <b>550</b>. The memory <b>550</b> may also contain instructions to enable the controller to operate the zoom actuator <b>520</b> and the focus actuator <b>525</b>. For example, the memory <b>550</b> may store instructions to enable the controller <b>530</b> to perform auto-focusing. The optical imaging system <b>500</b> may communicate with an external system, such as a navigation system or a workstation, via wired or wireless communication. In some embodiments, the optical imaging system <b>500</b> may include a wireless transceiver (not shown) to enable wireless communication. In some embodiments, the optical imaging system <b>500</b> includes a power source (e.g. a battery) or a connector to a power source, such as an AC adaptor. The optical imaging system <b>500</b> may receive power via a connection to an external system, such as an external workstation or processor.</p>
<p id="p-0068" num="0067">In some embodiments, the optical assembly <b>505</b>, zoom actuator <b>520</b>, focus actuator <b>525</b>, and camera <b>535</b> may all be housed within a single housing (not shown) of the optical imaging system. The controller <b>530</b>, memory <b>550</b>, 3D scanner <b>545</b>, wireless transceiver, and/or power source may also be housed within the housing. The optical imaging system <b>500</b> may also provide mechanisms to enable manual adjusting of the zoom optics <b>510</b> and/or focus optics <b>515</b>. Such manual adjusting may be enabled in addition to motorized adjusting of zoom and focus.</p>
<p id="p-0069" num="0068">The optical imaging system <b>500</b> may be mounted on a movable support structure, such as a positioning system (e.g. a robotic arm) of a navigation system, a manually operated support arm, a ceiling mounted support, a movable frame, or other such support structure. The optical imaging system <b>500</b> is removably mounted on the movable support structure. In some embodiments, the optical imaging system <b>500</b> may include a support connector, e.g. a mechanical coupling, to enable the optical imaging system <b>500</b> to be quickly and easily mounted or dismounted from the support structure. The support connector on the optical imaging system <b>500</b> may be suitable for connecting with a typical complementary connector on the support structure, e.g. as designed for typical end effectors. In some embodiments, the optical imaging system <b>500</b> may be mounted to the support structure together with other end effectors, or may be mounted to the support structure via another end effector.</p>
<p id="p-0070" num="0069">When mounted, the optical imaging system <b>500</b> may be at a known fixed position and orientation relative to the support structure, by calibrating the position and orientation of the optical imaging system <b>500</b> after mounting. By determining the position and orientation of the support structure, e.g., using a navigation system or by tracking the movement of the support structure from a known starting point, the position and orientation of the optical imaging system <b>500</b> may also be determined. In some embodiments, the optical imaging system <b>500</b> may include a manual release button that when actuated, enables the optical imaging system <b>500</b> to be manually positioned.</p>
<p id="p-0071" num="0070">In some embodiments, where the optical imaging system <b>500</b> is intended to be used in a navigation system environment, the optical imaging system <b>500</b> may include an array of trackable markers, which is mounted on a frame on the optical imaging system <b>500</b> to enable the navigation system to track the position and orientation of the optical imaging system <b>500</b>. Alternatively, or additionally, the movable support structure, such as a positioning system of the navigation system, on which the optical imaging system <b>500</b> is mounted, may be tracked by the navigation system. The position and orientation of the optical imaging system <b>500</b> may be determined by using the known position and orientation of the optical imaging system <b>500</b> relative to the movable support structure.</p>
<p id="p-0072" num="0071">The position and orientation of the optical imaging system <b>500</b> relative to a viewing target may be determined by a processor external to the optical imaging system <b>500</b>, such as a processor of the navigation system. Information about the position and orientation of the optical imaging system <b>500</b> may be used, together with a robotic positioning system, to maintain alignment of the optical imaging system <b>500</b> with the viewing target throughout the medical procedure.</p>
<p id="p-0073" num="0072">The navigation system tracks the position and orientation of the positioning system and/or the optical imaging system <b>500</b>, either collectively or independently. The navigation system may determine the desired joint positions for the positioning system so as to maneuver the optical imaging system <b>500</b> to an appropriate position and orientation to maintain alignment with the viewing target. For example, the positioning system may be configured to align the longitudinal axes of the optical imaging system <b>500</b> and the access port. This alignment may be maintained throughout the medical procedure automatically, without requiring explicit control input. In some embodiments, the operator may be able to manually move the positioning system and/or the optical imaging system <b>500</b>. During such manual movement, the navigation system may continue to track the position and orientation of the positioning system and/or the optical imaging system <b>500</b>. After completion of manual movement, the navigation system may reposition and reorient the positioning system and the optical imaging system <b>500</b> to regain alignment with the access port.</p>
<p id="p-0074" num="0073">The controller <b>530</b> may use information about the position and orientation of the optical imaging system <b>500</b> to perform auto-focusing. In particular, the controller <b>530</b> may be configured to perform auto-focusing operations for a camera of the optical imaging system <b>500</b>. For example, the controller <b>530</b> may determine a working distance between the optical imaging system <b>500</b> and a viewing target, and based on the working distance, determine the desired positioning of the focus optics <b>515</b> to obtain a focused image. The position of the viewing target may, for example, be determined by a navigation system. The working distance may be determined by the controller <b>530</b> using information about the position and orientation of the optical imaging system <b>500</b> and/or the positioning system relative to the viewing target. In some embodiments, the working distance may he determined by the controller <b>530</b> using an infrared light (not shown) mounted on or near a distal end of the optical imaging system <b>500</b>.</p>
<p id="p-0075" num="0074">In some embodiments, the controller <b>530</b> may perform auto-focusing without reference to information about the position and orientation of the optical imaging system <b>500</b>. For example, the controller <b>530</b> may adjust the focus actuator <b>525</b> to move the focus optics <b>515</b> into a range of focus positions and control the camera <b>535</b> to capture image data at each focus position. The controller <b>530</b> may then perform image processing on the captured images to determine which focus position has the sharpest image and determine that this focus position is the desired position of the focus optics <b>515</b>. The controller <b>530</b> may then control the focus actuator <b>525</b> to move the focus optics <b>515</b> to the desired position. Other auto-focus routines, such as those suitable for handheld cameras, may be implemented by the controller <b>530</b> as appropriate.</p>
<p id="p-0076" num="0075">A viewing target may be dynamically defined by an operator, for example, via touch input selecting a desired target on a touch-sensitive display, by using eye- or head-tracking to detect a point at which the operator's gaze is focused, and/or by voice command. The optical imaging system <b>500</b> may perform auto-focusing to dynamically focus the image on the defined viewing target, thereby enabling the operator to focus an image on different points within a field of view, without changing the field of view and without having to manually adjust the focus of the optical imaging system <b>500</b>.</p>
<p id="p-0077" num="0076">In at least some embodiments, the optical imaging system <b>500</b> may be configured to perform auto-focusing relative to a tracked tool, such as a medical instrument, that is used in a medical procedure. For example, the position and orientation of a medical instrument (e.g. a tracked pointer tool) may be determined, and the controller <b>530</b> may perform auto-focusing to focus the captured image on a point defined relative to the medical instrument. As the tracked tool is moved, the working distance between the optical imaging system <b>500</b> and a defined focus point of the tracked tool may change. The auto-focusing is performed in a manner similar to that as above described; however, instead of auto-focusing on a viewing target in the surgical field, the optical imaging system <b>500</b> focuses on a focus point that is defined relative to the tracked tool. The tracked tool may be used in the surgical field to guide the optical imaging system <b>500</b> to auto-focus on different points in the surgical field, enabling a surgeon to change the focus within a field of view, e.g. focus on a point other than at the center of the field of view, without changing the field of view and without needing to manually adjust the focus of the optical imaging system <b>500</b>. Where the field of view includes objects at different depths, the surgeon may use the tracked tool, e.g. a pointer, to indicate to the optical imaging system <b>500</b> the object and/or depth desired for auto-focusing.</p>
<p id="p-0078" num="0077">The controller <b>530</b> may receive information about the position and orientation of a medical instrument. This position and orientation information may be received, for example, from an external source, such as an external tracking system for tracking the medical instrument, or from another component of the optical imaging system <b>500</b>, e.g. an infrared sensor or a machine vision component of the optical imaging system <b>500</b>. The controller <b>530</b> may determine a focus point relative to the position and orientation of the medical instrument. The focus point may be predefined for a given medical instrument, e.g. the distal tip of a pointer, the distal end of a catheter, the distal end of an access port, the distal end of a soft tissue re-sector, the distal end of a suction, the target of a laser, or the distal tip of a scalpel), and is different for different medical instruments. The controller <b>530</b> may use this information, together with information about the known position and orientation of the optical imaging system <b>500</b> in order to determine the desired position of the focus optics <b>515</b> to achieve an image focused on the focus point defined relative to the medical instrument.</p>
<p id="p-0079" num="0078">Where the optical imaging system <b>500</b> is used with a navigation system (such as the medical navigation system <b>205</b>), the position and orientation of a medical instrument, e.g. tracked pointer tool <b>222</b>, tracked port <b>210</b>, etc. may be tracked and determined by the navigation system. The controller <b>530</b> of the optical imaging system <b>500</b> may automatically focus the optical imaging system <b>500</b> to a predetermined point relative to the tracked medical instrument. For example, the optical imaging system <b>500</b> may auto-focus on the tip of a tracked pointer tool or on the distal end of the access port <b>210</b>.</p>
<p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates an example embodiment of an optical imaging system <b>500</b>. Specifically, <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a perspective view of an end-effector which may house the components of the optical imaging system <b>500</b>. The end-effector may be tracked, for example, using an external tracking system. As shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> one or more tracking markers <b>1200</b> (e.g. spheres) may be coupled to the end-effector to facilitate tracking by a tracking system. The tracking system may include, at least, a stationary tracking camera <b>213</b> which detects the positions of the tracking markers <b>1200</b>, enabling the tracking system to determine the position and orientation of the end-effector. The camera <b>535</b> of the optical imaging system <b>500</b> may be in a fixed position and/or orientation relative to the end-effectors tracking markers <b>1200</b>. Accordingly, the position and orientation of the camera <b>535</b> may be tracked using the external tracking system. In particular, a distance between the camera <b>535</b> and a viewing target, such as a tracked medical instrument, may be obtained using the tracking system. That is, by tracking the position and/or orientation of a tracked medical instrument and the end-effector using tracking markers, an auto-focus driven focus distance of a camera of the optical imaging system <b>500</b> may be determined.</p>
<p id="p-0081" num="0080">In at least some embodiments, the optical imaging system <b>500</b> may perform auto-focusing relative to a medical instrument only when the focus point relative to the medical instrument is determined to be within the field of view of the optical imaging system <b>500</b>. Where the optical imaging system <b>500</b> is mounted on a movable support system, such as a robotic arm, if the focus point of the medical instrument is outside of the current field of view of the optical imaging system <b>500</b>, the movable support system may position and orient the optical imaging system <b>500</b> to bring the focus point of the medical instrument within the field of view of the optical imaging system <b>500</b>, in response to input such as a voice command, activation of a foot pedal, etc.</p>
<p id="p-0082" num="0081">The optical imaging system <b>500</b> may implement a time lag before performing auto-focus relative to a medical instrument, in order to avoid erroneously changing focus while the focus point of the medical instrument is brought into, and out of, the field of view. For example, the optical imaging system <b>500</b> may be configured to auto-focus on a focus point of a tracked medical instrument only after the focus point has been substantially stationary for a predetermined length of time, e.g. approximately 0.5 second to 1 second. In some embodiments, the optical imaging system <b>500</b> may also be configured to perform zooming with the focus point as the zoom center. For example, while a focus point is in the field of view, or after auto-focusing on a certain point in the field of view, the user may provide command input to instruct the optical imaging system <b>500</b> to zoom in on the focus point. The controller <b>530</b> may then position the zoom optics <b>520</b> accordingly to zoom in on the focus point. Where appropriate, the positioning system may automatically reposition the optical imaging system <b>500</b> as needed to center the zoomed in view on the focus point.</p>
<p id="p-0083" num="0082">In some embodiments, the optical imaging system <b>500</b> may automatically change between different auto-focus modes. For example, if the current field of view does not include any focus point defined by a medical instrument, the controller <b>530</b> may perform auto-focus based on preset criteria, e.g. to obtain the sharpest image or to focus on the center of the field of view. When a focus point defined by a medical instrument is brought into the field of view, the controller <b>530</b> may automatically switch mode to auto-focus on the focus point. In some embodiments, the optical imaging system <b>500</b> may change between different auto-focus modes in response to user input. In particular, a user may trigger auto-focus (e.g. single or continuous auto-focus) in the optical imaging system <b>500</b> via, for example, user command on a user interface, voice input, or activation of a foot pedal (e.g. button press, acknowledgement, etc.). In various examples of auto-focusing, whether or not relative to a medical instrument, the optical imaging system <b>500</b> may be configured to maintain the focus as the zoom is adjusted.</p>
<p id="p-0084" num="0083">The optical imaging system <b>500</b> may additionally generate a depth map (not shown). This is performed by capturing images of the same field of view, wherein the optical imaging system <b>500</b> focuses on points at a plurality of different depths to simulate 3-D depth perception. For example, the optical imaging system <b>500</b> may perform auto-focusing through a predefined depth range, through a depth of approximately 1 centimeter, and capturing focused images at a plurality of different depths through a depth range. The plurality of images captured at the corresponding different depths may be transmitted to an external system, such as an image viewing workstation, and the plurality of images may be aggregated into a set of depth images to form a depth map for the same field of view.</p>
<p id="p-0085" num="0084">The presently disclosed methods for performing auto-focus operations are described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>. More specifically, the methods <b>600</b> and <b>700</b> illustrated in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>, respectively, may enable recovery of focus in a continuous auto-focus mode for an optical imaging system. In a medical procedure, an optical imaging system (i.e. a camera of the optical imaging system) may automatically focus on a focus point defined relative to a tracked medical instrument, such as a pointer tool. When the tracked medical instrument is removed from a field of view of the camera, out-of-focus images may be produced by the camera. In particular, a focus distance of the camera may be set to random values as the tracked medical instrument is moved out of the field of view of the camera.</p>
<p id="p-0086" num="0085">This scenario is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>. The surgeon <b>201</b> may use a tracked medical instrument <b>802</b> during a procedure, and the camera <b>535</b> may be configured to auto-focus on a defined focus point relative to the tracked medical instrument <b>802</b>. By virtue of the auto-focusing, the surgeon <b>201</b> may be able to observe a surgical site of interest when the tracked medical instrument <b>802</b> is moved over, or in proximity to, the surgical site. However, if the surgeon <b>201</b> removes the tracked medical instrument <b>802</b> from a field of view of the camera <b>535</b>, the camera <b>535</b> may produce an out-of-focus image. This loss of focus may, at least temporarily, impede the surgeon <b>201</b> from observing the surgical site with the desired zoom and focus using the camera <b>535</b>.</p>
<p id="p-0087" num="0086">A desired auto-focus scenario is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>. In <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>, as the surgeon <b>201</b> removes the tracked medical instrument <b>802</b> from the field of view of the camera <b>535</b>, the focus of the camera <b>535</b> may change until the tracked medical instrument is completely outside of the field of view of the camera <b>535</b>. The focus distance of the camera <b>535</b> can then be returned to a most recent focus distance value that was set with intent, for example, by the surgeon <b>201</b>. In particular, the focus distance of the camera <b>535</b> may, after a period of fluctuation during removal of the tracked medical instrument from the field of view of the camera <b>535</b>, be set to the most recent value of focus distance that was determined to be stable prior to the removal from the field of view of the camera <b>535</b>.</p>
<p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows, in flowchart form, an example method <b>600</b> for recovering auto-focus in a camera of an optical imaging system. The method <b>600</b> may be implemented in a digital microscope system. For example, the method <b>600</b> may be implemented by a controller of an optical imaging system integrated into a digital microscope, or similar processing unit for controlling operations of a camera of an optical imaging system. The optical imaging system is configured for continuous auto-focusing. In particular, the camera is configured to automatically focus to a defined focus point relative to a tracked tool, such as a medical instrument.</p>
<p id="p-0089" num="0088">In operation <b>602</b>, the controller receives a signal from a tracking system for tracking a position of a medical instrument. The signal represents an indication of, at least, a position of the medical instrument relative to the camera of the optical imaging system. In particular, the signal may include a representation of the position of a defined point of the medical instrument with respect to a field of view of the camera. The tracking system may be configured to track a point (e.g. a distal tip) defined relative to the medical instrument, and the signal may indicate the position of this defined point with respect to the field of view of the camera. In some embodiments, the signal may additionally represent an orientation of the medical instrument relative to the camera. For example, the signal may include a representation of an orientation of a defined portion of the medical instrument with respect to a field of view of the camera.</p>
<p id="p-0090" num="0089">In operation <b>604</b>, the controller determines, based on the received signal, that the medical instrument is removed from a field of view of the camera. For example, the controller may determine that a defined point on the medical instrument is positioned outside of the field of view of the camera. In some embodiments, the controller may detect an approximate speed of the medical instrument and determine whether the medical instrument is being removed from the field of view of the camera based on the speed data. For example, the tracking system may be configured to detect an approximate speed of a defined point on the medical instrument and transmit the speed data to the controller at certain intervals. Based on the received speed data, if the approximate speed of (a defined point on) the medical instrument exceeds a predefined threshold speed, the controller may determine that the medical instrument is being removed from the field of view of the camera. Alternatively, the speed data may be obtained via a finite difference computation, as will be described in greater detail below.</p>
<p id="p-0091" num="0090">If the auto-focus mode is enabled for the camera, the controller may, upon determining that the medical instrument is removed from the field of view of the camera, adjust a focus distance of the camera. This behavior may be desirable in the continuous auto-focus mode in order to ensure that the latest stable focus position is recovered and the focus continues to track to the surgical site. Thus, in operation <b>606</b>, the controller retrieves, from a database, a first focus distance value representing a focus distance that was most recently set with intent for the camera. The database may, in some embodiments, be a rolling buffer containing recent focus distance data for the camera. For example, the rolling buffer may store a predetermined number of most recent auto-focus driven focus distance values for the camera and timestamps associated with the detected focus distance values. The focus distance values may be obtained, for example, from the camera or determined by the controller itself.</p>
<p id="p-0092" num="0091">The retrieved first focus distance may be a value of focus distance stored in the database with the most recent timestamp. The focus distance values that are stored in the database may represent, for example, stable focus distances, or focus distances corresponding to in-focus imaging by the camera. The stable focus distances may, in some embodiments, be associated with dwell periods which last longer than a predefined threshold. A dwell period refers to a period of time during which a tracked medical instrument &#x201c;dwells&#x201d; or remains in relatively fixed distance with respect to the camera. A longer dwell period may represent steady or stable positioning of the medical instrument relative to the camera, whereas a short dwell period may correlate to movement of the medical instrument. The controller (or the camera itself) may store those focus distance values that are determined to be stable based, for example, on dwell periods.</p>
<p id="p-0093" num="0092">In operation <b>608</b>, the controller automatically updates a focus distance of the camera to the retrieved first focus distance value. That is, the focus distance of the camera may be automatically set to the first focus distance value determined by the controller, and not a different focus distance value which may result from the default auto-focus behavior for the tracked medical instrument.</p>
<p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows, in flowchart form, another example method <b>700</b> for recovering auto-focus in a camera of an optical imaging system. As for the method <b>600</b>, the method <b>700</b> may be implemented in a digital microscope system. For example, the method <b>700</b> may be implemented by a controller of an optical imaging system integrated into a digital microscope, or similar processing unit for controlling operations of a camera of an optical imaging system. The optical imaging system is configured for continuous auto-focusing. In particular, the camera is configured to automatically focus to a defined focus point relative to a tracked tool, such as a medical instrument.</p>
<p id="p-0095" num="0094">The method <b>700</b> employs a speed-based approach to determining a stable focus distance value when recovering continuous auto-focusing for a camera. In particular, the method is based on an assumption that a focus distance that is set with intent by an operator of the camera will have an associated dwell period during which a tracked tool driven focus distance has an associated speed that is consistently below a predefined threshold. That is, a stable focus distance value may correspond to a period of time during which a tracked tool is moved at a speed that is less than a threshold. The continuous auto-focus of the camera may be recovered by automatically setting a focus distance of the camera to the most recent such stable focus distance value.</p>
<p id="p-0096" num="0095">In operation <b>702</b>, the controller stores focus distance values and associated timestamps for the camera in a database. The database may, for example, be a rolling buffer for storing a predetermined number of most recent focus distance values for the camera.</p>
<p id="p-0097" num="0096">The controller detects, in operation <b>704</b>, that a tracked medical instrument is out of field of view of the camera. For example, the controller may receive, from a tracking system for tracking the medical instrument, a signal representing an indication of the position and/or orientation of the medical instrument relative to the camera. Based on the received signal, the controller may determine whether the medical instrument is out of, or being removed from, the field of view of the camera. For example, the controller may compare coordinates associated with the camera's field of view with coordinates of the medical instrument (or a defined point relative to the medical instrument) in the same coordinate space. Upon comparing the coordinates, the controller may be able to determine whether the medical instrument is within, or falls outside of, the bounds of the camera's field of view.</p>
<p id="p-0098" num="0097">In operation <b>706</b>, the controller obtains, for each stored focus distance value, an associated speed value. The speed values represent the instantaneous speeds of the tracked medical instrument at the respective focus distances. That is, the associated speed value represents an approximate speed of a tracked point of the medical instrument at the focus distance. In at least some embodiments, the associated speed for a focus distance value may be approximated numerically. For example, the associated speed may be obtained based on a finite difference computation using stored focus distance values for the camera. That is, the rates at which the focus distance values change, presumably as a result of movement of the tracked medical instrument, may be approximated. Various techniques for computing finite difference approximations may be suitable. In some embodiments, the calculation of weights in finite difference formulas during the calculation of approximate speeds may be based on an approach proposed in &#x201c;Calculation of Weights in Finite Difference Formulas&#x201d; (Fornberg, SIAM Rev. vol, 40, no.3, pp. 685-691, September 1998), which is incorporated herein by reference. The controller may additionally, or alternatively, be configured to compute a speed curve associated with the stored focus distance values. In particular, a speed curve representing approximate speeds of the medical instrument which are associated with the stored focus distance values may be computed. Such speed curve may facilitate identification of focus distance values that are &#x201c;stable&#x201d;, or associated with dwell periods during which the associated speed falls below a given threshold.</p>
<p id="p-0099" num="0098">The speed values that are approximated numerically may be stored in association with the respective focus distance values. In operation <b>708</b>, the controller retrieves a first focus distance value that is associated with a speed which is below a predefined threshold speed. In particular, the controller retrieves the most recent such focus distance value. In some embodiments, the predefined threshold speed may be 0.1 meter per second. The controller may retrieve from a data buffer the focus distance value with the most recent timestamp and which is associated with a speed that falls below the predefined threshold.</p>
<p id="p-0100" num="0099">Upon retrieving the first focus distance value, the controller automatically updates a focus distance of the camera to the retrieved value, in operation <b>710</b>. That is, the focus distance of the camera may be automatically set to the first focus distance value in order to recover continuous auto-focus and for the focus to track to the surgical site.</p>
<p id="p-0101" num="0100">The various embodiments presented above are merely examples and are in no way meant to limit the scope of this application. Variations of the innovations described herein will be apparent to persons of ordinary skill in the art, such variations being within the intended scope of the present application. In particular, features from one or more of the above-described example embodiments may be selected to create alternative example embodiments including a sub-combination of features which may not be explicitly described above. In addition, features from one or more of the above-described example embodiments may be selected and combined to create alternative example embodiments including a combination of features which may not be explicitly described above. Features suitable for such combinations and sub-combinations would be readily apparent to persons skilled in the art upon review of the present application as a whole. The subject matter described herein and in the recited claims intends to cover and embrace all suitable changes in technology.</p>
<?detailed-description description="Detailed Description" end="tail"?>
</description>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text><b>1</b>. A processor-implemented method for performing auto-focus in a camera, the method comprising:
<claim-text>receiving a first user input for triggering a change between a plurality of auto-focus modes; and</claim-text>
<claim-text>in response to receiving the first user input:
<claim-text>determining that a focus point defined by a medical instrument is within a field of view of the camera; and</claim-text>
<claim-text>performing a single auto-focus operation for focusing on the focus point defined by the medical instrument.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the single auto-focus operation comprises updating a focus distance of the camera to a first focus distance value.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>determining that a continuous auto-focus mode for the camera is enabled prior to receiving the first user input,</claim-text>
</claim-text>
<claim-text>wherein performing the single auto-focus operation comprises switching from the continuous auto-focus mode to a single auto-focus mode.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first user input comprises input on a user interface.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first user input comprises voice command.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first user input comprises confirmation input for activation of a foot pedal.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>receiving a second user input for enabling a continuous auto-focus mode for the camera; and</claim-text>
<claim-text>in response to receiving the second user input, causing the camera to switch from the single auto-focus mode to the continuous auto-focus mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text><b>8</b>. A navigation system to support a medical procedure, the navigation system comprising:
<claim-text>a tracking system for tracking a position of a medical instrument;</claim-text>
<claim-text>a surgical camera for imaging a target surgical site; and</claim-text>
<claim-text>a processor coupled to the tracking system and the surgical camera, the processor being configured to:
<claim-text>receive a first user input for triggering a change between a plurality of auto-focus modes; and</claim-text>
<claim-text>in response to receiving the first user input:
<claim-text>determine that a focus point defined by a medical instrument is within a field of view of the camera; and</claim-text>
<claim-text>perform a single auto-focus operation for focusing on the focus point defined by the medical instrument.</claim-text>
</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text><b>9</b>. The navigation system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein performing the single auto-focus operation comprises updating a focus distance of the camera to a first focus distance value.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text><b>10</b>. The navigation system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the processor is further configured to:
<claim-text>determine that a continuous auto-focus mode for the camera is enabled prior to receiving the first user input,</claim-text>
</claim-text>
<claim-text>wherein performing the single auto-focus operation comprises switching from the continuous auto-focus mode to a single auto-focus mode.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text><b>11</b>. The navigation system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first user input comprises input on a user interface.</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text><b>12</b>. The navigation system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first user input comprises voice command.</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text><b>13</b>. The navigation system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the first user input comprises confirmation input for activation of a foot pedal.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text><b>14</b>. The navigation system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the processor is further configured to:
<claim-text>receive a second user input for enabling a continuous auto-focus mode for the camera; and</claim-text>
<claim-text>in response to receiving the second user input, cause the camera to switch from the single auto-focus mode to the continuous auto-focus mode.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text><b>15</b>. A non-transitory processor-readable storage medium storing processor-executable instructions that, when executed by a processor, are to cause the processor to:
<claim-text>receive a first user input for triggering a change between a plurality of auto-focus modes; and</claim-text>
<claim-text>in response to receiving the first user input:
<claim-text>determine that a focus point defined by a medical instrument is within a field of view of the camera; and</claim-text>
<claim-text>perform a single auto-focus operation for focusing on the focus point defined by the medical instrument.</claim-text>
</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text><b>16</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein performing the single auto-focus operation comprises updating a focus distance of the camera to a first focus distance value.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text><b>17</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions, when executed, are to further cause the processor to:
<claim-text>determine that a continuous auto-focus mode for the camera is enabled prior to receiving the first user input,</claim-text>
</claim-text>
<claim-text>wherein performing the single auto-focus operation comprises switching from the continuous auto-focus mode to a single auto-focus mode.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text><b>18</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first user input comprises confirmation input for activation of a foot pedal.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text><b>19</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first user input comprises voice command.</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text><b>20</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions, when executed, are to further cause the processor to:
<claim-text>receive a second user input for enabling a continuous auto-focus mode for the camera; and</claim-text>
<claim-text>in response to receiving the second user input, cause the camera to switch from the single auto-focus mode to the continuous auto-focus mode.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-application>
