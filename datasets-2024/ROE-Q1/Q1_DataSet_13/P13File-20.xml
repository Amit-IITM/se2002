<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]>
<us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230225635A1-20230720.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20230710" date-publ="20230720">
<us-bibliographic-data-application lang="EN" country="US">
<publication-reference>
<document-id>
<country>US</country>
<doc-number>20230225635</doc-number>
<kind>A1</kind>
<date>20230720</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>18125896</doc-number>
<date>20230324</date>
</document-id>
</application-reference>
<us-application-series-code>18</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>11</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>09</class>
<subclass>B</subclass>
<main-group>19</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>08</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>71</main-group>
<subgroup>06</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>24</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>20</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>18</main-group>
<subgroup>214</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>774</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>40</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>1128</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>24</main-group>
<subgroup>0062</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>24</main-group>
<subgroup>0075</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>71</main-group>
<subgroup>0622</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>71</main-group>
<subgroup>0669</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20230101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>18</main-group>
<subgroup>214</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>N</subclass>
<main-group>3</main-group>
<subgroup>08</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>774</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>20</main-group>
<subgroup>46</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>40</main-group>
<subgroup>23</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>09</class>
<subclass>B</subclass>
<main-group>19</main-group>
<subgroup>0038</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>69</main-group>
<subgroup>0071</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>2071</main-group>
<subgroup>063</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>2071</main-group>
<subgroup>0694</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>63</class>
<subclass>B</subclass>
<main-group>2220</main-group>
<subgroup>806</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<invention-title id="yoroxbb2j5e6y">METHODS AND SYSTEMS FOR FACILITATING INTERACTIVE TRAINING OF BODY-EYE COORDINATION AND REACTION TIME</invention-title>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>17179435</doc-number>
<date>20210219</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>11642047</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>18125896</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>16792190</doc-number>
<date>20200215</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>10930172</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>17179435</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>16555812</doc-number>
<date>20190829</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>10600334</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>16792190</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>62778244</doc-number>
<date>20181211</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>NEX Team Inc.</orgname>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="00" designation="us-only">
<addressbook>
<last-name>Zhang</last-name>
<first-name>Qi</first-name>
<address>
<city>Hong Kong</city>
<country>HK</country>
</address>
</addressbook>
</inventor>
<inventor sequence="01" designation="us-only">
<addressbook>
<last-name>Chan</last-name>
<first-name>Wing Hung</first-name>
<address>
<city>Hong Kong</city>
<country>HK</country>
</address>
</addressbook>
</inventor>
<inventor sequence="02" designation="us-only">
<addressbook>
<last-name>Mollet</last-name>
<first-name>Arron</first-name>
<address>
<city>San Jose</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="03" designation="us-only">
<addressbook>
<last-name>Lee</last-name>
<first-name>Keng Fai</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
</us-bibliographic-data-application>
<abstract id="abstract">
<p id="p-0001" num="0000">A computer implemented method for facilitating training of body-eye coordination using a computing device having access to a camera is disclosed. The method includes receiving a training video of a player from the camera; superimposing a visual cue onto the training video; extracting a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video; determining whether the player has responded to the visual cue by analyzing the body posture flow of the player; and generating a feedback to the player in response to determining that the player has responded to the visual cue. Multi-player embodiments of the present invention are also disclosed.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="211.24mm" wi="170.10mm" file="US20230225635A1-20230720-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="213.44mm" wi="149.27mm" file="US20230225635A1-20230720-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="208.36mm" wi="145.97mm" file="US20230225635A1-20230720-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="217.09mm" wi="157.31mm" file="US20230225635A1-20230720-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="189.31mm" wi="159.09mm" file="US20230225635A1-20230720-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="131.57mm" wi="140.80mm" file="US20230225635A1-20230720-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="158.24mm" wi="151.47mm" file="US20230225635A1-20230720-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="152.65mm" wi="152.65mm" file="US20230225635A1-20230720-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="214.63mm" wi="141.48mm" file="US20230225635A1-20230720-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="186.86mm" wi="153.67mm" file="US20230225635A1-20230720-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="226.40mm" wi="159.60mm" file="US20230225635A1-20230720-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="160.87mm" wi="162.05mm" file="US20230225635A1-20230720-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="133.69mm" wi="141.65mm" file="US20230225635A1-20230720-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="208.53mm" wi="135.13mm" file="US20230225635A1-20230720-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="179.15mm" wi="159.43mm" file="US20230225635A1-20230720-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="123.36mm" wi="133.01mm" file="US20230225635A1-20230720-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="212.09mm" wi="145.03mm" file="US20230225635A1-20230720-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="186.69mm" wi="92.54mm" file="US20230225635A1-20230720-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="116.84mm" wi="142.41mm" file="US20230225635A1-20230720-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="200.32mm" wi="146.73mm" file="US20230225635A1-20230720-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="206.42mm" wi="146.56mm" file="US20230225635A1-20230720-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="205.74mm" wi="145.71mm" file="US20230225635A1-20230720-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="115.32mm" wi="142.16mm" file="US20230225635A1-20230720-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="167.13mm" wi="90.42mm" file="US20230225635A1-20230720-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="170.94mm" wi="90.17mm" file="US20230225635A1-20230720-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="170.86mm" wi="86.28mm" file="US20230225635A1-20230720-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="168.06mm" wi="86.11mm" file="US20230225635A1-20230720-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="119.97mm" wi="142.75mm" file="US20230225635A1-20230720-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?>
<heading level="1" id="h-0001">REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">If an Application Data Sheet (ADS) has been filed on the filing date of this application, it is incorporated by reference herein. Any applications claimed on the ADS for priority under 35 U.S.C. &#xa7;&#xa7;119, 120, 121, or 365(c), and any and all parent, grandparent, great-grandparent, etc. applications of such applications, are also incorporated by reference, including any priority claims made in those applications and any material incorporated by reference, to the extent such subject matter is not inconsistent herewith.</p>
<p id="p-0003" num="0002">This application is also related to non-provisional U.S. Serial No. 16/109,923, filed on Aug. 23, 2018, entitled &#x201c;<i>Methods and Systems for Ball Game Analytics with a Mobile Device</i>&#x201d; (Docket No. NEX-1001), and is also related to U.S. Serial No. 16/445,893, filed on Jun. 19, 2019, entitled &#x201c;<i>Remote Multiplayer Interactive Physical Gaming with Mobile Computing Devices</i>&#x201d; (Docket No. NEX-1003); the entire disclosures of all referenced applications are hereby incorporated by reference in their entireties herein.</p>
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?>
<?summary-of-invention description="Summary of Invention" end="lead"?>
<heading level="1" id="h-0002">NOTICE OF COPYRIGHTS AND TRADEDRESS</heading>
<p id="p-0004" num="0003">A portion of the disclosure of this patent document contains material which is subject to copyright protection. This patent document may show and/or describe matter which is or may become tradedress of the owner. The copyright and tradedress owner has no objection to the facsimile reproduction by anyone of the patent disclosure as it appears in the U.S. Patent and Trademark Office files or records, but otherwise reserves all copyright and tradedress rights whatsoever.</p>
<heading level="1" id="h-0003">FIELD OF THE INVENTION</heading>
<p id="p-0005" num="0004">Embodiments of the present invention are in the field of sports training, and pertain particularly to methods and systems for providing interactive virtual coaching for performance training with a mobile device, the mobile device having one or more cameras for video capture.</p>
<heading level="1" id="h-0004">BACKGROUND OF THE INVENTION</heading>
<p id="p-0006" num="0005">The statements in this section may serve as a background to help understand the invention and its application and uses, but may not constitute prior art.</p>
<p id="p-0007" num="0006">In both professional and amateur sports, good coaching is essential for the development of technical, tactical, physical and drill skills, and personalized and targeted training often help improve fitness and performance in a particular sport while reducing chances of injury. Advances in modern computing and networking technology have allowed virtual access to experienced coaches and effective performance training programs, yet existing digital coaching and training applications are either passive in nature, where users or players are provided with instructions or drilling plans only, or function in an offline manner, where video recordings of players in action can be replayed, analyzed, and annotated, manually by a coach or the player, after a training or drill session is completed.</p>
<p id="p-0008" num="0007">More recently, real-time analytics systems have been developed to provide quantitative and qualitative game and player analytics, with uses in broadcasting, game strategizing, and team management, yet mass mainstream usage of such systems by individual players for customized performance training is still complex and expensive. Real-time tracking technology based on image recognition often requires use of multiple high-definition cameras mounted on top of a game area or play field for capturing visual data from multiple camera arrays positioned at multiple perspectives, calibration for different environments, and massive processing power in high-end desktop and/or server-grade hardware to analyze data from the camera arrays. Accurate tracking of player motion and forms, and real-time automated analysis require vast computational resources that hinder implementations with low-cost, general-purpose hardware with small form factors.</p>
<p id="p-0009" num="0008">Therefore, it would be an advancement in the state of the art to allow interactive, real-time virtual coaching and performance training, including facilitating training of body-eye coordination and reaction time, using just a mobile device by utilizing video data captured from a camera on the mobile device.</p>
<p id="p-0010" num="0009">It is against this background that various embodiments of the present invention were developed.</p>
<heading level="1" id="h-0005">BRIEF SUMMARY OF THE INVENTION</heading>
<p id="p-0011" num="0010">Some embodiments of the present invention include methods, systems, and apparatuses for providing virtual coaching of a physical activity using a computing device.</p>
<p id="p-0012" num="0011">In particular, one embodiment is a computer implemented method for facilitating training of body-eye coordination using a computing device having access to a camera. The method includes receiving a training video of a player from the camera; superimposing a visual cue onto the training video; extracting a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video; determining whether the player has responded to the visual cue by analyzing the body posture flow of the player; and generating a feedback to the player in response to determining that the player has responded to the visual cue.</p>
<p id="p-0013" num="0012">In some embodiments, the visual cue is a symbol superimposed onto the training video in a frame of the training video.</p>
<p id="p-0014" num="0013">In some embodiments, the determining whether the player has responded to the visual cue comprises determining a player movement to virtually touch the symbol in the frame with a body part.</p>
<p id="p-0015" num="0014">In some embodiments, the determining whether the player has responded to the visual cue comprises determining whether the player has virtually touched the symbol in the frame with a sports equipment object.</p>
<p id="p-0016" num="0015">In some embodiments, the determining whether the player has responded to the visual cue comprises determining whether the player has performed a predetermined sequence of movements.</p>
<p id="p-0017" num="0016">In some embodiments, the extraction of the posture flow of the player from the training video comprises using a Convolutional Neural Network (CNN) module, by detecting one or more key points of the player in a frame of the training video, where the CNN module has been trained using one or more prior videos.</p>
<p id="p-0018" num="0017">In some embodiments, the training video comprises a dribbling activity performed by the player, and the superimposing the training video with the visual cue is in response to determining that the player has dribbled for a predetermined number of times before a first time instant.</p>
<p id="p-0019" num="0018">In some embodiments, the method also includes waiting for a period of wait time before the superimposing the training video with the visual cue, where a duration of the wait time is based on a detected player action during the wait time.</p>
<p id="p-0020" num="0019">In some embodiments, the training video comprises a juggling activity performed by the player, where the superimposing the training video with the visual cue is in response to determining that the player has juggled for a predetermined number of times.</p>
<p id="p-0021" num="0020">In some embodiments, the method also includes generating a training statistic for the player based on the training video, where the training statistic includes at least a reaction speed.</p>
<p id="p-0022" num="0021">In some embodiments, the method also includes generating a training statistic for the player based on the training video, where the training statistic comprises a first current statistic that is associated with the training video and a second historical statistic that is associated with one or more historical training sessions associated with the player.</p>
<p id="p-0023" num="0022">In another aspect, another embodiment is a non-transitory computer-readable storage medium storing executable instructions, the executable instructions when executed by a hardware processor causing the hardware processor to execute a process for facilitating training of body-eye coordination. The process when executed by the hardware processor includes steps to receive a training video of a player from a camera; superimpose a visual cue onto the training video; extract a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video; determine whether the player has responded to the visual cue by analyzing the body posture flow of the player; and generate a feedback to the player in response to determining that the player has responded to the visual cue.</p>
<p id="p-0024" num="0023">In some embodiments, the visual cue is a symbol superimposed onto the training video in a frame of the training video.</p>
<p id="p-0025" num="0024">In some embodiments, to determine whether the player has responded to the visual cue comprises determining a player movement to virtually touch the visual cue with a body part.</p>
<p id="p-0026" num="0025">In some embodiments, to determine whether the player has responded to the visual cue comprises determining whether the player has virtually touched the visual cue with a sports equipment object.</p>
<p id="p-0027" num="0026">In some embodiments, to determine whether the player has responded to the visual cue comprises determining whether the player has performed a predetermined sequence of movements.</p>
<p id="p-0028" num="0027">In some embodiments, the extraction of the posture flow of the player from the training video comprises using a Convolutional Neural Network (CNN) module to detect one or more key points of the player in a frame of the training video, where the CNN module has been trained using one or more prior videos.</p>
<p id="p-0029" num="0028">In some embodiments, the training video comprises a dribbling activity performed by the player, where the superimposing the training video with the visual cue is in response to determining that the player has dribbled for a predetermined number of times before a first time instant.</p>
<p id="p-0030" num="0029">In some embodiments, the non-transitory storage medium further comprises executable instructions to wait for a period of wait time before the superimposing the training video with the visual cue, wherein a duration of the wait time is based on a detected player action during the wait time.</p>
<p id="p-0031" num="0030">In another aspect, another embodiment is a computing device having access to a camera, a hardware processor, and a non-transitory storage medium, the non-transitory storage medium storing executable instructions, the executable instructions when executed by the hardware processor causing the hardware processor to execute a process for facilitating training of body-eye coordination. The process when executed by the hardware processor includes steps to receive a training video of a player from the camera; superimpose a visual cue onto the training video; extract a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video; determine whether the player has responded to the visual cue by analyzing the body posture flow of the player; and generate a feedback to the player in response to determining that the player has responded to the visual cue.</p>
<p id="p-0032" num="0031">Yet other aspects of the present invention include methods, processes, and algorithms comprising the steps described herein, and also include the processes and modes of operation of the systems and servers described herein. Yet other aspects and embodiments of the present invention will become apparent from the detailed description of the invention when read in conjunction with the attached drawings.</p>
<?summary-of-invention description="Summary of Invention" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading level="1" id="h-0006">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0033" num="0032">Embodiments of the present invention described herein are exemplary, and not restrictive. Embodiments will now be described, by way of examples, with reference to the accompanying drawings, in which:</p>
<p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is an exemplary setup for enabling interactive virtual coaching and training with a mobile computing device, according to some embodiments of the present invention;</p>
<p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. <b>1</b>B and <b>1</b>C</figref> show respective diagrams representing an exemplary application running on the mobile computing device in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, while the user performs a training activity, according to some embodiments of the present invention;</p>
<p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> is an architectural overview of a mobile computing device-based system for interactive virtual coaching and training, according to some embodiments of the present invention;</p>
<p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an exemplary schematic diagram of a user computing entity for implementing an interactive virtual coaching and training system, according to exemplary embodiments of the present invention;</p>
<p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an exemplary schematic diagram of a management computing entity for implementing an interactive virtual coaching and training system, according to exemplary embodiments of the present invention;</p>
<p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an illustrative block diagram of a convolutional neural network (CNN) for image analysis, according to exemplary embodiments of the present invention;</p>
<p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an illustrative block diagram for a machine learning algorithm, according to exemplary embodiments of the present invention;</p>
<p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an illustrative flow diagram for training a machine learning algorithm, according to exemplary embodiments of the present invention;</p>
<p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram illustrating an exemplary NEX platform, according to exemplary embodiments of the present invention;</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a block diagram of an exemplary neural network for pose estimation, according to exemplary embodiments of the present invention;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a detailed block diagram illustrating an exemplary Feature Block, according to exemplary embodiments of the present invention;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is a detailed block diagram illustrating an exemplary separable convolutional neural network layer, according to exemplary embodiments of the present invention;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>8</b>D</figref> is a detailed block diagram illustrating an exemplary Initial Prediction Block, according to exemplary embodiments of the present invention;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>8</b>E</figref> is a detailed block diagram illustrating an exemplary Refine Block, according to exemplary embodiments of the present invention;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> is a block diagram of an exemplary neural network for ball detection, according to one embodiment of the present invention;</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> is a detailed block diagram illustrating an exemplary Modified SSDLite Block, according to one embodiment of the present invention;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a flowchart illustrating exemplary operations of a user device and associated algorithms for determining one or more statistics from a video of a training session, according to some embodiments of present invention;</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a flowchart illustrating exemplary operations for interactive virtual coaching and training, according to some embodiments of the present invention;</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> shows a diagram representing an exemplary application running on a mobile computing device, in which an exemplary dribbling workout may be selected, according to some embodiments of the present invention;</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> shows a diagram representing an exemplary application running on a mobile computing device, in which the mobile computing device may be oriented and calibrated for interactive virtual coaching, according to some embodiments of the present invention;</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. <b>12</b>C and <b>12</b>D</figref> show respective diagrams representing an exemplary application running on a mobile computing device, in which an interactive workout session may be started with posture control, according to some embodiments of the present invention;</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>13</b>A, <b>13</b>B, <b>13</b>C, and <b>13</b>D</figref> show respective diagrams representing an exemplary virtual coaching and training application running on a mobile computing device, which facilitates an interactive dribbling session, according to some embodiments of the present invention;</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram representing an exemplary interactive virtual coaching and training application running on a mobile computing device, in which user statistics are collected from an interactive dribbling session facilitated by the application, according to some embodiments of the present invention;</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIGS. <b>15</b>A and <b>15</b>B</figref> show respective diagrams representing an exemplary virtual coaching and training application running on a mobile computing device, in which player statistics and global rankings are displayed for interactive dribbling sessions facilitated by the application, according to some embodiments of the present invention;</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a diagram representing an exemplary interactive virtual coaching and training application running on a mobile computing device, in which a video review of highlights of the user&#x2019;s performance may be provided in a feed, according to some embodiments of the present invention;</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows a diagram representing an exemplary interactive virtual coaching and training application running on a mobile computing device, in which player statistics are provided in a feed, according to some embodiments of the present invention;</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram representing an exemplary interactive virtual coaching and training application running on a mobile computing device, which facilitates another exemplary interactive training activity, according to some embodiments of the present invention.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?detailed-description description="Detailed Description" end="lead"?>
<heading level="1" id="h-0007">DETAILED DESCRIPTION OF THE INVENTION</heading>
<p id="p-0061" num="0060">In the following description, for purposes of explanation, numerous specific details are set forth in order to provide a thorough understanding of the invention. It will be apparent, however, to one skilled in the art that the invention can be practiced without these specific details. In other instances, structures, devices, activities, and methods are shown using schematics, use cases, and/or flow diagrams in order to avoid obscuring the invention. Although the following description contains many specifics for the purposes of illustration, anyone skilled in the art will appreciate that many variations and/or alterations to suggested details are within the scope of the present invention. Similarly, although many of the features of the present invention are described in terms of each other, or in conjunction with each other, one skilled in the art will appreciate that many of these features can be provided independently of other features. Accordingly, this description of the invention is set forth without any loss of generality to, and without imposing limitations upon the invention.</p>
<p id="p-0062" num="0061">NEX, NEX TEAM, and HOMECOURT are trademark names carrying embodiments of the present invention, and hence, the aforementioned trademark names may be interchangeably used in the specification and drawing to refer to the products/services offered by embodiments of the present invention. The term NEX, NEX TEAM, or HOMECOURT may be used in this specification to describe the overall training video capturing, interactive coaching, and analytics generation platform, as well as the company providing said platform. With reference to the figures, embodiments of the present invention are now described in detail.</p>
<heading level="1" id="h-0008">Introduction and Overview</heading>
<p id="p-0063" num="0062">Broadly, embodiments of the present invention relate to virtual coaching and pertain particularly to methods and systems for interactive, real-time virtual coaching and performance training for physical activities and sport games, using a computing device having access to one or more cameras, by deploying artificial-intelligence (AI)-based computer vision techniques.</p>
<p id="p-0064" num="0063">It would be understood by persons of ordinary skill in the art that training or performance training activities discussed in this disclosure broadly refer to any physical exercise, workout, drill, or practice that improve a user&#x2019;s fitness and skill levels to better his or her ability to perform a given physical activity or sport. Training activities thus disclosed can maintain, condition, correct, restore, strengthen, or improve the physical ability, power, agility, flexibility, speed, quickness, reaction, endurance, and other physical and technical skills necessary for a physical activity or sport. Such a physical activity or sport may be competitive or non-competitive in nature, with or without specific goals or challenges, and may or may not be scored according to specific rules. A user of the system as disclosed herein is referred to as a player, including in non-competitive activities such as rehabilitative physical therapies and occupational therapies. A training session may involve one or more individual players. During a training session, individual skills such as power, speed, agility, flexibility, posture, balance, core strength, upper and lower-body strength, rhythm, swing, stroke, flick, running, stopping, dribbling, juggling, passing, catching, throwing, smashing, tackling, shooting, jumping, sprinting, serving, and goalkeeping may be isolated, broken down into specific movements, and worked upon. Such skills may be inter-dependent. For example, better core strength may lead to better stance and balance, and better body-eye and hand-eye coordination may lead to faster speed, shorter stopping time, and better control of a ball. Some training activities are tailored for specific demands of a particular sport. Embodiments of the present invention may be used for interactive virtual coaching in ball sports as well as other types of sports or physical activities, including but not limited to, basketball, soccer, baseball, football, hockey, tennis, badminton, juggling, archery, softball, volleyball, boxing, canoeing, kayaking, climbing, cycling, diving, equestrian, fencing, golf, gymnastics, handball, judo, karate, modern pentathlon, roller sport, rowing, rugby, sailing, shooting, swimming, surfing, table tennis, taekwondo, track and field, triathlon, water polo, weightlifting, wrestling, squash, wakeboard, wushu, dancing, bowling, netball, cricket, lacrosse, running, jogging, yo-yo, foot bagging, hand sacking, slinky, tops, stone skipping, and many other types of sports, games, and other activities in a similar fashion.</p>
<p id="p-0065" num="0064">More specifically, in one aspect, embodiments of the virtual coaching system disclosed herein relate to providing audio or visual instructions, prompts, triggers, or cues for individual training activities or movements, tracking and analyzing player movements using one or more computer vision algorithms running on a mobile computing device such as a smartphone, tablet, or laptop, and optionally providing audio or visual feedback based on the movement analysis, in real-time or near real-time. Embodiments of the present invention may enable the mobile computing device to determine the level of body-eye coordination, hand-eye coordination, reaction time, and the like, from an analysis of training videos taken of the player during a training exercise.</p>
<p id="p-0066" num="0065">A key feature of the present invention is the novel design of mobile AI-based computer vision techniques, to analyze player movements, generate player analytics and feedback, and facilitate interactive virtual coaching and training. Unlike existing computer vision-based monitoring systems that require dedicated sensor equipment such as high-resolution cameras mounted on top of a ball court or sensing bars mounted on top of a TV, embodiments of the present invention allow users to perform real-time monitoring, analysis and interactive training with a mobile device by utilizing simple on-device cameras and general-purpose processors. Innovative and efficient object detection and posture tracking techniques enable the analysis of images and/or video recordings captured by one or more on-device cameras to determine user or player analytics including movement patterns, body postures, and optionally other non-human objects such as balls present in the training area. In various embodiments, computer vision techniques such as image registration, motion detection, background subtraction, objection tracking, 3D-reconstruction techniques, cluster analysis techniques, camera calibration techniques such as camera pose estimation and sensor fusion, and modern machine learning techniques such as convolutional neural network (CNN), may be selectively combined to perform high accuracy analysis in real-time on the mobile device. The limited computational resources in a mobile device present a unique challenge. For instance, a smartphone&#x2019;s limited CPU processing power is heat-sensitive. A CPU clock rate may be reduced by the operating system (OS) whenever the phone heats up. Also, when a system consumes too much memory, the system or application running on the system may be terminated by the OS. In some embodiments, the amount of battery that the virtual coaching system consumes is controlled, otherwise the limited battery on a smartphone may not last a given duration (e.g., duration of a whole training session).</p>
<p id="p-0067" num="0066">Another key feature in this virtual coaching and performance training process is the simulation of external stimuli and the facilitation of user response or interaction through a user interface to complete some aspects of the physical training. An in-person partner or coach can provide instructions for a next set of drills, and often serves as an assistant in clocking time, providing targets and challenges, and giving changing orders at random time instances to train a player&#x2019;s reaction time. Embodiments of the present invention can simulate such external stimuli and track the player&#x2019;s physical reactions in response. That is, while the process of capturing a training video is passive without explicit user inputs, interactive virtual coaching and performance training involve active user interaction with an augmented virtual environment, through particular posture sequences and/or audio inputs, but without the use of wearable sensors or controls. For example, a user or player may be required to jump to a certain height for a given number of times to achieve a training goal. The desired height may be simulated as a virtual target line superimposed onto the training video, and interactivity may derive from the player trying to virtually touch the line with his hand or top of his head in the image plane of the training video.</p>
<p id="p-0068" num="0067">Similarly, other forms of external stimulation may be generated by embodiments of the present invention, including target objects and goals. Examples include but are not limited to, images indicative of certain movements, such as blocking movements in the game of basketball, noise effects similar to the sounds generated by a crowd of observers, and the like. Such external stimuli, goal, or cue may be designed to adjust a difficulty level of the training session, to tailor embodiments of the present invention to all skill levels, from beginner through advanced.</p>
<p id="p-0069" num="0068">In another aspect, embodiments of the virtual coaching and training system as disclosed herein relate to monitoring a training exercise using the mobile computing device, determining various player analytics, performance metrics, or statistics, and tracking of such data over time to generate a historical database, thereby allowing for player performance tracking over time. This and various other aspects of the disclosure may further serve as a virtual coach for users in extracting statistics associated with various performed tasks and activities, presenting historical trends, and providing feedback for the players to improve their skills.</p>
<p id="p-0070" num="0069">In general, the term analytics refers to meaningful patterns, knowledge, and information from data or statistics. In this disclosure, user or player analytics refer to quantitative and qualitative characterizations of player actions during one or more training sessions. For example, for a ball sport game, player analytics include but are not limited to, shot types, shot make/miss, shot score, player movement patterns, player moving speed, moving direction, reaction time, jump height and type, jump foot, landing foot, shot release time and angle, and posture statistics such as body bend angle, body rotation, leg bend ratio, and leg power. An analytic may be both a shot analytic specific to a given shot attempt, and a player analytic specific to an identified player. In addition, game analytics generally refers to statistical analytics generated from player analytics and optionally shot analytics over the duration of a game, and team analytics refers to analytics aggregated across players of a team.</p>
<p id="p-0071" num="0070">Player analytics or metrics are specific to different types of sports. In an illustrative basketball dribbling training example, one or more users may be able to perform a dribbling workout, and an application on a mobile computing device implementing an embodiment of the present invention may be configured to monitor the dribbling workout to determine dribble speed, dribble accuracy, ability of the user to make specific movements during dribbling, combinations thereof, and/or the like.</p>
<p id="p-0072" num="0071">As noted, the physical activities and sports being trained for include traditional physical games played in the real world, such as on a court, field, trail, and the like. Interactivity with the virtual world provides training opportunities for specific skills and techniques. In some embodiments, another level of interactivity may exist among one or more users linked through a network, where multiple users or players may train together at the same time, and training results may be compared across time.</p>
<heading level="1" id="h-0009">Exemplary Embodiment for Virtual Coaching and Training for Body-Eye Coordination</heading>
<p id="p-0073" num="0072">As an illustrative embodiment, <figref idref="DRAWINGS">FIGS. <b>1</b>A to <b>1</b>D</figref> show respective setup and architectural overview of a NEX system for virtual coaching and performance training in body-eye coordination and reaction time for basketball dribbling.</p>
<p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is an exemplary setup <b>100</b> for enabling interactive virtual coaching and performance training with a mobile computing device, according to some embodiments of the present invention. A user or player <b>110</b> trains in front of a mobile computing device <b>115</b> secured on a mounting apparatus. The optional mounting apparatus may be a tripod or a kickstand. During a training session, an embodiment of the NEX system implemented on mobile computing device <b>115</b> may provide audio or visual instructions, goals, or cues, for user <b>110</b> to perform a next movement or set of movements. Mobile computing device <b>115</b> may comprise one or more cameras for capturing a training video of user <b>110</b>, for example using a front-facing camera, for computer vision-based, real-time, near real-time, or off-line posture analysis. The captured training video may be presented to the user through a display screen on the mobile computing device, with or without superimposed graphical or textual instructions, cues, analytics, statistics, or other visual training information add-ons. In some embodiments, mobile computing device <b>115</b> may be coupled to a larger external display, through a wireless or wired connection, such that the user may see the captured training video and optional training information with better clarity.</p>
<p id="p-0075" num="0074">Correspondingly, <figref idref="DRAWINGS">FIGS. <b>1</b>B and <b>1</b>C</figref> show respective diagrams <b>120</b> and <b>140</b> of an exemplary application running on the mobile computing device <b>115</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, while the user performs a dribble training activity, according to some embodiments of the present invention. While user <b>110</b> is dribbling ball <b>112</b>, a visual cue <b>130</b> appears on the screen at a random location as generated by the NEX system for user <b>110</b> to touch with his non-dribbling hand. Visual cue <b>130</b> is presented for a cue period, which may have a pre-determined duration, or may terminate once it is determined that the player has touched visual cue <b>130</b>. Depending on how fast the player touches cue <b>130</b> from when cue <b>130</b> first appears, a point score <b>150</b> may be given by the NEX system, displayed to user <b>110</b> as a feedback, and be added to a total score <b>122</b>. If user <b>110</b> does not touch visual cue <b>130</b> successfully within the pre-determined cue period, a zero point score may be given. In some embodiments, a training period of three minutes may be counted down through a timer <b>124</b>.</p>
<p id="p-0076" num="0075">In this basketball training example, a baseline training activity of ball dribbling is required, whereby the user tries to hit cue targets presented to the user at the user device with his non-dribbling hand as the targets appear on the display. Points may be awarded to the user whenever the user successfully virtually touches a given target without stopping the dribbling activity. Further, more points may be awarded to the user for virtually touching the targets quicker while dribbling. In some embodiments, a termination symbol such as an &#x201c;X&#x201d; may be displayed to allow the user to abort the training session.</p>
<p id="p-0077" num="0076">In various embodiments, the interactive and reactionary nature of a digital virtual workout may serve to address various deficiencies of training without the benefit of a partner or coach, which may further allow users to practice in a way that more effectively translates to game situations. In one embodiment, altering the scope of the user&#x2019;s attention, for example, having the user concentrate on targets rather than on dribbling dynamics, may serve to better mimic one or more game-like situations, such as surveying the floor while dribbling. Further, in various embodiments, the graphical user interface (GUI) presented to the user at the user device may be relatively simple, have familiar elements of design, and may represent a futuristic approach to displaying such analyses and providing feedback and training routines.</p>
<p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>1</b>D</figref> is an architectural overview <b>160</b> of a mobile computing device-based system for interactive virtual coaching and performance training, according to some embodiments of the present invention. A NEX system <b>180</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> may be implemented on mobile device such as <b>115</b> to capture player actions in a game area. An input training video <b>170</b> thus captured by mobile computing device <b>115</b> is analyzed by NEX system <b>180</b> using one or more computer vision algorithms, which may also be implemented on mobile computing device <b>115</b>. Player motion, movement, or posture, and optionally training objects present in the video may be analyzed at a step <b>182</b> to initiate the training session, after the mobile computing device or the captured training video has been calibrated or adjusted for levelness, distance from the player, brightness under a current lighting condition, and other similar environmental parameters. Next, the training video may be augmented with a visual cue, where a graphical or textual symbol is superimposed onto the training video, at a step <b>184</b>. At step <b>186</b>, player posture flows and object flows may be further detected and analyzed, to determine how the player has responded to the presented visual cue. In this particular example, a player is prompted to virtually touch the visual cue with his or her non-dribbling hand. Based on user responses determined through the user&#x2019;s posture flow, NEX system <b>180</b> generates a feedback to the user at a step <b>188</b>, and optionally generate one or more player analytics. In some embodiments, training data <b>192</b> from one or more other players, such as training video recordings and player analytics, may be downloaded from a NEX server <b>195</b> via a network <b>190</b>. While not shown explicitly here, NEX server <b>195</b> may comprise one or more databases for storing training videos and player analytics, and one or more processors for generating live or historical training statistics for participating users. Exemplary implementations for NEX server <b>195</b> are provided with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p>
<p id="p-0079" num="0078">In some embodiments, a convolutional neural network (CNN) may be applied to some or all frames of training video <b>170</b> to detect game objects such as basketballs and hockey pucks, as well as individual players and their postures. A tracking algorithm may be performed to track all detected balls and postures, where multiple balls or players may be present in each frame of the shot attempt video, to generate multiple ball flows and posture flows. In some embodiments, a flow refers to object instances from different frames. All object instances in the same flow may be considered the same object. In other words, for a ball or posture in a flow, all instances of the ball or posture in all frames of the video are identified as the same object. In various embodiments, object clustering or classification methods such as k-means, affinity propagation, density-based spatial clustering of applications with noise (DBSCAN) and/or k-nearest neighbors (KNN) may be applied to differentiate detected player images into multiple players.</p>
<p id="p-0080" num="0079">When a single player is being recorded, a single posture flow may be detected and associated with the player directly. When multiple players are being recorded, NEX system <b>180</b> may distinguish the players based on visual features such as jersey colors, or distinguishing facial or body features, and each player may register with NEX system <b>180</b> before the start of the training session by logging in such visual features. For example, in a single-device, multi-player, &#x201c;2-player reaction&#x201d; training session, the camera on the mobile computing device may capture sufficient training area to allow two players to train together, where the two players compete in responding to the visual cue. In the case where the player competes in virtually touching the visual cue, the player who virtually touches the visual cue first may be rewarded a positive point score, similar to the single-player case.</p>
<p id="p-0081" num="0080">To detect objects of interests such as court lines, balls and players from frames of the input video, one or more convolutional neural networks (CNN) may be applied. Each CNN module may be trained using one or more prior input videos. A CNN utilizes the process of convolution to capture the spatial and temporal dependencies in an image, and to extract features from the input video for object detection. Feature extraction in turn enables segmentation or identification of image areas representing these objects such as balls and players, and further analysis to determine player body postures. A ball moves through space, leading to changing size and location from video frame to video frame. A player also moves through space while handling the ball, leading to both changing locations, sizes, and body postures.</p>
<p id="p-0082" num="0081">In computer vision, pose or posture estimation is the task of identifying or detecting the position and orientation of an object in an image, relative to some coordinate system. This is generally formulated as the process of determining key point locations that describe the object. In the case of a ball, pose estimation may refer to determining the center and radius of the ball in the image plane. Hand pose estimation, on the other hand, is the process of determining finger joints and fingertips in a given image, where the whole hand is viewed as one object. Head pose estimation is the process of determining and analyzing facial features to obtain the 3D orientation of human head with respect to some reference point. Human pose estimation is the process of detecting major parts and joints of the body, such as head, torso, shoulder, ankle, knee, and wrist. In this disclosure, a &#x201c;player image&#x201d; refers to the image of a human player segmented from the input video, for example as defined by a bounding box; &#x201c;posture&#x201d; and &#x201c;pose&#x201d; are used interchangeably to refer to either a player image or a set of key points extracted from the player image to represent body pose or posture. In addition, instead of only determining whether an object such as a ball or a player is present in a given video frame, object detection or extraction in the present disclosure refers to determining the relative position, size, and/or pose of a ball, player, or other entities of interest.</p>
<p id="p-0083" num="0082">Once objects are detected or extracted from individual frames, object flows may be established by grouping detected objects along a timeline. Object movements across frames are continuous in the sense that object locations can only change in small increments from one video frame to the next. Thus, detected objects may be grouped based on location information into one or more object flows. For example, object flows may be established by computing a matching score for each object and existing object flow combination, and assigning objects to existing object flows with the highest matching score. At the beginning when no object flows yet exist, an initialization process may be performed based on an initial collection of a small number of objects, detected with high accuracy. In addition, a new flow may be created if the detected object does not match to any existing flows with a high score.</p>
<heading level="1" id="h-0010">Implementation Using Computer Program Products, Methods, and Computing Entities</heading>
<heading level="2" id="h-0011">Exemplary System Architecture</heading>
<p id="p-0084" num="0083">An exemplary embodiment of the present disclosure may include one or more user computing entities <b>200</b>, one or more networks, and one or more server or management computing entities <b>300</b>, as shown in <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>. Each of these components, entities, devices, systems, and similar words used herein interchangeably may be in direct or indirect communication with, for example, one another over the same or different wired or wireless networks. Additionally, while <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> illustrate the various system entities as separate, standalone entities, the various embodiments are not limited to this particular architecture.</p>
<heading level="2" id="h-0012">Exemplary User Computing Entity</heading>
<p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an exemplary schematic diagram of a user computing device for implementing a virtual coaching and performance training system, according to exemplary embodiments of the present invention. A user operates a user computing device <b>200</b> that includes one or more components as shown. As will be recognized, these architectures and descriptions are provided for exemplary purposes only and are not limiting to the various embodiments.</p>
<p id="p-0086" num="0085">In general, the terms device, system, computing entity, entity, and/or similar words used herein interchangeably may refer to, for example, one or more computers, computing entities, desktops, mobile phones, tablets, phablets, notebooks, laptops, distributed systems, gaming consoles (e.g., Xbox, Play Station, Wii), watches, glasses, key fobs, radio frequency identification (RFID) tags, ear pieces, scanners, cameras, wristbands, kiosks, input terminals, servers or server networks, blades, gateways, switches, processing devices, processing entities, set-top boxes, relays, routers, network access points, base stations, the like, and/or any combination of devices or entities adapted to perform the functions, operations, and/or processes described herein. Such functions, operations, and/or processes may include, for example, transmitting, receiving, retrieving, operating on, processing, displaying, storing, determining, creating, generating, generating for display, monitoring, evaluating, comparing, and/or similar terms used herein interchangeably. In various embodiments, these functions, operations, and/or processes can be performed on data, content, information, and/or similar terms used herein interchangeably. Furthermore, in embodiments of the present invention, user computing device <b>200</b> may be a mobile device, and may be operated by a user participating in an interactive physical training activity. On the other hand, a NEX server <b>195</b> may be implemented according to the exemplary schematic diagram shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, possibly in the cloud, and possibly with logically or physically distributed architectures.</p>
<p id="p-0087" num="0086">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the user computing entity <b>200</b> may include an antenna <b>270</b>, a radio transceiver <b>220</b>, and a processing unit <b>210</b> that provides signals to and receives signals from the transceiver. The signals provided to and received from the transceiver may include signaling information in accordance with air interface standards of applicable wireless systems. In this regard, the user computing entity <b>200</b> may be capable of operating with one or more air interface standards, communication protocols, modulation types, and access types. More particularly, the user computing entity <b>200</b> may operate in accordance with any of a number of wireless communication standards and protocols. In some embodiments, user computing entity <b>200</b> may operate in accordance with multiple wireless communication standards and protocols, such as 5G, UMTS, FDM, OFDM, TDM, TDMA, E-TDMA, GPRS, extended GPRS, CDMA, CDMA2000, 1xRTT, WCDMA, TD-SCDMA, GSM, LTE, LTE advanced, EDGE, E-UTRAN, EVDO, HSPA, HSDPA, MDM, DMT, Wi-Fi, Wi-Fi Direct, WiMAX, UWB, IR, NFC, ZigBee, Wibree, Bluetooth, and/or the like. Similarly, the user computing entity <b>200</b> may operate in accordance with multiple wired communication standards and protocols, via a network and communication interface <b>222</b>.</p>
<p id="p-0088" num="0087">Via these communication standards and protocols, the user computing entity <b>200</b> can communicate with various other computing entities using concepts such as Unstructured Supplementary Service Data (USSD), Short Message Service (SMS), Multimedia Messaging Service (MMS), Dual-Tone Multi-Frequency Signaling (DTMF), and/or Subscriber Identity Module Dialer (SIM dialer). User computing entity <b>200</b> can also download changes, add-ons, and updates, for instance, to its firmware, software (e.g., including executable instructions, applications, program modules), and operating system.</p>
<p id="p-0089" num="0088">In some implementations, processing unit <b>210</b> may be embodied in several different ways. For example, processing unit <b>210</b> may be embodied as one or more complex programmable logic devices (CPLDs), microprocessors, multi-core processors, coprocessing entities, application-specific instruction-set processors (ASIPs), microcontrollers, and/or controllers. Further, the processing unit may be embodied as one or more other processing devices or circuitry. The term circuitry may refer to an entirely hardware embodiment or a combination of hardware and computer program products. Thus, processing unit <b>210</b> may be embodied as integrated circuits, application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), programmable logic arrays (PLAs), hardware accelerators, other circuitry, and/or the like. As will therefore be understood, processing unit <b>210</b> may be configured for a particular use or configured to execute instructions stored in volatile or non-volatile media or otherwise accessible to the processing unit. As such, whether configured by hardware or computer program products, or by a combination thereof, processing unit <b>210</b> may be capable of performing steps or operations according to embodiments of the present invention when configured accordingly.</p>
<p id="p-0090" num="0089">In some embodiments, processing unit <b>210</b> may comprise a control unit <b>212</b> and a dedicated arithmetic logic unit <b>214</b> (ALU) to perform arithmetic and logic operations. In some embodiments, user computing entity <b>200</b> may optionally comprise a graphics processing unit <b>240</b> (GPU) for specialized image and video rendering tasks, and/or an artificial intelligence (AI) accelerator <b>242</b>, specialized for applications including artificial neural networks, machine vision, and machine learning. In some embodiments, processing unit <b>210</b> may be coupled with GPU <b>240</b> and/or AI accelerator <b>242</b> to distribute and coordinate processing tasks.</p>
<p id="p-0091" num="0090">In some embodiments, user computing entity <b>200</b> may include a user interface, comprising an input interface <b>250</b> and an output interface <b>252</b>, each coupled to processing unit <b>210</b>. User input interface <b>250</b> may comprise any of a number of devices or interfaces allowing the user computing entity <b>200</b> to receive data, such as a keypad (hard or soft), a touch display, a mic for voice/speech, and a camera for motion or posture interfaces. User output interface <b>252</b> may comprise any of a number of devices or interfaces allowing user computing entity <b>200</b> to provide information to a user, such as through the touch display, or a speaker for audio outputs. In some embodiments, output interface <b>252</b> may connect user computing entity <b>200</b> to an external loudspeaker or projector, for audio or visual output.</p>
<p id="p-0092" num="0091">User computing entity <b>200</b> may also include volatile and/or non-volatile storage or memory <b>230</b>, which can be embedded and/or may be removable. A non-volatile memory may be ROM, PROM, EPROM, EEPROM, flash memory, MMCs, SD memory cards, Memory Sticks, CBRAM, PRAM, FeRAM, NVRAM, MRAM, RRAM, SONOS, FJG RAM, Millipede memory, racetrack memory, and/or the like. The volatile memory may be RAM, DRAM, SRAM, FPM DRAM, EDO DRAM, SDRAM, DDR SDRAM, DDR2 SDRAM, DDR3 SDRAM, RDRAM, TTRAM, T-RAM, Z-RAM, RIMM, DIMM, SIMM, VRAM, cache memory, register memory, and/or the like. The volatile and non-volatile storage or memory may store an operating system <b>214</b>, application software <b>216</b>, data <b>218</b>, databases, database instances, database management systems, programs, program modules, scripts, source code, object code, byte code, compiled code, interpreted code, machine code, executable instructions, and/or the like to implement the functions of user computing entity <b>200</b>. As indicated, this may include a user application that is resident on the entity or accessible through a browser or other user interface for communicating with a management computing entity and/or various other computing entities.</p>
<p id="p-0093" num="0092">In some embodiments, user computing entity <b>200</b> may include location determining aspects, devices, modules, functionalities, and/or similar words used herein interchangeably. For example, user computing entity <b>200</b> may include outdoor positioning aspects, such as a location module adapted to acquire, for example, latitude, longitude, altitude, geocode, course, direction, heading, speed, universal time (UTC), date, and/or various other information/data. In one embodiment, the location module may acquire data, sometimes known as ephemeris data, by identifying the number of satellites in view and the relative positions of those satellites. Alternatively, the location information may be determined by triangulating the user computing entity&#x2019;s position in connection with a variety of other systems, including cellular towers, Wi-Fi access points, and/or the like. Similarly, user computing entity <b>200</b> may include indoor positioning aspects, such as a location module adapted to acquire, for example, latitude, longitude, altitude, geocode, course, direction, heading, speed, time, date, and/or various other information/data. Some of the indoor systems may use various position or location technologies including RFID tags, indoor beacons or transmitters, Wi-Fi access points, cellular towers, nearby computing devices (e.g., smartphones, laptops) and/or the like. For instance, such technologies may include the iBeacons, Gimbal proximity beacons, Bluetooth Low Energy (BLE) transmitters, NFC transmitters, and/or the like. These indoor positioning aspects can be used in a variety of settings to determine the location of someone or something to within inches or centimeters.</p>
<p id="p-0094" num="0093">In an interactive physical training session, a user computing entity <b>200</b> may be deployed (e.g., installed; configured; accepted; installed and accepted; configured and accepted; installed, configured, and accepted; or the like) in a training area that includes players and/or game equipment. In some embodiments, at least one input device on user computing entity <b>200</b> may collect or may be configured to collect information (e.g., data, metadata, and/or signaling) indicative of operational features of the training area and/or equipment for analysis by processing unit <b>210</b>. For example, computer vision algorithms as implemented on user computer entity <b>200</b> may be configured to detect the location of court lines, field boundaries, one or more balls, or goal posts in an input video as captured by an input camera device.</p>
<p id="p-0095" num="0094">In some embodiments, a system for virtual coaching and performance training may include at least one user computing device such as a mobile computing device and optionally a mounting apparatus for the at least one mobile computing device. The mounting apparatus may be a tripod or a kickstand, and may mount the electronic device with a camera of the user computing device positioned to monitor a training area. In some embodiments, the user computing device may be hand-held or put on the ground leaning against certain articles such as a water bottle. In some embodiments, the system for virtual coaching and performance training further comprises a sound device, for example, earbuds (e.g., wireless earbuds) or a speaker system (e.g., a public address (PA) system) coupled to the at least one user computing device. The sound device may serve to provide instruction and feedback regarding the training session to the user. In some embodiments, the system optionally comprises an optical device such as a projector, a projection lamp, a laser pointing system, a jumbotron, a television screen, or the like, that can facilitate an interactive training session. For example, a laser pointing system may point to a location in the training area to direct the user to position himself or herself, or it may point to a location in a display of the training video as the visual cue, to direct the user to perform a desired set of physical movements.</p>
<p id="p-0096" num="0095">In some embodiments, user computing entity <b>200</b> may communicate to external devices like other smartphones and/or access points to receive information such as software or firmware, or to send information (e.g., training data such as analytics, statistics, scores, recorded video, etc.) from the memory of the user computing device to external systems or devices such as servers, computers, smartphones, and the like.</p>
<p id="p-0097" num="0096">In some embodiments, two or more users may establish a connection between their computing devices using a network utilizing any of the networking protocols listed previously. At least two of the users may be in geographically different training areas. In some embodiments, the user computing devices may use a network interface such as <b>222</b> to communicate with various other computing entities, such as by communicating data, content, information, and/or similar terms used herein interchangeably that can be transmitted, received, operated on, processed, displayed, stored, and/or the like.</p>
<p id="p-0098" num="0097">In some embodiments, data such as training statistics, scores, and videos may be uploaded by one or more user computing devices to a server such as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> when the device accesses a network connection, such as a wireless access point or hotspot. The data transfer may be performed using protocols like file transfer protocol (FTP), MQ telemetry transport (MQTT), advanced message queuing protocol (AMQP), hypertext transfer protocol (HTTP), and HTTP secure (HTTPS). These protocols may be made secure over transport layer security (TLS) and/or secure sockets layer (SSL).</p>
<p id="p-0099" num="0098">In some embodiments, audio generated by a user computing device and/or audio generated by one or more users may be used to facilitate an interactive training session. In some embodiments, audio may be used to (i) direct users to particular positions on training areas (with further audio feedback to help the users locate themselves more accurately), (ii) inform users about a motion or action that a user needs to do as part of the training (e.g., shoot a ball at a basket, perform a back flip, perform an exercise such as pushups, and the like), (iii) provide feedback to the user (e.g., to inform them if the users are making a wrong move, running out of time, have successfully completed a given drill, or achieved a particular score), or (iv) report on the progress of the training session (statistics, leaderboard, and the like). In some embodiments, speech recognition and corresponding responses (e.g., audio, visual, textual, etc. responses) may also be used to facilitate the training session by allowing users to set options, correct mistakes, or start or stop the training session.</p>
<p id="p-0100" num="0099">In some embodiments, artificial intelligence-based computer vision algorithms may be used to perform at least one of the following: (i) ensure that users are located where they should be, (ii) determine when/if users successfully complete a task, (iii) rank the quality of users&#x2019; motion/action, and (iv) award quality points or other attributes depending on the nature of the users&#x2019; motion (e.g., in a game of basketball, determining whether a user scored by dunking or by performing a layup).</p>
<p id="p-0101" num="0100">In various embodiments, during the physical activities performed by users, the mobile computing device may not be on the user&#x2019;s person, and instructions may be given via a speaker or other remote devices connected to the mobile device. Further, computer vision algorithms may be used on the mobile device to guide and monitor training being conducted within the mobile device camera&#x2019;s field of view. Accordingly, embodiments of devices described herein can employ artificial intelligence (AI) to facilitate automating one or more training features of functionalities as described herein.</p>
<p id="p-0102" num="0101">To provide for or aid in the numerous determinations (e.g., determine, ascertain, infer, calculate, predict, prognose, estimate, derive, forecast, detect, compute) of training settings, player postures and player analytics described herein, components described herein may examine the entirety or a subset of data to which it is granted access and can provide for reasoning about or determine states of the system or environment from a set of observations as captured via events and/or data. Determinations may be employed to identify a specific context or action, or may generate a probability distribution over states, for example. The determinations may be probabilistic. That is, the computation of a probability distribution over states of interest based on a consideration of data and events. Determinations may also refer to techniques employed for composing higher-level events from a set of events and/or data.</p>
<p id="p-0103" num="0102">Such determinations may result in the construction of new events or actions from a set of observed events and/or stored event data, whether the events are correlated in close temporal proximity, and whether the events and data come from one or several event and data sources. For example, training instructions and feedbacks to player may be generated from one or more player analytics derived from user training actions. Further, components disclosed herein may employ various classification schemes (e.g., explicitly trained via training data or implicitly trained via observing behavior, preferences, historical information, receiving extrinsic information, etc.) and/or systems (e.g., support vector machines, neural networks, expert systems, Bayesian belief networks, fuzzy logic, data fusion engines, etc.) in connection with performing automatic and/or determined action in connection with the claimed subject matter. Thus, classification schemes and/or systems may be used to automatically learn and perform a number of functions, actions, and/or determinations.</p>
<heading level="1" id="h-0013">Exemplary Management Computing Entity</heading>
<p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an exemplary schematic diagram of a management computing entity <b>300</b>, such as NEX server <b>195</b>, for implementing a virtual coaching and performance training system, according to exemplary embodiments of the present invention. The terms computing entity, computer, entity, device, system, and/or similar words used herein interchangeably are explained in detail with reference to user computing entity <b>200</b>.</p>
<p id="p-0105" num="0104">As indicated, in one embodiment, management computing entity <b>300</b> may include one or more network or communications interface <b>320</b> for communicating with various computing entities, such as by communicating data, content, information, and/or similar terms used herein interchangeably that can be transmitted, received, operated on, processed, displayed, stored, and/or the like. For instance, management computing entity <b>300</b> may communicate with the user computing device <b>200</b> and/or a variety of other computing entities. Network or communications interface <b>320</b> may utilize a wired data transmission protocol, such as fiber distributed data interface (FDDI), digital subscriber line (DSL), Ethernet, asynchronous transfer mode (ATM), frame relay, data over cable service interface specification (DOCSIS), or any other wired transmission protocol. Similarly, management computing entity <b>300</b> may be configured to communicate via wireless external communication networks using any of a variety of standards and protocols as discussed with reference to user computing device <b>200</b>.</p>
<p id="p-0106" num="0105">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in one embodiment, management computing entity <b>300</b> may include or be in communication with one or more processing unit <b>310</b> (also referred to as processors, processing circuitry, processing element, and/or similar terms used herein interchangeably) that communicate with other elements within the management computing entity <b>300</b>. As will be understood, processing unit <b>310</b> may be embodied in a number of different ways. For example, as one or more CPLDs, microprocessors, multi-core processors, coprocessing entities, ASIPs, microcontrollers, and/or controllers, in the form of integrated circuits, application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), programmable logic arrays (PLAs), hardware accelerators, other circuitry, and/or the like. As will therefore be understood, processing unit <b>310</b> may be configured for a particular use or configured to execute instructions stored in volatile or non-volatile media <b>330</b> and <b>340</b>. As such, whether configured by hardware or computer program products, or by a combination thereof, processing unit <b>310</b> may be capable of performing steps or operations according to embodiments of the present disclosure when configured accordingly.</p>
<p id="p-0107" num="0106">Although not shown explicitly, management computing entity <b>300</b> may include or be in communication with one or more input elements, such as a keyboard, a mouse, a touch screen/display, a camera for motion and movement input, a mic for audio input, a joystick, and/or the like. Management computing entity <b>300</b> may also include or be in communication with one or more output elements such as speaker, screen/display, and/or the like.</p>
<p id="p-0108" num="0107">In various embodiments, one or more of the components of management computing entity <b>300</b> may be located remotely from other management computing entity components, such as in a distributed system or in the cloud. Furthermore, one or more of the components may be combined and additional components performing functions described herein may be included in the management computing entity <b>300</b>.</p>
<heading level="1" id="h-0014">Machine Vision and Machine Learning Modules</heading>
<p id="p-0109" num="0108">As described herein, embodiments of the present invention use one or more artificial intelligence, machine vision, and machine learning algorithms or modules for analyzing training videos and facilitating interactive virtual coaching and performance training sessions. Various exemplary machine vision algorithms are within the scope of the present invention used for performing object recognition, gesture recognition, pose estimation, and so forth. The following description describes in detail some illustrative machine vision and machine learning algorithms for implementing some embodiments of the present invention.</p>
<heading level="2" id="h-0015">Illustrative Machine Vision Architectures</heading>
<p id="p-0110" num="0109">Some exemplary machine vision algorithms utilize a deep learning network (DLN), for example a convolutional neural network (CNN). Neural networks are computer systems inspired by the human brain. They can be viewed as parallel, densely interconnected computational models that adaptively learn through automatic adjustment of system parameters based on training data. Input information are modified based on system parameters when traversing through layers of interconnected neurons or nodes, to activate or trigger particular outputs. The design of a neural network refers to the configuration of its architecture or topology, or the specific arrangements of layers and nodes in the network. The applicability, utility, and optimality of a neural network, and the framework in which the neural network is deployed are often mutually interdependent. Convolutional Neural Networks utilize the process of convolution to reduce the number of model parameters involved, while successfully capturing the spatial and temporal dependencies in an image.</p>
<p id="p-0111" num="0110">More specifically, <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an illustrative block diagram <b>400</b> of a convolutional neural network (CNN) for image analysis and object recognition, according to exemplary embodiments of the present invention. This exemplary CNN module <b>400</b> may be utilized for implementing various machine vision algorithms described herein. For example, it may be designed and trained to determine gestures and poses and other machine vision tasks required by the present invention, as would be recognized by one of ordinary skill in the art. An input layer <b>402</b> is connected via a multiplicity of hidden layers <b>404</b> to an output layer <b>406</b>. Input layer <b>402</b> is a map for pixels of an input image. Exemplary hidden layers may include, but are not limited to, convolutional layers, Rectified Linear Units (ReLU), pooling layers, normalization layers, and fully connected layers. A convolutional layer applies a convolution or correlation operation by a kernel matrix to the input data to generate a feature map of the input image. ReLU is a non-linear activation function. Pooling layers reduce the dimensionality of the data to decrease the required computational power. A fully connected layer has full connections to all activations in the previous layer, and is needed before classification or output activation at output layer <b>406</b>. Successive convolution-ReLU-pooling stages allow the successive extraction of low-level to high-level features, from edges, general shapes such as lines and circles, to specific shapes representing specific objects. <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>E</figref> provide exemplary block diagrams of a detailed neural network design for pose estimation.</p>
<p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows only one illustrative CNN architecture that is within the scope of the present invention, but the present invention is not limited to the use of CNNs. Other machine vision algorithms are also within the scope of the present invention.</p>
<heading level="2" id="h-0016">Illustrative Machine Learning Architectures</heading>
<p id="p-0113" num="0112">As states herein, various exemplary machine vision and machine learning algorithms are within the scope of the present invention for performing object recognition, gesture recognition, pose estimation, and so forth. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an illustrative block diagram <b>500</b> for a machine learning algorithm, according to exemplary embodiments of the present invention.</p>
<p id="p-0114" num="0113">In particular, a supervised machine learning algorithm is shown, comprising an illustrative random forest algorithm. Random forest algorithms are a method for classification and regression. By using a multitude of decision tree predictors <b>504</b>, each depending on the values of a random subset of a training data set <b>502</b>, the chances of overfitting to the training data set may be minimized. The decision tree predictors are voted or averaged at a decision step <b>506</b> to obtain predictions <b>508</b> of the random forest algorithm. For the task of object recognition, input <b>502</b> to the machine learning algorithm may include feature values, while output <b>508</b> may include predicted gestures and/or poses associated with a user. Random forest is only one illustrative machine learning algorithm that is within the scope of the present invention, and the present invention is not limited to the use of random forest. Other machine learning algorithms, including but not limited to, nearest neighbor, decision trees, support vector machines (SVM), Adaboost, Bayesian networks, various neural networks including deep learning networks, evolutionary algorithms, and so forth, are within the scope of the present invention.</p>
<p id="p-0115" num="0114">In short, embodiments of devices, systems, and their various components described herein may employ artificial intelligence (AI) to facilitate automating one or more functions described herein, including object recognition, gesture recognition, and pose estimation.</p>
<heading level="2" id="h-0017">Training Machine Learning Algorithms</heading>
<p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an exemplary flow diagram <b>600</b> for training a machine learning (ML) algorithm, which may be utilized in object recognition, pose estimation, and object flow construction, according to exemplary embodiments of the present invention;</p>
<p id="p-0117" num="0116">The training process begins at step <b>610</b> with data acquisition. At step <b>620</b>, acquired data are pre-processed, or prepared. At step <b>630</b>, a machine learning model is trained using training data <b>625</b>. At step <b>640</b>, the model is evaluated and tested, and further refinements to the model are fed back into step <b>630</b>. At step <b>650</b>, optimal model parameters are selected, for deployment at step <b>660</b>. New data <b>627</b> may be used by the deployed model to make predictions.</p>
<p id="p-0118" num="0117">A starting point for any machine learning method such as used by the machine learning component above is a documented dataset containing multiple instances of system inputs and correct outcomes (e.g., training data <b>625</b>). This data set may be used, using methods known in the art, including but not limited to standardized machine learning methods such as parametric classification methods, non-parametric methods, decision tree learning, neural networks, methods combining both inductive and analytic learning, and modeling approaches such as regression models, to train the machine learning system and to evaluate and optimize the performance of the trained system. Thus, it would be understood by peoples of ordinary skill in the art that &#x201c;training data&#x201d; <b>625</b> as referred to in this subsection are directed to data for training a machine vision algorithm or a machine learning algorithm, while training data <b>192</b> refer to data collected from interactive training sessions. In some embodiments, training data <b>625</b> and training data <b>192</b> may comprise one or more overlapping components, such as recorded training videos.</p>
<p id="p-0119" num="0118">The quality of the output of the machine learning system output depends on (a) pattern parameterization, (b) learning machine design, and (c) quality of the training database. These components may be refined and optimized using various methods. For example, the database may be refined by adding datasets for new documented gestures and poses. The quality of the database may be improved, for example, by populating the database with cases in which the gestures and/or poses were correctly recognized. In one embodiment, the database includes data, for example, of mistaken identification of gestures and/or poses, which may assist in the evaluation of a trained system.</p>
<p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram illustrating and summarizing some functionalities provided by an extended NEX platform <b>700</b>, according to one embodiment of the present invention. In particular, a NEX device <b>750</b> may take in training goals <b>720</b>, and facilitate different types of training activities, optionally generating player analytics <b>730</b>, providing live streaming <b>770</b> of training sessions, and enable the replay and share <b>760</b> of training video and analytics. NEX platform <b>700</b> may also receive training session recordings <b>710</b>, for local analysis to generate training analytics or statistics.</p>
<p id="p-0121" num="0120">In some embodiments, NEX platform <b>700</b> also enables multi-player training processes <b>780</b>, where multiple users located at the same or geographically different training areas may train at the same time or asynchronously, using a single mobile computing device or multiple mobile computing devices. Following are several exemplary embodiments for multi-player training, which are discussed for illustrative purposes only and do not limit the scope of the invention:</p>
<p id="p-0122" num="0121">As discussed with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, in a first single-device, multi-player embodiment of the NEX system, a single mobile computing device may be used to capture a training video of multiple players in the same training area, where the multiple players may compete or cooperate in physically responding or reacting to a single visual cue augmented to the training video. The captured training video may first be analyzed to detect individual player images or player postures from one or more frames of the training video, and player image clusters or player posture flows may be established for individual players for tracking player motion during the training session. If players compete, the player who responds or reacts to the visual cue first may be awarded a positive point score while others are given a zero point score; if players cooperate, for example when training for a doubles game in tennis or badminton, point scores may be tallied across all players.</p>
<p id="p-0123" num="0122">In a second single-device, multi-player embodiment, a single mobile computing device may be used to capture a training video of multiple players in the same training area, where multiple, player-specific visual cues may be augmented to the same training video. That is, each player responds to his or her own visual cue. This embodiment is particularly useful when only a single large external display is available for facilitating training of multiple players, possibly at different skill levels.</p>
<p id="p-0124" num="0123">In a multi-device, multi-player embodiment of the NEX system for basketball dribbling training, two or more players or users of the NEX system may each dribble in front of a respective mobile computing device, each responding to randomly generated visual cues, at the same or different difficulty levels. Training scores may be exchanged periodically between the multiple devices in real time to keep players updated on other player&#x2019;s training performance. The training session may terminate when a first player reaches a target score.</p>
<p id="p-0125" num="0124">In another multi-device, multi-player embodiment, the same sequence of visual cues may be presented to all players, and augmented to all respective training videos captured by individual mobile computing devices. For a given visual cue, each player sees the same visual symbol at the same position in the image plane of his or her training video, although the exact time instance that such a visual cue appears within the training session may differ, based on how fast the player has been responding to individual visual cues. Again, training scores or other training data may be exchanged and compared periodically, or at the end of the training session. This sequence of visual cues may be viewed as a training challenge controlled by the NEX system, enabling different players to train simultaneously, or at different, non-overlapping time periods asynchronously. The configuration of a sequence of visual cues may be based on a difficulty level of the training session.</p>
<p id="p-0126" num="0125">In a third multi-device, multi-player embodiment, the same sequence of visual cues again may be presented to all players and augmented to all respective training videos in real time, and players may compete in responding to the visual cue, for example touching a visual symbol, with the player who responds to the visual cue first awarded a positive point score, and other players awarded fewer or zero points based on their response quality. Response quality may be measured by a speed of the response, an accuracy measure for player movements, and/or other similar quality metrics. It would be understood by persons of ordinary skill in the art that such a real-time multi-player game has higher delay requirements to ensure synchronization among different players. In some embodiments, different players may be located at the same training area, with individual mobile computing devices connected via a local area network (LAN).</p>
<p id="p-0127" num="0126">Further expanding on the different multi-player embodiments above, players may be grouped into teams, where members of the same team may be located at the same training area, or at geographically different training areas. When a single training video is captured of multiple players in the same team, more than one visual cues may be augmented to the training video, to be responded by these players on the same team, in a coordinated or uncoordinated fashion, before a scored is tallied and a next set of visual cues is presented. In various embodiments, teams may compete on a total score, on a respective set of challenges, or on the same set of challenges in real-time.</p>
<p id="p-0128" num="0127">Although NEX device <b>750</b> as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> serves as the core for a NEX platform <b>700</b>, in some embodiments such as in multi-player training, NEX platform <b>700</b> may be networked among multiple user devices, where a NEX server implemented according to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be connected to multiple camera-enabled user computing devices implemented according to the embodiment shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and each used to capture physical training data, and for providing player analytics. Such training video and/or analytics data may be uploaded to the NEX server, which in term may store and facilitate sharing of such data among individual players/users and teams.</p>
<heading level="2" id="h-0018">Exemplary Convolutional Neural Networks (CNNs) for Pose Estimation</heading>
<p id="p-0129" num="0128"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a block diagram <b>800</b> of an exemplary neural network for pose estimation, according to some embodiments of the present invention. Here neural network layers or blocks are drawn with thickened lines. In this illustrative example, a two-branch CNN efficiently detects poses of multiple people in an input image by predicting part confidence maps for body parts, and part affinity fields for body part-to-body part association, effectively decoupling the detection of a body part such as an arm or leg, and the assignment of the detected body part to an individual person. A part affinity field (PAF) is a 2D vector field that encodes the location and orientation of body parts including limbs over the image domain. A PAF encodes the association between body parts, where body parts belonging to the same person are linked.</p>
<p id="p-0130" num="0129">The illustrative network shown in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> performs the following steps to estimate the pose of one or more persons in an input image:
<ul id="ul0001" list-style="none">
<li id="ul0001-0001" num="0130">1. Use a convolutional network block as a feature extractor to compute a feature map from an input image;</li>
<li id="ul0001-0002" num="0131">2. Turn the feature map into a key point heat map and an affinity field heat map using another convolutional network block;</li>
<li id="ul0001-0003" num="0132">3. Refine the key point heat map and the affinity field heat map using yet another convolutional network block, and repeat for several times;</li>
<li id="ul0001-0004" num="0133">4. Use Rectified Linear Units (ReLU), separable convolutional layers and/or batch normalization techniques to improve the accuracy and performance of the network;</li>
<li id="ul0001-0005" num="0134">5. Compute final poses by linking the key points using the affinity field heat map.</li>
</ul>
</p>
<p id="p-0131" num="0135">More specifically, an input image <b>802</b> is first passed through a feature block <b>810</b> to generate a feature map <b>812</b>. Initial prediction blocks <b>820</b> and <b>824</b> then extract a key point map <b>822</b> and an affinity field map <b>826</b>, respectively. A concatenation operation <b>830</b> is performed before further refinements are carried out in multiple iterations. For each stage of iteration, refine blocks such as <b>832</b>, <b>836</b>, <b>842</b>, and <b>846</b> predict refined key point maps such as <b>834</b> and <b>844</b>, and refined affinity field maps such as <b>838</b> and <b>848</b>, respectively. Concatenation operations such as <b>840</b> are performed to generate input for the next stage. A total of N refinements may be carried out, where N may be any positive integer. For example, N may equal to 5 in some embodiments of the present invention. After the last refinement stage, key point heat map <b>844</b> is examined in step <b>850</b> to find peaks as human joint points or key points <b>852</b>. Such key points may be linked in step <b>860</b> to generate final poses <b>862</b>, by performing bipartite matching using affinity field heat map <b>848</b> to compute weights between key points. In this illustrative example, key point map <b>844</b> may comprise 18 channels, while affinity field map <b>848</b> may comprise 34 channels.</p>
<p id="p-0132" num="0136"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a detailed block diagram illustrating an exemplary Feature Block <b>810</b>, according to some embodiments of the present invention. In this example, separable convolutional layers (SCL) are deployed with different kernel and stride sizes.</p>
<p id="p-0133" num="0137">Correspondingly, <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is a detailed block diagram illustrating an exemplary separable convolutional neural network layer <b>870</b>, according to some embodiments of the present invention. A depth-wise separable convolution or a separable convolution layer factorizes a conventional, full convolution operation into a first depth-wise convolution to filter the input channels, and a second point-wise convolution to combine outputs of the depth-wise network layer to build a feature map. Depth-wise separable convolutions trade significant improvements in computational efficiency for a small reduction in accuracy. Batch optimization and ReLU blocks further help improve the accuracy and performance of the network layer. Furthermore, in some embodiments, inverted residuals may be utilized to connect linear bottleneck layers between individual depth-wise separable convolutional layers, which also tradeoff computation and accuracy. Linear bottleneck layers reduce the dimensionality of the input, while inverted residuals use shortcut connections between the bottlenecks to enable faster training and better accuracy.</p>
<p id="p-0134" num="0138"><figref idref="DRAWINGS">FIG. <b>8</b>D</figref> is a detailed block diagram illustrating an exemplary Initial Prediction Block <b>820</b>, according to some embodiments of the present invention; <figref idref="DRAWINGS">FIG. <b>8</b>E</figref> is a detailed block diagram illustrating an exemplary Refine Block <b>832</b>, according to some embodiments of the present invention. Both comprise multiple separable convolutional layers having different kernel sizes. The input, output, and kernel sizes as shown in <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>E</figref> are for illustrative purposes only, and other similar hyperparameter values may be used in various embodiments of the present invention.</p>
<p id="p-0135" num="0139">In some implementations of the present invention, one or more of existing software modules may be utilized, including but not limited to, CoreML for CNN object and key point detection, SceneKit for rendering an AR court, and CoreMotion for understanding a mobile device&#x2019;s orientation.</p>
<heading level="2" id="h-0019">Exemplary Convolutional Neural Networks (CNNs) for Object Detection</heading>
<p id="p-0136" num="0140">Many sports and corresponding performance training methods or techniques require additional equipment, such as a ball, hoop, marker cones, hurdles, batons, rackets, and the like. The detection of moving and/or static non-human objects from the training video is needed for some training activities to help determine player actions and player analytics.</p>
<p id="p-0137" num="0141"><figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> are respective block diagrams of an exemplary neural network for ball detection, according to one embodiment of the present invention. This object detector is presented for illustrative purposes only, and some embodiments of the present invention may utilize other computer vision system designs for object detection.</p>
<p id="p-0138" num="0142"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> is a block diagram <b>900</b> of an exemplary neural network for ball detection, according to some embodiments of the present invention. In particular, <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> shows a CNN-based ball detector utilizing an optimized, modified MobileNetV2 framework as a feature extractor and a modified SSDLite framework for multi-scale object detection. An input image <b>910</b> is first processed through a Modified MobileNetV2 block <b>920</b>, the output of which is processed through a Modified SSDLite module <b>930</b> comprising two Modified SSDLite blocks <b>932</b> and <b>934</b>, to generate output <b>936</b>. The input, output, and kernel sizes as shown in <figref idref="DRAWINGS">FIGS. <b>9</b>A and <b>9</b>B</figref> are for illustrative purposes only, and other similar hyperparameter values may be used in various embodiments of the present invention.</p>
<p id="p-0139" num="0143">MobileNetV2 is an efficient convolutional neural network design for resource-constrained, mobile device-based computer vision applications. A first key building block of MobileNetV2 is depth-wise separable convolutions, which factorize a conventional, full convolutional operation into a first depth-wise convolution to filter the input channels, and a second point-wise convolution to combine outputs of the depth-wise network layer to build a feature map. Depth-wise separable convolutions trade significant improvements in computational efficiency for a small reduction in accuracy. A second key building block of MobileNetV2 is inverted residuals connecting linear bottleneck layers between individual depth-wise separable convolutional layers, which also tradeoff computation and accuracy. Linear bottleneck layers reduce the dimensionality of the input, while inverted residuals use shortcut connections between the bottlenecks to enable faster training and better accuracy.</p>
<p id="p-0140" num="0144">Although not shown explicitly in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, in this exemplary embodiment, two MobileNetV2 output layers and 14 bottleneck operators may be used, a non-obvious reduction from the conventional setup with 6 MobileNetV2 output layers and 17 bottleneck operators. Such modifications optimize the feature extraction process to not only reduce the overall computational complexity but also improve the achievable accuracy by tailoring to the specific small input and ball detection goal.</p>
<p id="p-0141" num="0145"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> is a detailed block diagram illustrating an exemplary Modified SSDLite Block, such as <b>932</b> or <b>934</b> in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, according to some embodiments of the present invention. SSD refers to a Single Shot MultiBox Detector, a multi-object detection framework using a single deep neural network to discretize feature maps into multi-scale bounding boxes. SSD eliminates separate bounding box proposal generation and feature resampling stages to improve computation efficiency without compromising detection accuracy. SSDLite is a mobile-customized variant that utilizes depth-wise separable convolution in SSD prediction layers. Modified SSDLite block <b>940</b> shown in the exemplary embodiment of <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> further tailors and improves the accuracy of SSDLite by adding dropout layers.</p>
<p id="p-0142" num="0146">More specifically, in Modified SSDLite Block <b>940</b>, parallel network blocks <b>942</b>, <b>944</b>, and <b>946</b> are utilized to process the input data separately for output confidence, output classes, and output anchors and bounding boxes. Each block has the same architecture, comprising a depth-wise convolution in 2D space, dropout, batch normalization, further convolution, and a functional operation for classification. Feature maps thus generated are reshaped and/or concatenated via processing block <b>960</b> to generate output data.</p>
<p id="p-0143" num="0147">For the ball detection task, two positive object classes may be considered: &#x201c;ball&#x201d; and &#x201c;ball-in-hand.&#x201d; With conventional SSD or SSDLite framework, a single softmax function may be used to activate among background (e.g., no positive), and these two classes. By comparison, Modified SSDLite Block <b>940</b> is designed so that it may classify a ball out of a background, but does not always classify between ball and ball-in-hand for some training data. Such a design takes into account several factors. First, ball and ball-in-hand are not always distinguishable, even for a human. In addition to motion blur, background and other objects such as leg, arm, other people in the background could look like a hand in terms of shape and/or color. Second, having a classifier distinguish between ball and ball-in-hand may not always be worthwhile and may even compromise detection accuracy since there are &#x201c;gray areas&#x201d; where an input may be classified either way. Instead, within Modified SSDLite Block <b>940</b>, a sigmoid function is used to produce confidence levels of whether a ball is present against a background, while a softmax function is used to classify between ball and ball-in-hand, or two output classes instead of three output classes for conventional SSD/SSDLite frames. As a further reduction to computational complexity, loss function and/or back propagation may be disabled if a given training case is in the &#x201c;gray area.&#x201d;</p>
<heading level="1" id="h-0020">Exemplary Statistics Generation for Virtual Coaching</heading>
<p id="p-0144" num="0148"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a flowchart <b>1000</b> illustrating exemplary operations of a user device and associated algorithms for determining one or more statistics from a video of a training session, according to some embodiments of present invention. At step <b>1002</b>, a training session may be initiated, and a video associated with the training session may be obtained. At step <b>1004</b>, one or more object flows are extracted from the video. At step <b>1006</b>, the one or more extracted object flows may be used to generate one or more statistics, which may be displayed on the user device, on a networked external display, or stored in memory for later use. While not shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, additional steps such as device calibration may occur before step <b>1002</b>. Similarly, additional steps may occur in between steps <b>1002</b> and <b>1004</b>, and in between steps <b>1004</b> and <b>1006</b>, such as various AI-based machine vision algorithms, such as but not limited to convolutional neural networks (CNN), and the like, as described above. Finally, while not shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, additional steps may occur after step <b>1006</b>, such as storing the statistics, transmitting the statistics to a server, and the like.</p>
<heading level="1" id="h-0021">Virtual Coaching and Training of Basketball Dribbling for Body-Eye Coordination</heading>
<p id="p-0145" num="0149">Example operations described herein and in particular, the example operations described above in connection with <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>10</b></figref> may be performed by an application running on an electronic device, such as shown and described in connection with <figref idref="DRAWINGS">FIG. <b>2</b></figref> above.</p>
<p id="p-0146" num="0150"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a flowchart <b>1100</b> illustrating exemplary operations for interactive virtual coaching and training, according to some embodiments of the present invention. Upon initiation at step <b>1110</b>, a training video is captured at step <b>1120</b> of a player, using a camera on a mobile computing device. At step <b>1130</b>, the training video is augmented with a visual cue for a cue period starting from a first time instance. At step <b>1140</b>, a body posture flow of the player is analyzed to determine whether the player has responded to the visual cue at a second time instant within the cue period, where the body posture flow is extracted from the training video by performing a computer vision algorithm on one or more frames of the training video. In step <b>1150</b>, a feedback is generated in response to determining that the player has responded to the visual cue.</p>
<heading level="2" id="h-0022">Training Initiation and Device Calibration</heading>
<p id="p-0147" num="0151">To initiate a training session, the mobile computing device may first be adjusted for levelness, distance from the player, brightness under a current lighting condition, and other similar environmental parameters. <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> shows an exemplary diagram <b>1200</b> representing an exemplary application running on the mobile computing device, in which an exemplary dribbling workout may be selected, according to some embodiments of the present invention. <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> shows a diagram <b>1230</b> of an interface for optionally orienting and calibrating the mobile computing device for interactive virtual coaching, according to some embodiments of the present invention.</p>
<p id="p-0148" num="0152">In <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, the user may select a first option such as &#x201c;Dribble Workout&#x201d; <b>1242</b> in order to open the setup screen <b>1230</b> for device calibration. Other single or multi-player workouts and challenges may be selected via similar interfaces.</p>
<p id="p-0149" num="0153">In <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, the mobile computing device is oriented in the landscape direction. Instructions <b>1232</b> are displayed to the user to place the device on flat ground, with a front facing camera aimed at the area where the player will train, and with the display screen facing the player. When placed on the ground, the device may be tilted to achieve a certain inclination at which the training actions can be captured entirely. On the right side, graphical symbols are shown to assist the player in tilting the device to a desired 18 degrees. A symbol such as a top line <b>1240</b> may be configured to move in a correlated manner as the angle of the mobile computing device changes when the user tilts the device forward or backward, with respect to a baseline vertical orientation. The user may be able to match top line <b>1240</b> with a reference line <b>1242</b> that may correlate to approximately 18 degrees from a perpendicular posterior tilt. In one embodiment, a &#x201c;Tilt Forward&#x201d; instruction and/or a &#x201c;Tilt Backward&#x201d; instruction may be presented to the user depending on which direction from 18 degrees the device is oriented with respect to the reference vertical orientation. In some embodiments, when the user device maintains a given predetermined angle for a predetermined period of time, for example, approximately 3 seconds, the next user interface may be configured to automatically appear on the display of the user device. In some embodiments, the device orientation and calibration process shown in <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> is optional and may be skipped. For example, the user may choose to place the mobile computing device on a tripod above ground level at an appropriate height such that the tiling process is skipped entirely. In some embodiments, depending on the nature of the training activity, the player may be required to place the mobile computing device in a portrait orientation. In some embodiments, a given symbol such as an &#x201c;X&#x201d; may serve to abort the workout session on the user device to bring the user back to a home screen.</p>
<p id="p-0150" num="0154"><figref idref="DRAWINGS">FIGS. <b>12</b>C and <b>12</b>D</figref> show respective diagrams <b>1260</b> and <b>1280</b> of an interface for starting an interactive workout session with posture control, according to some embodiments of the present invention. In this exemplary embodiment, the user may first position himself at an appropriate distance from the user device in accordance with one or more instructions <b>1264</b> presented to the user on a display screen of the user device. A wireframe figure <b>1262</b> may be presented to provide a reference scale for the user, with the user backing away from the mobile computing device until his image approximately overlaps with wireframe figure <b>1262</b>, as shown in <figref idref="DRAWINGS">FIG. <b>12</b>D</figref>. For other basketball training activities or other sports, the user may be required to move closer to or farther away from the mobile computing device, and the desired scale may be achieved by adjusting the size of wireframe figure <b>1262</b>. Once the user performs a predetermined activity such as a given number of uninterrupted dribbles, the workout session may automatically start.</p>
<p id="p-0151" num="0155">In various embodiments, instructions <b>1264</b> may vary based at least in part on the nature of the training activity, the nature of environment in which the training is being performed, such as dimension of a room in which the training session is being performed and the like, combinations thereof, and/or the like. In this embodiment, once the user performs a predetermined activity, such as a given number of uninterrupted dribbles, the workout session automatically starts. There may be many other ways to start a workout session, including, but not limited to, providing a verbal cue to the user device, setting a timer to automatically start the workout, having another user provide input to the user device to start the workout session, using a separate device such as a smart watch signal to start the workout session, combinations thereof, and/or the like. Again, a given symbol such as an &#x201c;X&#x201d; or a &#x201c;stop&#x201d; icon may serve to abort the workout session on the user device to bring the user back to a home screen.</p>
<heading level="2" id="h-0023">Interactive Training</heading>
<p id="p-0152" num="0156"><figref idref="DRAWINGS">FIGS. <b>13</b>A, <b>13</b>B, <b>13</b>C, and <b>13</b>D</figref> show respective diagrams <b>1300</b>, <b>1320</b>, <b>1340</b>, and <b>1360</b> of an interactive dribbling session facilitated by an exemplary interactive virtual coaching and performance training application running on a mobile computing device, according to some embodiments of the present invention. More specifically, screenshots of a training video are shown as a training session is underway. In this example, presentation of the NEX application running on the user device, and interaction by the user with the virtual setup presented by the NEX application may serve to help the user improve hand-eye coordination and the user&#x2019;s reaction times.</p>
<p id="p-0153" num="0157">Generally, for a chosen training activity, a given workout session, or a portion of the given workout session, may have a predetermined total training duration, such as 3 minutes or 30 minutes. A training session may also last until a user request is received to terminate. Once the training session is being recorded by a camera on the mobile computing device, after a wait time, a visual cue may be augmented onto the training video. For example, the visual cue may appear at a pre-determined location or a random location on the image plane of the training video.</p>
<p id="p-0154" num="0158">A duration of the wait time for the visual cue may be fixed or random, and may be user-configured, or automatically configured by the NEX application, optionally based on a difficulty setting of the training session. A wait time may be measured from the beginning of the training session first, then from the end of a last set of player movements. In this dribbling example, a randomly determined wait time after which the visual cue appears may be a time associated with a randomly generated number of dribbles that the user performs, such as between 4 to 10 dribbles. From diagram <b>1300</b> to <b>1320</b>, approximately one second has lapsed before a visual cue <b>1322</b> appears on the screen, towards user <b>1310</b>&#x2032;s left side.</p>
<p id="p-0155" num="0159">Once augmented onto the training video, the visual cue may last a fixed or a random cue period. A duration of this cue period may be predetermined, user-configured, automatically configured by the NEX application, optionally based on a difficulty setting of the training session, or based on a user response to the visual cue.</p>
<p id="p-0156" num="0160">In different embodiments, the visual cue may be a textual message or a graphical symbol, indicative of an instruction, prompt, or stimulus to evoke a physical response from the user, in the form of a body movement or a sequence of body movements. In the example shown in <figref idref="DRAWINGS">FIGS. <b>13</b>A to <b>13</b>D</figref>, visual cue <b>1322</b> is a large dot indicative of a target for user <b>1310</b> to touch virtually with a non-dribbling hand. Visual cue <b>1322</b> is superimposed onto the training video, at a position located to the left of user <b>1310</b> when user <b>1310</b> is dribbling with his left hand in diagram <b>1320</b>. To respond to this visual cue, user <b>1310</b> passes the ball from his left hand to his right hand first in diagram <b>1340</b>, then extends his non-dribbling hand towards the left to touch displayed symbol <b>1322</b> in diagram <b>1360</b>. After user <b>1310</b>&#x2032;s left hand remains in the frame of the target symbol <b>1322</b> for a predetermined period of time, such as approximately 0.4 seconds, the symbol may disappear from the training video, as presented to the user in this example. The disappearance or removal of the cue symbol may occur immediately, or gradually by shrinking the symbol in size or fading the symbol in color. The cue period may be defined as the time period during which the visual cue is augmented to, or virtually present in the training video. The disappearance of the visual cue symbol may be viewed as a feedback to the user, such that the user may retract the extended non-dribbling arm and continue the dribble activity with either hand. The NEX system may then restart the process of waiting for a randomly determined number of dribbles by the user before a new visual cue symbol is augmented onto the training video as a virtual target.</p>
<p id="p-0157" num="0161">In addition to being a shaded or colored dot, the visual cue may alternatively be one or more other symbols such as a horizontal virtual line above the user&#x2019;s head for the user to touch with his head by jumping up, two vertical virtual lines for the user to touch with respective hands or feet when running laterally from side-to-side, a distant virtual goal for the user to shoot a ball towards, a set of virtual cones for the user to move around or dribble around in a predetermined pattern, or a set of virtual markers for the user to jump step in a specific pattern or order. Similarly, in some embodiments, the visual cue may indicate an area that the user can move towards, into, or out of; in some embodiments, the visual cue may indicate one or a sequence of directions towards which the user can move a body part towards, for example, rotating his head left or right, or lifting his chin up. Again, the visual cue may be any textual message or a graphical symbol, indicative of an instruction, prompt, or stimulus to evoke a physical response from the user, in the form of a body movement or a sequence of body movements.</p>
<p id="p-0158" num="0162">As described with reference to <figref idref="DRAWINGS">FIGS. <b>13</b>A to <b>13</b>D</figref>, the physical response from the user may comprise a body movement to virtually touch a target visual cue with a body part such as hand, head, nose, forehead, finger, wrist, arm, elbow, leg, knee, foot, ankle, hip, and the like, in the image plane. The user may extend, bend, squat, lunge, push, pull, jump, leap, spin, rotate, balance, run, shoot, swing, bat, or perform other similar actions as instructed by the NEX application to complete the physical response. In some embodiments, the physical response may comprise a player movement to virtually touch the visual cue with a sports equipment object such as a ball, a rope, a baton, a racquet, and the like, again in the image plane. In some embodiments, the physical response may comprise a particular body movement or sequence of body movements, such as one or more back squats, split squats, pull-up, pushup, lateral plyometric jumps, forward high knee running, lateral side-to-side running, jumping jacks, shuttle runs, and the like. A quality of the player body movements may be determined by the NEX application via one or more computer vision algorithms, and a physical response may be deemed completed only if the quality of the player body movement is satisfactory.</p>
<p id="p-0159" num="0163">To determine that the player has responded to the visual cue, one or more frames of the training video are analyzed using one or more computer vision algorithms. For example, as discussed with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, and <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>9</b>B</figref>, one or more CNNs may be applied to some or all frames of the training video as the video is being captured to detect objects such as basketballs or soccer balls, and individual user and their postures. Input video frames may be downsampled in spatial or temporal domain to reduce the needed computation complexity. A tracking algorithm may be performed to track all detected objects and postures, to generate one or more object flows including one or more user posture flows. By examining the motion sequence and/or location of key feature points of a posture flow, such as those for a hand and fingers, the NEX system may determine whether and when the user has responded to the visual cue. Each CNN may be implemented in a separate software or hardware module on the mobile computing device, and each CNN may have been trained using one or more prior training videos.</p>
<p id="p-0160" num="0164">In some embodiments, a quality point score may be earned by the user based on a characteristic of the user&#x2019;s physical response, such as accuracy or duration. For example, the visual cue symbol may be displayed for a predetermined amount of time, such as approximately 5 seconds, which may be configurable by the user and may be based at least in part on a difficulty setting associated with the training session. After 5 seconds, the visual cue symbol may disappear from the view presented at the display of the user device, if the user has not responded successfully by virtually touching the visual cue. The point score associated with accurately virtually touching the cue symbol by the user&#x2019;s non-dribbling hand may be configured to decrease in value over time. For example, the point score may start at 10 points for a reaction time of less than 0.5 seconds, and decrease at the rate of about 1 point every approximately 0.5 seconds, leading to a score range between 0 and 10. A reaction time may be computed as the time between when the visual cue first appears, and the time instant when the physical response is detected by the computer vision algorithms. A section of the view presented to the user, such as virtual scoreboard <b>1362</b> shown in <figref idref="DRAWINGS">FIGS. <b>13</b>C and <b>13</b>D</figref>, may be updated accordingly. In this disclosure, a reaction time is considered synonymous to a reaction speed.</p>
<p id="p-0161" num="0165">In various aspects, such a training session design may serve to allow for users having a wide range of performance capabilities to benefit from a training session. For example, in the described basketball training session, dribbling faster may make cue targets appear quicker; accordingly, the virtual training session may serve to directly correlate a dribble frequency required by the user to a difficulty level of the training session. Moreover, in some embodiments, the higher the difficulty of the training session, or the more stringent the performance criteria for successfully completing a training session may be, the higher the final score may be awarded to a given user.</p>
<p id="p-0162" num="0166">Once it is determined that the player has responded to the visual cue, a feedback may be generated to the player. In various embodiments, such a feedback may take on different forms and/or serve different purposes. For example, the disappearance or removal of the visual cue from the training video may be considered as a feedback to the user to signal that a current round or instant of the training has completed, and the player may restore to a startup posture; the point score as discussed above may also be considered a feedback, indicative of how fast the user has successfully responded to the visual cue. In some embodiments, the feedback may be a congratulatory audio message generated based on a current point score, a cumulative point score, or characteristics of the physical response elicited from the user. In some embodiments, the feedback may be a review of the user&#x2019;s form in responding to the visual cue, and instructions on how to achieve a better form. This feedback to the player may or may not be communicated to the player in real-time, in the form of a visual or audio display to the user.</p>
<heading level="2" id="h-0024">Training Performance Review</heading>
<p id="p-0163" num="0167"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an exemplary diagram <b>1400</b> of user statistics and scores collected from an interactive dribbling session facilitated by an exemplary interactive virtual coaching and training application running on a mobile computing device, according to some embodiments of the present invention. In various embodiments, when a training session either expires or is aborted or terminated, for example, via a user input indicative of an abort operation, a review screen such as <b>1400</b> may be displayed to the user at the user device. The review screen may display various metrics <b>1440</b> associated with the performance of the user <b>1420</b> in the particular training session, including, but not limited to, the user&#x2019;s final obtained score, and a ranking <b>1410</b> of the current score as compared to the user&#x2019;s personal historical records. Moreover, the user may be presented with the option of starting another workout as part of the review screen or as part of another screen. In some embodiments, the user may be served with the option to begin another workout from the displayed review screen so that the user may leave his user device in place and thereby avoid the need to go through a device setup or calibration for consecutive workouts. Again, a given symbol such as an &#x201c;X&#x201d; may serve to abort the workout session on the user device to bring the user back to a home screen.</p>
<p id="p-0164" num="0168"><figref idref="DRAWINGS">FIGS. <b>15</b>A and <b>15</b>B</figref> show respective diagrams <b>1500</b> and <b>1550</b> of player statistics <b>1510</b> and global rankings <b>1560</b> for interactive dribbling sessions facilitated by an exemplary interactive virtual coaching and training application running on a mobile computing device, according to some embodiments of the present invention. The NEX application allows for the presentation of a workout review to the users at the display of the user device. In one embodiment, such a workout review may allow the user to select, for example via a swiping motion at the user device having a touch screen, between statistics related to their performance for a given workout session, and a comparison of the given workout session statistics to the user&#x2019;s own previous workout sessions in addition to other user&#x2019;s best workout session statistics. In one embodiment, the application may be configured to display a video option to the user at the user device. In particular, the selection of the video which may be positioned anywhere on the display of the user device, such as in the lower section of the display of the user device, may provide the user with the option to view a portion or the entirety of the workout session. In some embodiments, the selection of the video option may be configured to process the video of the workout session (e.g., using one or more AI-based algorithms) to show key features (e.g., highlights) of a predetermined duration (e.g., a 30 sec highlight compilation) associated with the performance of the user during the workout session. For example, such a presentation may be configured to show the quickest reactions, or the continuous 30 sec segment in which the most points were scored by the user in performing a given workout and/or training activity. Moreover, the determination and presentation of such statistical reviews to the user may serve to provide insight into areas of user deficiency and strength with respect to one or more portions of the workout session such that users may easily identify where the most improvement is needed. As noted, various AI-based algorithms (e.g., neural networks, convolutional neural networks, Bayesian techniques, random forests, combinations thereof, and/or the like) may be used to automatically analyze and identify the areas of user strength and deficiency without explicit user identification.</p>
<p id="p-0165" num="0169"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a diagram <b>1600</b> of a video review <b>1610</b> of highlights of the user&#x2019;s performance, according to some embodiments of the present invention. As noted, a video review may display various statistics related to the user&#x2019;s performance during the training session. For example, the video review may serve to describe and display the reaction time <b>1620</b> of each individual reaction of the user as the video presentation progresses in time. In another embodiment, in the case of a basketball training session, a metric such as the dribble speed <b>1630</b> of the user may also be a live metric allowing the user to analyze their dribble speed and effort while performing the activities associated with the training session.</p>
<p id="p-0166" num="0170"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows an additional diagram <b>1700</b> of player statistics provided in a feed, according to some embodiments of the present invention. In one embodiment, such a feed may be in the form of a feed card <b>1770</b> that may feature a highlight video (e.g., an approximately 30 second highlight) of a given workout and a statistical review <b>1760</b> of the performance of the user during the given training session.</p>
<heading level="1" id="h-0025">Other Exemplary Embodiments</heading>
<p id="p-0167" num="0171"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram <b>1800</b> of another exemplary interactive training activity facilitated by an exemplary interactive virtual coaching and training application running on a mobile computing device, according to some embodiments of the present invention. While the examples described herein so far have been mainly directed to the user performing a dribbling movement while virtually touching virtual symbols presented to the user at the display of a user device, other training exercises may be facilitated as well. In this example, a lateral movement training exercise is presented to a user <b>1830</b>, in which user <b>1830</b> waits for one or more on-screen cues such as the shading in area <b>1820</b>. The user may then react to the on-screen cues by laterally moving himself out of area <b>1810</b> in which he is currently located to shaded area <b>1820</b>. In alternative embodiments, area <b>1810</b> may be shaded as a cue and the user may respond by moving laterally out of area <b>1810</b> correspondingly. In some embodiments, the training area may be divided into more than one area for player <b>1830</b> to move into and/or out of. Also, an arrow or other virtual symbols may be used to designate the area for user <b>1830</b> to move into or out of. Lateral movement training without ball dribbling is suitable for home practice in a living room instead of on a ball court.</p>
<p id="p-0168" num="0172">In this particular exemplary training exercise, the level of difficulty of the training session may progress and be contingent on the amount of time spent by the user in area <b>1810</b>. When the user chooses to perform the training with a basketball to make it into a lateral dribbling exercise, the level of difficulty of the training session may also depend on the number and frequency of dribble mistakes, other undesirable behavior as defined by the game and/or training session, combinations thereof, and/or the like. In various embodiments, such complementary training session may be helpful in allowing the user to better train for a given sport such as the game of basketball. By introducing a way to train for lateral movement quickness as described by this example training exercise, this particular exercise may serve to mimic aspects of gameplay, such as avoiding defenders on the court.</p>
<p id="p-0169" num="0173">In some embodiments, one or more human trainers such as a partner or a coach may be integrated into the training sessions described herein. For example, a human trainer may provide cones for users to dribble around to mimic defenders, and the NEX system may provide randomly generated visual cues around such cones or audio cues for users to dribble or move in a particular order or pattern. A human trainer or a ball shooting machine may also feed balls continuously to a user, and the NEX system may provide additional cues, visual or audio, as instructions for a next set of movements. Such advanced training techniques may be better suited to describing real-world gaming situations where obstacles are not static and may be unpredictable.</p>
<p id="p-0170" num="0174">In various embodiments, the following additional implementation possibilities are contemplated to be within the scope of the present invention. These exemplary embodiments are for illustrative purposes only and do not limit the scope of the present invention.</p>
<p id="p-0171" num="0175">A virtual coaching and performance training system that facilitates training sessions to monitor cross-over style movements in the game of basketball.</p>
<p id="p-0172" num="0176">A virtual coaching and performance training system which facilitates training sessions that incorporate various dribble challenges, for example to dribble uninterrupted for a number of times, to dribble with one hand only while keeping the other hand fixed, to dribble with one hand in a particular pattern such as around a shape of figure &#x201c;8&#x201d;, or to switch among different dribbling types including front dribble, behind-the-back dribble, under-the-leg dribble and the like.</p>
<p id="p-0173" num="0177">A virtual coaching and performance training system which facilitates multi-player training, as described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p>
<p id="p-0174" num="0178">A virtual coaching and performance training system that allows a training video to be sent via a communications interface to a television viewing medium, such as AirPlay to an AppleTV. As discussed previously with reference to <figref idref="DRAWINGS">FIG. <b>1</b>D</figref>, an external display such as a television or a projector may be coupled to the mobile computing device, such that the user may see the captured training video and virtual training cues and information with higher resolution and better clarity. By having a wireless connection to a larger display device, the user may also train at a larger distance away from the mobile computing device over a larger training area.</p>
<p id="p-0175" num="0179">A virtual coaching and performance training system that places the user in an augmented reality (AR) environment, possibly using additional equipment such as head mounted displays and/or projection lamps integrated into the system. For example, instead of editing or annotating a recorded training video file or stream, a projection lamp coupled to the mobile computing device may be used to project a visual cue onto a projection of the training video, as an augmentation of the training video. The training video may be analyzed while taking into consideration of the projected visual cue, to determine user posture flows and user responses. In another example, in addition to recording the user and the training environment as the training video, additional virtual elements may be generated, optionally based on captured user inputs, and the training video may be partially or fully virtualized to place the user in an AR or virtual reality (VR) environment.</p>
<p id="p-0176" num="0180">Systems, methods, and apparatuses that provide analysis and real-time and/or near real-time correction of movement patterns and postures of users during training sessions. As discussed previously, visual or audio feedback information in the form of textual messages and graphical symbols may be given to the user as the NEX system analyzes the user&#x2019;s posture flow.</p>
<heading level="1" id="h-0026">Conclusions</heading>
<p id="p-0177" num="0181">One of ordinary skill in the art knows that the use cases, structures, schematics, and flow diagrams may be performed in other orders or combinations, but the inventive concept of the present invention remains without departing from the broader scope of the invention. Every embodiment may be unique, and methods/steps may be either shortened or lengthened, overlapped with the other activities, postponed, delayed, and continued after a time gap, such that every end-user device is accommodated by the server to practice the methods of the present invention.</p>
<p id="p-0178" num="0182">The present invention may be implemented in hardware and/or in software. Many components of the system, for example, signal processing modules or network interfaces etc., have not been shown, so as not to obscure the present invention. However, one of ordinary skill in the art would appreciate that the system necessarily includes these components. A computing device, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, is a hardware that includes at least one processor coupled to a memory. The processor may represent one or more processors (e.g., microprocessors), and the memory may represent random access memory (RAM) devices comprising a main storage of the hardware, as well as any supplemental levels of memory, e.g., cache memories, non-volatile or back-up memories (e.g., programmable or flash memories), read-only memories, etc. In addition, the memory may be considered to include memory storage physically located elsewhere in the hardware, e.g. any cache memory in the processor, as well as any storage capacity used as a virtual memory, e.g., as stored on a mass storage device.</p>
<p id="p-0179" num="0183">The hardware of a computing device also typically receives a number of inputs and outputs for communicating information externally. For interface with a user, the hardware may include one or more user input devices (e.g., a keyboard, a mouse, a scanner, a microphone, a camera, etc.) and a display (e.g., a Liquid Crystal Display (LCD) panel). For additional storage, the hardware may also include one or more mass storage devices, e.g., a floppy or other removable disk drive, a hard disk drive, a Direct Access Storage Device (DASD), an optical drive (e.g., a Compact Disk (CD) drive, a Digital Versatile Disk (DVD) drive, etc.) and/or a tape drive, among others. Furthermore, the hardware may include an interface to one or more networks (e.g., a local area network (LAN), a wide area network (WAN), a wireless network, and/or the Internet among others) to permit the communication of information with other computers coupled to the networks. It should be appreciated that the hardware typically includes suitable analog and/or digital interfaces to communicate with each other.</p>
<p id="p-0180" num="0184">In some embodiments of the present invention, the entire system can be implemented and offered to the end-users and operators over the Internet, in a so-called cloud implementation. No local installation of software or hardware would be needed, and the end-users and operators would be allowed access to the systems of the present invention directly over the Internet, using either a web browser or similar software on a client, which client could be a desktop, laptop, mobile device, and so on. This eliminates any need for custom software installation on the client side and increases the flexibility of delivery of the service (software-as-a-service), and increases user satisfaction and ease of use. Various business models, revenue models, and delivery mechanisms for the present invention are envisioned, and are all to be considered within the scope of the present invention.</p>
<p id="p-0181" num="0185">The hardware operates under the control of an operating system, and executes various computer software applications, components, program code, libraries, objects, modules, etc. to perform the methods, processes, and techniques described above.</p>
<p id="p-0182" num="0186">In general, the method executed to implement the embodiments of the invention may be implemented as part of an operating system or a specific application, component, program, object, module or sequence of instructions referred to as &#x201c;computer program(s)&#x201d; or &#x201c;program code(s).&#x201d; The computer programs typically comprise one or more instructions set at various times in various memory and storage devices in a computing device or computer, and that, when read and executed by one or more processors in the computer, cause the computer to perform operations necessary to execute elements involving the various aspects of the invention. Moreover, while the invention has been described in the context of fully functioning computers and computer systems, those skilled in the art will appreciate that the various embodiments of the invention are capable of being distributed as a program product in a variety of forms, and that the invention applies equally regardless of the particular type of machine or computer-readable media used to actually effect the distribution. Examples of computer-readable media include but are not limited to recordable type media such as volatile and non-volatile memory devices, floppy and other removable disks, hard disk drives, optical disks (e.g., Compact Disk Read-Only Memory (CD-ROMS), Digital Versatile Disks, (DVDs), etc.), and digital and analog communication media.</p>
<p id="p-0183" num="0187">Although specific embodiments of the disclosure have been described, one of ordinary skill in the art will recognize that numerous other modifications and alternative embodiments are within the scope of the disclosure. For example, any of the functionality and/or processing capabilities described with respect to a particular device or component may be performed by any other device or component. Further, while various illustrative implementations and architectures have been described in accordance with embodiments of the disclosure, one of ordinary skill in the art will appreciate that numerous other modifications to the illustrative implementations and architectures described herein are also within the scope of this disclosure.</p>
<p id="p-0184" num="0188">Blocks of the block diagrams and flow diagrams support combinations of means for performing the specified functions, combinations of elements or steps for performing the specified functions, and program instruction means for performing the specified functions. It will also be understood that each block of the block diagrams and flow diagrams, and combinations of blocks in the block diagrams and flow diagrams, may be implemented by special-purpose, hardware-based computer systems that perform the specified functions, elements or steps, or combinations of special-purpose hardware and computer instructions.</p>
<p id="p-0185" num="0189">A software component may be coded in any of a variety of programming languages. An illustrative programming language may be a lower-level programming language such as an assembly language associated with a particular hardware architecture and/or operating system platform. A software component comprising assembly language instructions may require conversion into executable machine code by an assembler prior to execution by the hardware architecture and/or platform.</p>
<p id="p-0186" num="0190">A software component may be stored as a file or other data storage construct. Software components of a similar type or functionally related may be stored together such as, for example, in a particular directory, folder, or library. Software components may be static (for example, preestablished or fixed) or dynamic (for example, created or modified at the time of execution).</p>
<p id="p-0187" num="0191">Software components may invoke or be invoked by other software components through any of a wide variety of mechanisms. Invoked or invoking software components may comprise other custom-developed application software, operating system functionality (for example, device drivers, data storage (for example, file management) routines, other common routines and services, etc.), or third-party software components (for example, middleware, encryption, or other security software, database management software, file transfer or other network communication software, mathematical or statistical software, image processing software, and format translation software).</p>
<p id="p-0188" num="0192">Software components associated with a particular solution or system may reside and be executed on a single platform or may be distributed across multiple platforms. The multiple platforms may be associated with more than one hardware vendor, underlying chip technology, or operating system. Furthermore, software components associated with a particular solution or system may be initially written in one or more programming languages but may invoke software components written in another programming language.</p>
<p id="p-0189" num="0193">Computer-executable program instructions may be loaded onto a special-purpose computer or other particular machine, a processor, or other programmable data processing apparatus to produce a particular machine, such that execution of the instructions on the computer, processor, or other programmable data processing apparatus causes one or more functions or operations specified in the flow diagrams to be performed. These computer program instructions may also be stored in a computer-readable storage medium (CRSM) that upon execution may direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable storage medium produce an article of manufacture including instruction means that implement one or more functions or operations specified in the flow diagrams. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational elements or steps to be performed on the computer or other programmable apparatus to produce a computer-implemented process.</p>
<p id="p-0190" num="0194">Although embodiments have been described in language specific to structural features and/or methodological acts, it is to be understood that the disclosure is not necessarily limited to the specific features or acts described. Rather, the specific features and acts are disclosed as illustrative forms of implementing the embodiments. Conditional language, such as, among others, &#x201c;can,&#x201d; &#x201c;could,&#x201d; &#x201c;might,&#x201d; or &#x201c;may,&#x201d; unless specifically stated otherwise, or otherwise understood within the context as used, is generally intended to convey that certain embodiments could include, while other embodiments do not include, certain features, elements, and/or steps. Thus, such conditional language is not generally intended to imply that features, elements, and/or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding, with or without user input or prompting, whether these features, elements, and/or steps are included or are to be performed in any particular embodiment.</p>
<p id="p-0191" num="0195">Although the present invention has been described with reference to specific exemplary embodiments, it will be evident that the various modification and changes can be made to these embodiments without departing from the broader scope of the invention. Accordingly, the specification and drawings are to be regarded in an illustrative sense rather than in a restrictive sense. It will also be apparent to the skilled artisan that the embodiments described above are specific examples of a single broader invention which may have greater scope than any of the singular descriptions taught. There may be many alterations made in the descriptions without departing from the scope of the present invention.</p>
<?detailed-description description="Detailed Description" end="tail"?>
</description>
<us-claim-statement>What is claimed is:</us-claim-statement>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text><b>1</b>. A computer implemented method for facilitating training of body-eye coordination using a computing device having access to a camera, comprising:
<claim-text>receiving a training video of a player from the camera;</claim-text>
<claim-text>superimposing a visual cue onto the training video;</claim-text>
<claim-text>extracting a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video;</claim-text>
<claim-text>determining whether the player has responded to the visual cue by analyzing the body posture flow of the player; and</claim-text>
<claim-text>generating a feedback to the player in response to determining that the player has responded to the visual cue.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text><b>2</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the visual cue is a symbol superimposed onto the training video in a frame of the training video.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text><b>3</b>. The computer implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the determining whether the player has responded to the visual cue comprises determining a player movement to virtually touch the symbol in the frame with a body part.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text><b>4</b>. The computer implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the determining whether the player has responded to the visual cue comprises determining whether the player has virtually touched the symbol in the frame with a sports equipment object.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text><b>5</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining whether the player has responded to the visual cue comprises determining whether the player has performed a predetermined sequence of movements.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text><b>6</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the extracting the body posture flow of the player from the training video comprises using a Convolutional Neural Network (CNN) module to detect one or more key points of the player in a frame of the training video, and</claim-text>
<claim-text>wherein the CNN module has been trained using one or more prior videos.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text><b>7</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the training video comprises a dribbling activity performed by the player, and</claim-text>
<claim-text>wherein the superimposing the training video with the visual cue is in response to determining that the player has dribbled for a predetermined number of times before a first time instant.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text><b>8</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>waiting for a period of wait time before the superimposing the training video with the visual cue, wherein a duration of the wait time is based on a detected player action during the wait time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text><b>9</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>,
<claim-text>wherein the training video comprises a juggling activity performed by the player, and</claim-text>
<claim-text>wherein the superimposing the training video with the visual cue is in response to determining that the player has juggled for a predetermined number of times.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text><b>10</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>generating a training statistic for the player based on the training video,</claim-text>
<claim-text>wherein the training statistic includes at least a reaction speed.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text><b>11</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:
<claim-text>generating a training statistic for the player based on the training video,</claim-text>
<claim-text>wherein the training statistic comprises a first current statistic that is associated with the training video and a second historical statistic that is associated with one or more historical training sessions associated with the player.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text><b>12</b>. A non-transitory computer-readable storage medium storing executable instructions, the executable instructions when executed by a hardware processor causing the hardware processor to execute a process for facilitating training of body-eye coordination, the executable instructions comprising steps to:
<claim-text>receive a training video of a player from a camera;</claim-text>
<claim-text>superimpose a visual cue onto the training video;</claim-text>
<claim-text>extract a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video;</claim-text>
<claim-text>determine whether the player has responded to the visual cue by analyzing the body posture flow of the player; and</claim-text>
<claim-text>generate a feedback to the player in response to determining that the player has responded to the visual cue.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text><b>13</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the visual cue is a symbol superimposed onto the training video in a frame of the training video.</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text><b>14</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the executable instructions to determine whether the player has responded to the visual cue comprises executable instructions to determine a player movement to virtually touch the visual cue with a body part.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text><b>15</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the executable instructions to determine whether the player has responded to the visual cue comprises executable instructions to determine whether the player has virtually touched the visual cue with a sports equipment object.</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text><b>16</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the executable instructions to determine whether the player has responded to the visual cue comprises executable instructions to determine whether the player has performed a predetermined sequence of movements.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>,
<claim-text>wherein the executable instructions to extract the body posture flow of the player from the training video comprises executable instructions to use a Convolutional Neural Network (CNN) module to detect one or more key points of the player in a frame of the training video, and</claim-text>
<claim-text>wherein the CNN module has been trained using one or more prior videos.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>,
<claim-text>wherein the training video comprises a dribbling activity performed by the player, and</claim-text>
<claim-text>wherein the executable instructions to superimpose the training video with the visual cue are executed in response to determining that the player has dribbled for a predetermined number of times before a first time instant.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising executable instructions to:
<claim-text>wait for a period of wait time before the executable instructions to superimpose the training video with the visual cue, wherein a duration of the wait time is based on a detected player action during the wait time.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text><b>20</b>. A computing device comprising a hardware processor and a non-transitory storage medium, the non-transitory storage medium storing executable instructions, the executable instructions when executed by the hardware processor causing the hardware processor to execute a process for facilitating training of body-eye coordination, the executable instructions comprising steps to:
<claim-text>receive a training video of a player from a camera;</claim-text>
<claim-text>superimpose a visual cue onto the training video;</claim-text>
<claim-text>extract a body posture flow of the player from the training video by performing a computer vision algorithm on one or more frames of the training video;</claim-text>
<claim-text>determine whether the player has responded to the visual cue by analyzing the body posture flow of the player; and</claim-text>
<claim-text>generate a feedback to the player in response to determining that the player has responded to the visual cue.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-application>
