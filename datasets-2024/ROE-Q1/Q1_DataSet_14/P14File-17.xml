<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]>
<us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230225576A1-20230720.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20230704" date-publ="20230720">
<us-bibliographic-data-application lang="EN" country="US">
<publication-reference>
<document-id>
<country>US</country>
<doc-number>20230225576</doc-number>
<kind>A1</kind>
<date>20230720</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>17996699</doc-number>
<date>20210108</date>
</document-id>
</application-reference>
<us-application-series-code>17</us-application-series-code>
<priority-claims>
<priority-claim sequence="01" kind="national">
<country>CN</country>
<doc-number>202010311652.2</doc-number>
<date>20200420</date>
</priority-claim>
</priority-claims>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>9</main-group>
<subgroup>28</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>11</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>9</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>05</class>
<subclass>D</subclass>
<main-group>1</main-group>
<subgroup>02</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>9</main-group>
<subgroup>2805</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>11</main-group>
<subgroup>4011</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>11</main-group>
<subgroup>4061</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>9</main-group>
<subgroup>009</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>9</main-group>
<subgroup>2852</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>05</class>
<subclass>D</subclass>
<main-group>1</main-group>
<subgroup>0214</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>47</class>
<subclass>L</subclass>
<main-group>2201</main-group>
<subgroup>04</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>05</class>
<subclass>D</subclass>
<main-group>2201</main-group>
<subgroup>0215</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>G</section>
<class>05</class>
<subclass>D</subclass>
<main-group>2201</main-group>
<subgroup>0203</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<invention-title id="d2e61">OBSTACLE AVOIDANCE METHOD AND APPARATUS FOR SELF-WALKING ROBOT, ROBOT, AND STORAGE MEDIUM</invention-title>
<us-parties>
<us-applicants>
<us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>Beijing Roborock Innovation Technology Co., Ltd.</orgname>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
<residence>
<country>CN</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="00" designation="us-only">
<addressbook>
<last-name>WU</last-name>
<first-name>Zhen</first-name>
<address>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="01" designation="us-only">
<addressbook>
<last-name>XIE</last-name>
<first-name>Haojian</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="02" designation="us-only">
<addressbook>
<last-name>PENG</last-name>
<first-name>Song</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="03" designation="us-only">
<addressbook>
<last-name>WANG</last-name>
<first-name>Yixing</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
<inventor sequence="04" designation="us-only">
<addressbook>
<last-name>HU</last-name>
<first-name>Zhiying</first-name>
<address>
<city>Beijing</city>
<country>CN</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<pct-or-regional-filing-data>
<document-id>
<country>WO</country>
<doc-number>PCT/CN2021/070912</doc-number>
<date>20210108</date>
</document-id>
<us-371c12-date><date>20221020</date></us-371c12-date>
</pct-or-regional-filing-data>
</us-bibliographic-data-application>
<abstract id="abstract">
<p id="p-0001" num="0000">An obstacle avoidance method for a self-moving robot includes: obtaining and recording, by the self-moving robot, information about an obstacle encountered during traveling, wherein the information about the obstacle includes type information and location information of the obstacle; and receiving, by the self-moving robot, an operation instruction for a specified obstacle, wherein the operation instruction is configured for instructing the self-moving robot, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="78.74mm" wi="144.61mm" file="US20230225576A1-20230720-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="142.07mm" wi="99.82mm" file="US20230225576A1-20230720-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="170.60mm" wi="96.94mm" file="US20230225576A1-20230720-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="96.60mm" wi="146.64mm" file="US20230225576A1-20230720-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="155.87mm" wi="86.70mm" file="US20230225576A1-20230720-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="150.79mm" wi="165.27mm" file="US20230225576A1-20230720-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">The present application is a US national phase filing under 35 U.S.C. &#xa7; 371 claiming the benefit of PCT international application under PCT/CN2021/070912, which claims priority to Chinese Patent Application No. 202010311652.2, filed on Apr. 20, 2020, both which is incorporated herein by reference in their entireties.</p>
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?>
<?summary-of-invention description="Summary of Invention" end="lead"?>
<heading id="h-0002" level="1">TECHNICAL FIELD</heading>
<p id="p-0003" num="0002">The present disclosure relates to the field of robot control technologies, and in particular, to an obstacle avoidance method and device for a self-moving robot, a robot, and a storage medium.</p>
<heading id="h-0003" level="1">BACKGROUND</heading>
<p id="p-0004" num="0003">With the development of artificial intelligence technologies, various intelligent robots, such as a sweeping robot, a mopping robot, a vacuum cleaner, a weeder or the like, emerge. These cleaning robots can automatically identify sweeping routes, and can also identify and record obstacle information during a cleaning process, which improves route optimization of a subsequent cleaning process. This not only frees labor force, reduces manpower costs, but also improves cleaning efficiency.</p>
<p id="p-0005" num="0004">In particular, some sweeping robots equipped with cameras have a capability of identifying types of obstacles, and can adopt different obstacle avoidance policies according to different types of obstacles. However, due to uncertainties in an identification capability, an error frequently occurs in an identification result, and obstacle avoidance is automatically performed in a place where obstacle avoidance should not be performed in a cleaning route, resulting in missed sweeping in some regions and poor user experience.</p>
<heading id="h-0004" level="1">SUMMARY</heading>
<p id="p-0006" num="0005">Embodiments of the present disclosure provide an obstacle avoidance method for a self-moving robot, applied to a robot side, including: obtaining and recording information about an obstacle encountered during traveling, wherein the information about the obstacle includes type information and location information of the obstacle; and receiving an operation instruction for a specified obstacle, wherein the operation instruction instructs, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</p>
<p id="p-0007" num="0006">Optionally, detecting the obstacle of the same type as the specified obstacle in a region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, confirming that the obstacle of the same type as the specified obstacle is detected.</p>
<p id="p-0008" num="0007">Optionally, detecting the obstacle of the same type as the specified obstacle in a region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, determining whether the obstacle is in the region range labeled with the specified obstacle; and if yes, confirming that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle.</p>
<p id="p-0009" num="0008">Optionally, receiving the operation instruction for a specified obstacle specifically includes: receiving an operation instruction for ignoring the specified obstacle sent from a user through a human-machine interaction interface.</p>
<p id="p-0010" num="0009">Optionally, the operation instruction includes type information and location information of the specified obstacle.</p>
<p id="p-0011" num="0010">Optionally, the method further includes: allocating, after encountering the obstacle during traveling, an identifier to the obstacle, wherein the operation instruction includes an identifier of the specified obstacle; and searching, after receiving the operation instruction for the specified obstacle, for corresponding type information and location information according to the identifier of the specified obstacle.</p>
<p id="p-0012" num="0011">Optionally, the region range includes a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or all regions accessible by the self-moving robot.</p>
<p id="p-0013" num="0012">An embodiment of the present disclosure provides an obstacle avoidance device for a self-moving robot, applied to a robot side, and including: an obtaining unit, configured to obtain and record information about an obstacle encountered during traveling, wherein the information about the obstacle includes type information and location information of the obstacle; and a receiving unit, configured to receive an operation instruction for a specified obstacle, wherein the operation instruction instructs, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</p>
<p id="p-0014" num="0013">Optionally, detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, confirming that the obstacle of the same type as the specified obstacle is detected.</p>
<p id="p-0015" num="0014">Optionally, detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, determining whether the obstacle is in the region range labeled with the specified obstacle; and if yes, confirming that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle.</p>
<p id="p-0016" num="0015">Optionally, receiving the operation instruction for a specified obstacle specifically includes: receiving the operation instruction for ignoring the specified obstacle sent from a user through a human-machine interaction interface.</p>
<p id="p-0017" num="0016">Optionally, the operation instruction includes type information and location information of the specified obstacle.</p>
<p id="p-0018" num="0017">Optionally, the device further includes: a unit, configured to: allocate, after encountering the obstacle during traveling, an identifier to the obstacle, wherein the operation instruction includes an identifier of the specified obstacle; and a unit, configured to: search, after receiving the operation instruction for the specified obstacle, for corresponding type information and location information according to the identifier of the specified obstacle.</p>
<p id="p-0019" num="0018">Optionally, the region range includes a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or all regions accessible by the self-moving robot.</p>
<p id="p-0020" num="0019">An embodiment of the present disclosure provides a robot, including a processor and a memory, wherein the memory stores a computer program instruction that can be executed by the processor, and the computer program instruction implements operations of the foregoing method when being executed by the processor.</p>
<p id="p-0021" num="0020">An embodiment of the present disclosure provides a non-transitory computer readable storage medium, where a computer program instruction is stored, and the computer program instruction implements operations of the foregoing method when being invoked and executed by a processor.</p>
<?summary-of-invention description="Summary of Invention" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading>
<p id="p-0022" num="0021">To describe the technical solutions in the embodiments of the present disclosure or in the prior art more clearly, the following briefly describes the accompanying drawings used for describing the embodiments or the prior art. Clearly, the accompanying drawings in the following description illustrates some embodiments of the present disclosure, and one of ordinary skill in the art can still derive other drawings from these accompanying drawings without creative efforts.</p>
<p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic view of an application scenario according to an embodiment of the present disclosure;</p>
<p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a three-dimensional structural view of a self-moving robot according to an embodiment of the present disclosure;</p>
<p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a top view of a structure of a self-moving robot according to an embodiment of the present disclosure;</p>
<p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a bottom view of a structure of a self-moving robot according to an embodiment of the present disclosure;</p>
<p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic flowchart of an obstacle avoidance method for a self-moving robot according to an embodiment of the present disclosure;</p>
<p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic configuration diagram of an obstacle avoidance APP of a self-moving robot according to an embodiment of the present disclosure;</p>
<p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram of an obstacle avoidance device for a self-moving robot according to an embodiment of the present disclosure; and</p>
<p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of an electronic structure of a robot according to an embodiment of the present disclosure.</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?detailed-description description="Detailed Description" end="lead"?>
<heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading>
<p id="p-0031" num="0030">To make the objectives, technical solutions, and advantages of the embodiments of the present disclosure clearer, the following clearly and completely describes the technical solutions in the embodiments of the present disclosure with reference to the accompanying drawings in the embodiments of the present disclosure. Clearly, the described embodiments are merely some rather than all of the embodiments of the present disclosure. Based on the embodiments of the present disclosure, one of ordinary sill in the art can obtain other embodiments without creative efforts, which all fall within the scope of the present disclosure.</p>
<p id="p-0032" num="0031">It should be understood that although terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, &#x201c;third&#x201d;, and the like may be used in the embodiments of the present disclosure to describe . . . , the . . . should not be limited by the terms. These terms are merely used to distinguish . . . . For example, without departing from the scope of the embodiments of the present disclosure, first . . . may also be referred to as second . . . , and similarly, second . . . may also be referred to as first . . . .</p>
<p id="p-0033" num="0032">An embodiment of the present disclosure provides a possible application scenario, and the application scenario includes an automatic cleaning apparatus <b>100</b>, such as a sweeping robot, a mopping robot, a vacuum cleaner, and a weeder. In this embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for example, a household sweeping robot is taken as an example for description. In an operation process of the household sweeping robot, sweeping may be performed according to a preset route or an automatically planned route, but inevitably, the household sweeping robot cannot move when being stuck in some place, such as a chair <b>200</b> or a table. In this case, the sweeping robot may identify an obstacle by using a cloud server <b>300</b>, a local server, or a storage system of the sweeping robot, and label the place as an obstacle location, and perform automatic obstacle avoidance when the sweeping robot moves to the place for the next time. In this embodiment, the robot may be provided with a touch-sensitive display or controlled by a mobile terminal, so as to receive an operation instruction input by a user. The automatic cleaning apparatus may be provided with various sensors, such as a buffer, a cliff sensor, an ultrasonic sensor, an infrared sensor, a magnetometer, an accelerometer, a gyroscope, and an odograph (a specific structure of each sensor is not described in detail, and any one of the sensors may be applied to the automatic cleaning apparatus), and may further be provided with a wireless communications module such as a Wi-Fi module and a Bluetooth module to connect to an intelligent terminal or a server, and receive, through the wireless communications module, the operation instruction transmitted by the intelligent terminal or the server.</p>
<p id="p-0034" num="0033">As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an automatic cleaning apparatus <b>100</b> may travel on the ground with various combinations of the movement with respect to the following three mutually perpendicular axes defined by a body <b>110</b>: a front and rear axis X, a lateral axis Y, and a central vertical axis Z. The forward drive direction along the front and rear axis X is labeled as &#x201c;forward&#x201d;, and the backward drive direction along the front and rear axis X is labeled as &#x201c;backward&#x201d;. The direction of the lateral axis Y is essentially a direction that passes through an axis defined by the center point of a drive wheel module <b>41</b> and extends between the right wheel and the left wheel of the robot.</p>
<p id="p-0035" num="0034">The automatic cleaning apparatus <b>100</b> can rotate around the Y-axis. When the front part of the automatic cleaning apparatus <b>100</b> is tilted upward, and the back part is tilted downward, this case is referred to as &#x201c;pitch up&#x201d;; and when the front part of the automatic cleaning apparatus <b>100</b> is tilted downward, and the back part is tilted upward, this case is referred to as &#x201c;pitch down&#x201d;. In addition, the automatic cleansing apparatus <b>100</b> may rotate around the Z-axis. In the forward direction of the automatic cleaning apparatus <b>100</b>, when the automatic cleaning apparatus <b>100</b> is tilted to the right side of the X-axis, it rotates rightward, and when the automatic cleaning apparatus <b>100</b> is tilted to the left side of the X-axis, it rotates leftward.</p>
<p id="p-0036" num="0035">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the automatic cleaning apparatus <b>100</b> includes a machine body <b>110</b>, a sensing system <b>120</b>, a control system, a drive system <b>140</b>, a cleaning system, an energy system, and a human-machine interaction system <b>180</b>.</p>
<p id="p-0037" num="0036">The machine body <b>110</b> includes a front part <b>111</b> and a back part <b>112</b>, and has a substantially circular shape (circular shape in both the front and back parts), or may have other shapes, including but not limited to an approximately D shape of a rectangular front part and a circular back part, a rectangle of a rectangular front part and a rectangular back part, or a square shape.</p>
<p id="p-0038" num="0037">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the sensing system <b>120</b> includes a location determining device <b>121</b> disposed on the machine body <b>110</b>, a collision sensor and a proximity sensor that are disposed on a buffer <b>122</b> of the front part <b>111</b> of the machine body <b>110</b>, a cliff sensor disposed on a lower part of the machine body, and sensor devices such as a magnetometer, an accelerometer, a gyro, an odograph (ODO) that are disposed inside the machine body, which are configured to provide various pieces of location information and motion state information of the machine to the control system <b>130</b>. The location determining device <b>121</b> includes but is not limited to a camera and a laser direct structuring (LDS) device.</p>
<p id="p-0039" num="0038">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the front part <b>111</b> of the machine body <b>110</b> may carry the buffer <b>122</b>. When the drive wheel module <b>141</b> drives the robot to move on the ground during cleaning, the buffer <b>122</b> detects one or more events in a driving path of the automatic cleaning apparatus <b>100</b> through a sensor system such as an infrared sensor disposed thereon, and the automatic cleaning apparatus <b>100</b> may control, based on the events detected by the buffer <b>122</b> such as an obstacle or a wall, the drive wheel module <b>141</b>, causing the automatic cleaning apparatus <b>100</b> to respond to the events, such as keeping away from the obstacle.</p>
<p id="p-0040" num="0039">The control system <b>130</b> is disposed on a circuit board in the machine body <b>110</b>, and includes a computing processor, such as a central processing unit and an application processor, in communication with a non-transitory memory, such as a hard disk, a flash memory, and a random access memory. According to obstacle information fed back by the LDS device, the application processor draws an instant map in an environment in which the robot is located through a locating algorithm, such as simultaneous localization and mapping (SLAM). In addition, with reference to distance information and speed information that are fed back by the sensor devices such as the sensors disposed on the buffer <b>122</b>, the cliff sensor, the magnetometer, the accelerometer, the gyroscope, and the odograph, the control system comprehensively determines a current working state, a location, and a current posture of the sweeping robot, such as crossing a doorsill, climbing onto a carpet, locating at a cliff, being stuck at the top or the bottom, a dust box being full, and being picked up. In addition, the control system may provide a specific next action policy for various cases, so that operation of the robot better meets a requirement of the user and has better user experience.</p>
<p id="p-0041" num="0040">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the drive system <b>140</b> may operate the automatic cleansing apparatus <b>100</b> to cross the ground based on a drive command with distance and angle information (for example, x, y, and &#x3b8; components). The drive system <b>140</b> includes the drive wheel module <b>141</b>. The drive wheel module <b>141</b> may control both the left wheel and the right wheel at the same time. To control movement of the cleaning robot more accurately, the drive wheel module <b>141</b> includes a left drive wheel module and a right drive wheel module. The left and right drive wheel modules are opposed to each other along a lateral axis defined by the body <b>110</b>. To enable the robot to move more stably or more robustly on the ground, the robot may include one or more driven wheels <b>142</b>, and the driven wheels include but are not limited to universal wheels. The drive wheel module includes a moving wheel, a driving motor, and a control circuit configured to control the drive motor. The drive wheel module may further be connected to both a circuit configured to measure a drive current and an odograph. The drive wheel module <b>141</b> may be detachably connected to the machine body <b>110</b>, so as to facilitate disassembly and maintain. The drive wheel may have an offset drop suspension system, which is fixed to the machine body <b>110</b> in a moveable manner, for example, attached to the machine body <b>110</b> in a rotatable manner, and receives a spring offset that is offset downward and away from the machine body <b>110</b>. The spring offset enables the drive wheel to maintain contact with the ground and traction with respect to the ground with a grounding force, and a cleaning element of the automatic cleaning apparatus <b>100</b> also contacts the ground <b>10</b> with a pressure.</p>
<p id="p-0042" num="0041">The cleaning system may be a dry cleaning system and/or a wet cleaning system. As a dry cleaning system, a primary cleaning function is derived from a cleaning system <b>151</b> formed by a brushroll, a dust box, a fan, an air outlet, and connection parts connecting the four. The brushroll that interferes with the ground sweeps rubbish on the ground and rolls it to the front of a suction port between the brushroll and the dust box. Then, the rubbish is suctioned into the dust box by suction air generated by the fan and passing through the dust box. The dry cleaning system may further include a side brush <b>152</b> having a rotation shaft that is angled relative to the ground and is configured to move debris into a brushroll region of the cleaning system.</p>
<p id="p-0043" num="0042">The energy system includes a rechargeable battery, such as a Ni-H battery and a lithium battery. The rechargeable battery may be connected to a charging control circuit, a charging temperature detection circuit of a battery pack, and a battery under-voltage monitoring circuit. The charging control circuit, the charging temperature detection circuit of the battery pack, and the battery under-voltage monitoring circuit are further connected to a single-chip control circuit. The cleaning robot is charged by connecting to a charging pile by using a charging electrode disposed on a side or at the bottom of the machine body. If dust is deposited on the exposed charging electrode, a plastic body around the electrode may be melted and deformed due to a charge accumulation during charging; and even the electrode itself is deformed, and the cleaning robot cannot be charged properly.</p>
<p id="p-0044" num="0043">The human-machine interaction system <b>180</b> includes buttons disposed on a panel, and the buttons are configured for the user to select a function; may further include a display screen and/or an indicating lamp and/or a speaker, wherein the display screen, the indicating lamp, and the speaker present a current state or function selection item of the machine to the user; and may further include a mobile phone client program. For a path navigation type automatic cleaning apparatus, a mobile phone client can display a map of an environment in which the automatic cleaning apparatus is located and a location of the machine to the user, so as to provide a more enriched and humanized function item to the user.</p>
<p id="p-0045" num="0044">According to the obstacle avoidance method for the cleaning robot according to this embodiment of the present disclosure, the cleaning robot obtains, during traveling, obstacle information in a traveling route in real time through the sensor or the image obtaining device of the robot, and labels the obstacle information in the working map. When it is subsequently manually determined that some types of obstacles are not required to be avoided, an ignoring operation may be performed. An obstacle avoidance operation is not performed on these types of obstacles any more in the subsequent cleaning process.</p>
<p id="p-0046" num="0045">In one implementation, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an embodiment of the present disclosure provides an obstacle avoidance method for a self-moving robot, which is applied to a robot side and includes the following method steps.</p>
<p id="p-0047" num="0046">In step S<b>502</b>, information of an obstacle encountered during travelling is acquired and recorded, wherein the information of the obstacle includes type information and location information of the obstacle.</p>
<p id="p-0048" num="0047">The obstacle refers to any object that may prevent the robot from moving. As described above, the automatic cleaning apparatus includes a collision sensor, a proximity sensor, a cliff sensor, and a sensor device such as a magnetometer, an accelerometer, a gyro, and an odograph (ODO) that are disposed inside a machine body. These sensors provide various pieces of location information and motion status information of the machine to a control system <b>130</b> at a fixed frequency. The control system <b>130</b> obtains various sensor parameters of the automatic cleaning apparatus in a traveling process in real time, and further determines and analyzes a current location and working status of the automatic cleaning apparatus.</p>
<p id="p-0049" num="0048">The automatic cleaning apparatus shoots an image in real time through a camera carried by the automatic cleaning apparatus during traveling, where image information may be complete photo information, or may be feature information extracted from the image to reflect an object in the image, for example, image information obtained after extracting a feature point. For example, the feature information includes abstract features of the image, such as a corner, an object boundary line, a color gradient, an optical flow gradient, and a location relationship between features. The control system <b>130</b> obtains image information parameters of the automatic cleaning apparatus in real time during traveling, and further analyzes and determines the current location and operation status of the automatic cleaning apparatus.</p>
<p id="p-0050" num="0049">The information of the obstacle includes the type information of the obstacle and the location information of the obstacle. The type information of the obstacle refers to a category that the obstacle is identified as, and which is an identification result of a sensor or an image identification device according to a preset identification classification. The classification includes but is not limited to a power strip bar, a pedestal, excrement, a weight scale, a clew, a shoe, and a fixed object (for example, a sofa, a bed, a table, and a chair). The control system classifies obstacles according to different features of the obstacles obtained by the sensor or the image identification device. For example, if an image is photographed by the image identification device, the control system automatically classifies obstacles according to obstacle labels, or the obstacles may be identified as fixed objects according to a collision strength and a collision location that are received by a collision sensor.</p>
<p id="p-0051" num="0050">The location information of the obstacle includes location information labeled in a working map of the robot according to a specific location of the identified obstacle, for example, a coordinate location. The working map is region information generated by the cleaning robot according to a working region. The working map may be obtained by external inputs after the cleaning robot arrives at the working region, or may be automatically obtained by the cleaning robot in multiple working processes. The working map typically includes partitions formed according to architectural features of the working region, such as a bedroom region, a living room region, and a kitchen region of a home; and a corridor region, a work region, and a rest region of an office region.</p>
<p id="p-0052" num="0051">As an example, the obstacle information may be obtained by the image obtaining device through obtaining image information in real time during traveling. When the image information meets a preset model condition, it is determined that there is an obstacle at a current location, and a category is labeled according to a preset model, which specifically includes a following process. In the traveling process, the robot obtains image information in real time during the traveling, when there is an obstacle image, the obstacle image is compared with an image model trained by the robot, and an identified obstacle is classified according to a comparison result. For example, when an image of &#x201c;shoe&#x201d; is shot, the image is matched with models of multiple types stored by the robot, and when a proportion that the image is matched with &#x201c;shoe&#x201d; is higher, the image is classified as a shoe. Certainly, if identification is not clear through the obstacle image, image identification may be performed on the obstacle from multiple angles. For example, when a probability that a front ball-like object is identified as a clew is 80%, a probability that the front ball-like object is identified as excrement is 70%, and a probability that the front ball-like object is identified as a ball is 75%, the three probabilities are relatively approximate to each other and it is difficult for the object to be classified. The robot may choose to obtain image information from another angle to perform a second identification until the robot can identify the object as a certain category with a relatively large probability difference. For example, the probability that the object is identified as a clew is 80%, and the probabilities that the object is identified as other objects are all below 50%, then the obstacle is classified as a clew.</p>
<p id="p-0053" num="0052">The obstacle image information refers to information through which it can be determined that the robot is trapped, and includes but is not limited to image information obtained by the image obtaining device of the cleaning robot, radar information obtained by a radar system, and the like. The automatic cleaning apparatus shoots an image in real time during traveling through a camera carried by the automatic cleaning apparatus, wherein image information may be complete photo information, or may be feature information extracted from the image to reflect the image, for example, image information obtained after extracting a feature point. For example, the feature information includes abstract features of the image, such as a corner, an object boundary line, a color gradient, an optical flow gradient, and a location relationship of the features.</p>
<p id="p-0054" num="0053">After a coordinate point of the obstacle location is labeled as an obstacle, a region with a certain area and shape around the obstacle location is selected as an obstacle region; and the obstacle is located in the obstacle region. In specific implementation, the foregoing range may be a circular region with a preset length as a radius and the coordinate point labeled as an obstacle as the center, or may be a quadrilateral with the coordinate point labeled as an obstacle as the center and a preset length as an edge length. The length of the radius and the edge length may be fixed, or may be set to different values according to various types of obstacles.</p>
<p id="p-0055" num="0054">During the traveling, the robot may determine, through the foregoing image sensor, whether there is an obstacle at a current location. When there is an obstacle, the location coordinates are labeled on a working map, and type information of the obstacle is recorded in combination with an image device, wherein the type information of the obstacle includes but is not limited to a power strip bar, a pedestal, excrement, a weight scale, a clew, a shoe, and a fixed object (for example, a sofa, a bed, a table, and a chair).</p>
<p id="p-0056" num="0055">In step S<b>504</b>, an operation instruction for a specified obstacle is received, wherein the operation instruction instructs, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</p>
<p id="p-0057" num="0056">In an example, receiving the operation instruction for the specified obstacle specifically includes: receiving the operation instruction for ignoring the specified obstacle sent from a user through a human-machine interaction interface. The human-machine interaction interface includes but is not limited to a mobile phone APP interface, a human-machine interaction interface of the robot, a voice interaction interface, and the like. The specified obstacle includes, for example, one or more of a socket, a pedestal, excreta, a weight scale, a clew, a shoe, and a fixed object.</p>
<p id="p-0058" num="0057">In an example, the operation instruction includes type information and location information of the specified obstacle, specifically, may be an instruction for an obstacle of certain type at a certain location, for example, an instruction for excreta in a living room; or may be an instruction for obstacles of certain types at a certain location, for example, an instruction for excreta and a socket in a living room; or may be an instruction for an obstacle of a certain type at certain locations, for example, an instruction for excreta in a living room and a bedroom.</p>
<p id="p-0059" num="0058">As an example, after encountering an obstacle in the traveling process, the robot allocates an identifier to the obstacle, for example, allocates different identifiers to different obstacles such as a socket, a pedestal, excreta, a weight scale, a clew, a shoe, and a fixed object, and the identifier may be displayed on an operation interface of a mobile phone. The operation instruction includes an identifier of the specified obstacle; and after the operation instruction for the specified obstacle is received, corresponding type information and location information are searched for according to the identifier of the specified obstacle.</p>
<p id="p-0060" num="0059">Content of the instruction may be as follows: when an obstacle of the same type as the specified obstacle is detected in a region range labeled with the specified obstacle, no obstacle avoidance operation is performed on the obstacle of the same type. The region range may be a range at a certain distance from the location where the specified obstacle is labeled; or may be a zone where the specified obstacle is labeled, for example, an ignoring instruction is indicated for any obstacle marked with excreta detected in a bedroom; or may be all regions accessible by the robot, that is, no obstacle avoidance operation is to be performed on the obstacle of the same type in the entire space.</p>
<p id="p-0061" num="0060">In an embodiment, detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, confirming that the obstacle of the same type as the specified obstacle is detected.</p>
<p id="p-0062" num="0061">The region range includes a region with a fixed size and shape with a coordinate location of the specified obstacle as a geometric center; or a zone where the obstacle is located, where the zone is automatically divided by the self-moving robot or is manually divided by the user.</p>
<p id="p-0063" num="0062">In an example, the region range includes a circle that takes the coordinate location labeled with the obstacle information as a center and a fixed length as the radius; or a zone where the coordinate location labeled with the obstacle information is located, where the zone is a region divided according to the working map. In a specific implementation process, an ignored range may be a circle that takes the coordinate point as the center and a fixed length as the radius, preferably 50 cm. For a sweeping robot that has a region dividing capability, the ignored range may be a zone where the coordinate point is located, for example, a mark for slipper in a bedroom is ignored; or in the entire cleaning space, shoes are ignored.</p>
<p id="p-0064" num="0063">When the moving robot travels within a region range (for example, a circle with a radius of 50 cm) labeled with the specified obstacle (for example, a shoe), the moving robot matches a type of an obstacle detected in real time with the type of the specified obstacle, for example, determines whether a type of the currently detected obstacle is a shoe, and if the matching succeeds, determines that the obstacle of the same type as the specified obstacle is detected, and performs an ignoring operation.</p>
<p id="p-0065" num="0064">In another embodiment, detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, determining whether the obstacle is located within the region range labeled with the specified obstacle; and if yes, determining that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle.</p>
<p id="p-0066" num="0065">In the moving process, the moving robot obtains obstacle information in an identification capability range in real time, and matches a type of a detected obstacle with the type of the specified obstacle (for example, a shoe); if the matching succeeds, it indicates that the obstacle type is a shoe and belongs to the type of the specified obstacle, and then determines whether the obstacle is located within the region range labeled with the specified obstacle; and if yes, determines that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle, and performs an ignoring operation.</p>
<p id="p-0067" num="0066">In the sweeping process or after the sweeping process, a corresponding location on a sweeping map of the APP displays the type of an obstacle identified by the sweeping robot in the current sweeping process. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the type of the obstacle includes but is not limited to a socket, a pedestal, excreta, a weight scale, a clew, a shoe, and a fixed object (for example, a sofa, a bed, a table, and a chair). In this case, the user may manually operate the obstacles. For example, the user may choose to ignore at least one of the obstacles, and label an operation instruction for ignoring the obstacle on the working map. After detecting the user's operation instruction for ignoring a certain obstacle at a certain location, the robot records the type and coordinates of the obstacle. In a subsequent cleaning process, if the sweeping robot detects this type of obstacle within a certain coordinate range, the obstacle mark is ignored, that is, this type of obstacle is not considered to appear at the location, and no obstacle avoidance operation is to be performed on the obstacle. For example, the user selects an ignoring operation for a slipper mark at a certain location in a bedroom, and thereafter, when the cleaning robot moves near the mark, the cleaning robot keep cleaning without performing obstacle avoidance. Alternatively, when the cleaning robot moves to the bedroom region in which the mark is located, no obstacle avoidance is required to be performed on the slipper mark.</p>
<p id="p-0068" num="0067">According to the obstacle avoidance method for the robot provided in this embodiment of the present disclosure, in the traveling process, the robot automatically obtains obstacle information during traveling by using the sensor or the image obtaining device of the robot, and labels the type and location of the obstacle. After the obstacle information is manually confirmed, when this type of obstacle information is encountered again, obstacle avoidance or sweeping is performed accordingly, thereby avoiding missed sweeping in the cleaning region caused by blind obstacle avoidance, improving cleaning accuracy, and improving user experience.</p>
<p id="p-0069" num="0068">In one implementation, as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, an embodiment of the present disclosure provides an obstacle avoidance device for a robot, applied to a robot side and including:
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0000">
    <ul id="ul0002" list-style="none">
        <li id="ul0002-0001" num="0069">an obtaining unit <b>702</b>, configured to obtain and record information of an obstacle encountered during traveling, wherein the information of the obstacle includes type information and location information of the obstacle; and</li>
        <li id="ul0002-0002" num="0070">a receiving unit <b>704</b>, configured to receive an operation instruction for a specified obstacle, wherein the operation instruction instructs, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</li>
    </ul>
    </li>
</ul>
</p>
<p id="p-0070" num="0071">Optionally, detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, determines that the obstacle of the same type as the specified obstacle is detected.</p>
<p id="p-0071" num="0072">Optionally, detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle specifically includes: matching a type of a detected obstacle with a type of the specified obstacle; and if the matching succeeds, determining whether the obstacle is in the region range labeled with the specified obstacle; and if yes, confirming that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle.</p>
<p id="p-0072" num="0073">Optionally, receiving the operation instruction for the specified obstacle specifically includes: receiving the operation instruction for ignoring the specified obstacle sent from a user through a human-machine interaction interface.</p>
<p id="p-0073" num="0074">Optionally, the operation instruction includes type information and location information of the specified obstacle.</p>
<p id="p-0074" num="0075">Optionally, the device further includes: a unit, configured to: allocate, after encountering the obstacle during travelling, an identifier to the obstacle, wherein the operation instruction includes an identifier of the specified obstacle; and a unit, configured to: search, after receiving the operation instruction for the specified obstacle, for corresponding type information and location information according to the identifier of the specified obstacle.</p>
<p id="p-0075" num="0076">Optionally, the region range includes a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or a zone where the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by the user; or all regions accessible by the self-moving robot.</p>
<p id="p-0076" num="0077">According to the obstacle avoidance device for the robot provided in this embodiment of the present disclosure, the robot automatically obtains obstacle information during traveling by using the sensor or the image obtaining device of the robot, and labels the type and location of the obstacle. After the obstacle information is manually confirmed, when this type of obstacle information is encountered again, obstacle avoidance is performed accordingly, thereby avoiding missed sweeping in the cleaning region caused by blind obstacle avoidance, improving cleaning accuracy, and improving user experience.</p>
<p id="p-0077" num="0078">An embodiment of the present disclosure provides a non-transitory computer readable storage medium, in which a computer program instruction is stored, and the computer program instruction implements operations of the foregoing method steps when being invoked and executed by a processor.</p>
<p id="p-0078" num="0079">An embodiment of the present disclosure provides a robot, including a processor and a memory, where the memory stores a computer program instruction that can be executed by the processor, and the processor implements operations of the method of any of the foregoing embodiments when executing the computer program instruction.</p>
<p id="p-0079" num="0080">As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the robot may include a processing device (such as a central processing unit, a graphics processing unit, or the like) <b>801</b> that may perform various appropriate actions and processing according to a program stored in a read-only memory (ROM) <b>802</b> or a program loaded from a storage device <b>808</b> into a random access memory (RAM) <b>803</b>. In the RAM <b>803</b>, various programs and data required for operation of an electronic robot <b>800</b> are further stored. The processing device <b>801</b>, the ROM <b>802</b>, and the RAM <b>803</b> are connected to each other by a bus <b>804</b>. An input/output (I/O) interface <b>805</b> is also connected to the bus <b>804</b>.</p>
<p id="p-0080" num="0081">Generally, the following devices may be connected to the I/O interface <b>805</b>: input devices <b>806</b> including, for example, a touchscreen, a touchpad, a keyboard, a mouse, a camera, a microphone, an accelerometer, and a gyroscope; output devices <b>807</b> including, for example, a liquid crystal display (LCD), a loudspeaker and a vibrator; storage devices <b>808</b> including, for example, a hard disk; and a communications device <b>809</b>. The communications device <b>809</b> may allow the electronic robot to communicate wirelessly or through a wired connection with another robot to exchange data. Although <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an electronic robot with various devices, it should be understood that it is not required to implement or provide all illustrated devices. Alternatively, more or fewer devices may be implemented or provided.</p>
<p id="p-0081" num="0082">In particular, according to the embodiments of the present disclosure, the process described above with reference to the flowchart may be implemented as a software program of the robot. For example, an embodiment of the present disclosure includes a robot software program product that includes a computer program carried on a readable medium, and the computer program includes program code used to perform the method illustrated in the flowchart. In such an embodiment, the computer program may be downloaded and installed from a network by using the communications device <b>809</b>, installed from the storage device <b>808</b>, or installed from the ROM <b>802</b>. When the computer program is executed by the processing device <b>801</b>, the foregoing functions defined in the method in the embodiments of the present disclosure are executed.</p>
<p id="p-0082" num="0083">It should be noted that the foregoing computer readable medium in the present disclosure may be a computer readable signal medium, a computer readable storage medium, or any combination of the two. The computer readable storage medium may be, for example, an electrical, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any combination thereof. More specific examples of the computer readable storage medium may include but are not limited to: an electrical connection having one or more conducting wires, a portable computer disk, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or flash memory), an optical fiber, a portable compact disk read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination thereof. In the present disclosure, the computer readable storage medium may be any tangible medium that includes or stores a program, and the program may be used by or in combination with an instruction execution system, apparatus, or device. In the present disclosure, the computer readable signal medium may include a data signal propagated in a baseband or as a part of a carrier, which carries computer readable program code. Such a propagated data signal may be in multiple forms, including but not limited to an electromagnetic signal, an optical signal, or any suitable combination thereof. The computer readable signal medium may further be any computer readable medium other than the computer readable storage medium, and the computer readable signal medium may send, propagate, or transmit a program that is used by or in combination with an instruction execution system, apparatus, or device. The program code included in the computer readable medium may be transmitted by using any suitable medium, including but not limited to: a wire, an optical cable, a radio frequency (RF), or any suitable combination thereof.</p>
<p id="p-0083" num="0084">The computer readable medium may be included in the foregoing robot, or may exist separately and not be assembled into the robot.</p>
<p id="p-0084" num="0085">Computer program code for performing the operations of the present disclosure may be written in one or more programming languages or a combination thereof, such as object-oriented programming languages Java, Smalltalk, C++, and conventional procedural programming languages such as &#x201c;C&#x201d; or similar program design languages. The program code may be executed completely on a user computer, partially on a user computer, as an independent package, partially on a user computer and partially on a remote computer, or completely on a remote computer or server. In cases involving a remote computer, the remote computer may be connected to a user computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or may be connected to an external computer (for example, through the Internet by using an Internet service provider).</p>
<p id="p-0085" num="0086">Flowcharts and block diagrams in the accompanying drawings illustrate possible architectures, functions, and operations of systems, methods, and computer program products according to various embodiments of the present disclosure. In this regard, each block in a flowchart or block diagram may represent a module, program segment, or part of code that includes one or more executable instructions for implementing a specified logical function. It should also be noted that in some alternative implementations, functions marked in the block may also occur in different order than those marked in the accompanying drawings. For example, two blocks represented in succession may actually be executed in substantially parallel, and they may sometimes be executed in reverse order, depending on the functions involved. It should also be noted that each block in the block diagram and/or flowchart and a combination of blocks in the block diagram and/or flowchart may be implemented by using a dedicated hardware-based system that performs a specified function or operation, or may be implemented by using a combination of dedicated hardware and a computer instruction.</p>
<p id="p-0086" num="0087">The apparatus embodiment described above is merely an example. The units described as separate parts can or cannot be physically separate, and parts displayed as units can or cannot be physical units, can be located in one location, or can be distributed on a plurality of network units. Some or all of the modules can be selected based on an actual need to implement the solutions of the embodiments. A person of ordinary skill in the art can understand and implement the embodiments of the present application without creative efforts.</p>
<p id="p-0087" num="0088">Finally, it should be noted that the previous embodiments are merely intended for describing the technical solutions of the present disclosure, but not for limiting the present disclosure. Although the present disclosure is described in detail with reference to the previous embodiments, persons of ordinary skill in the art should understand that they can still make modifications to the technical solutions described in the previous embodiments or make equivalent replacements to some technical features thereof, without departing from the spirit and scope of the technical solutions of the embodiments of the present disclosure.</p>
<?detailed-description description="Detailed Description" end="tail"?>
</description>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text><b>1</b>. An obstacle avoidance method for a self-moving robot, comprising:
<claim-text>obtaining and recording, by the self-moving robot, information about an obstacle encountered during traveling, wherein the information about the obstacle comprises type information and location information of the obstacle; and</claim-text>
<claim-text>receiving, by the self-moving robot, an operation instruction for a specified obstacle, wherein the operation instruction is configured for instructing the self-moving robot, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle comprises:</claim-text>
<claim-text>matching a type of a detected obstacle with a type of the specified obstacle; and</claim-text>
<claim-text>in response to that the matching succeeds, confirming that the obstacle of the same type as the specified obstacle is detected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>detecting the obstacle of the same type as the specified obstacle in the region range labeled with the specified obstacle comprises:</claim-text>
<claim-text>matching a type of a detected obstacle with a type of the specified obstacle;</claim-text>
<claim-text>in response to that the matching succeeds, determining whether the obstacle is in the region range labeled with the specified obstacle; and</claim-text>
<claim-text>in response to determining the obstacle is in the region range, confirming that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:
<claim-text>receiving the operation instruction for the specified obstacle comprises:</claim-text>
<claim-text>receiving the operation instruction for ignoring the specified obstacle sent from a human-machine interaction interface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein:
<claim-text>the operation instruction comprises type information and location information of the specified obstacle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:
<claim-text>allocating, after encountering the obstacle during traveling, an identifier to the obstacle, wherein the operation instruction comprises an identifier of the specified obstacle; and</claim-text>
<claim-text>searching, after receiving the operation instruction for the specified obstacle, for corresponding type information and location information according to the identifier of the specified obstacle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or</claim-text>
<claim-text>all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-08-14" num="08-14">
<claim-text><b>8</b>-<b>14</b>. (canceled)</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text><b>15</b>. A self-moving robot, comprising a processor and a memory, wherein the memory stores a computer program instruction that can be executed by the processor, and when being executed by the processor, the computer program instruction implements actions comprising:
<claim-text>obtaining and recording information about an obstacle encountered during traveling, wherein the information about the obstacle comprises type information and location information of the obstacle; and</claim-text>
<claim-text>receiving an operation instruction for a specified obstacle, wherein the operation instruction is configured for instructing the self-moving robot, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text><b>16</b>. A non-transitory computer readable storage medium, wherein a computer program instruction is stored, and the computer program instruction, when being invoked and executed by a processor, implements actions comprising:
<claim-text>obtaining and recording information about an obstacle encountered during traveling, wherein the information about the obstacle comprises type information and location information of the obstacle; and</claim-text>
<claim-text>receiving an operation instruction for a specified obstacle, wherein the operation instruction is configured for instructing a self-moving robot, when detecting an obstacle of a same type as the specified obstacle in a region range labeled with the specified obstacle, not to perform an obstacle avoidance operation on the obstacle of the same type.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text><b>17</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or</claim-text>
<claim-text>all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text><b>18</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text><b>19</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or</claim-text>
<claim-text>all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text><b>20</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or</claim-text>
<claim-text>all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00021" num="00021">
<claim-text><b>21</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or</claim-text>
<claim-text>all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00022" num="00022">
<claim-text><b>22</b>. The robot according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processor is configured to:
<claim-text>match a type of a detected obstacle with a type of the specified obstacle; and</claim-text>
<claim-text>in response to that the matching succeeds, confirm that the obstacle of the same type as the specified obstacle is detected.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00023" num="00023">
<claim-text><b>23</b>. The robot according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processor is configured to:
<claim-text>match a type of a detected obstacle with a type of the specified obstacle;</claim-text>
<claim-text>in response to that the matching succeeds, determine whether the obstacle is in the region range labeled with the specified obstacle; and</claim-text>
<claim-text>in response to determining the obstacle is in the region range, confirm that the obstacle of the same type as the specified obstacle is detected in the region range labeled with the specified obstacle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00024" num="00024">
<claim-text><b>24</b>. The robot according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the processor is further configured to:
<claim-text>receive the operation instruction for ignoring the specified obstacle sent from a human-machine interaction interface.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00025" num="00025">
<claim-text><b>25</b>. The robot according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the operation instruction comprises type information and location information of the specified obstacle.</claim-text>
</claim>
<claim id="CLM-00026" num="00026">
<claim-text><b>26</b>. The robot according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the processor is further configured to:
<claim-text>allocate, after encountering the obstacle during traveling, an identifier to the obstacle, wherein the operation instruction comprises an identifier of the specified obstacle; and</claim-text>
<claim-text>search, after receiving the operation instruction for the specified obstacle, for corresponding type information and location information according to the identifier of the specified obstacle.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00027" num="00027">
<claim-text><b>27</b>. The robot according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the region range comprises a region with a fixed size and shape and with a coordinate location of the specified obstacle as a geometric center; or
<claim-text>a zone in which the obstacle is located, wherein the zone is automatically divided by the self-moving robot or is manually divided by a user; or</claim-text>
<claim-text>all regions accessible by the self-moving robot.</claim-text>
</claim-text>
</claim>
</claims>
</us-patent-application>
