<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]>
<us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230225636A1-20230720.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20230704" date-publ="20230720">
<us-bibliographic-data-application lang="EN" country="US">
<publication-reference>
<document-id>
<country>US</country>
<doc-number>20230225636</doc-number>
<kind>A1</kind>
<date>20230720</date>
</document-id>
</publication-reference>
<application-reference appl-type="utility">
<document-id>
<country>US</country>
<doc-number>18180677</doc-number>
<date>20230308</date>
</document-id>
</application-reference>
<us-application-series-code>18</us-application-series-code>
<classifications-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>11</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>40</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>18</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>18</main-group>
<subgroup>2135</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>77</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
<classification-ipcr>
<ipc-version-indicator><date>20060101</date></ipc-version-indicator>
<classification-level>A</classification-level>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>84</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
</classification-ipcr>
</classifications-ipcr>
<classifications-cpc>
<main-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>1128</subgroup>
<symbol-position>F</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</main-cpc>
<further-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>7267</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>1123</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>5</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>40</main-group>
<subgroup>20</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20230101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>18</main-group>
<subgroup>295</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20230101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>F</subclass>
<main-group>18</main-group>
<subgroup>2135</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>7715</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>85</subgroup>
<symbol-position>L</symbol-position>
<classification-value>I</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>2503</main-group>
<subgroup>40</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>2503</main-group>
<subgroup>42</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20130101</date></cpc-version-indicator>
<section>A</section>
<class>61</class>
<subclass>B</subclass>
<main-group>2576</main-group>
<subgroup>00</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
<classification-cpc>
<cpc-version-indicator><date>20220101</date></cpc-version-indicator>
<section>G</section>
<class>06</class>
<subclass>V</subclass>
<main-group>10</main-group>
<subgroup>147</subgroup>
<symbol-position>L</symbol-position>
<classification-value>A</classification-value>
<action-date><date>20230720</date></action-date>
<generating-office><country>US</country></generating-office>
<classification-status>B</classification-status>
<classification-data-source>H</classification-data-source>
<scheme-origination-code>C</scheme-origination-code>
</classification-cpc>
</further-cpc>
</classifications-cpc>
<invention-title id="d2e43">AUTOMATICALLY CLASSIFYING ANIMAL BEHAVIOR</invention-title>
<us-related-documents>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>17241863</doc-number>
<date>20210427</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>11622702</doc-number>
</document-id>
</parent-grant-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>18180677</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<continuation>
<relation>
<parent-doc>
<document-id>
<country>US</country>
<doc-number>15767544</doc-number>
<date>20180411</date>
</document-id>
<parent-grant-document>
<document-id>
<country>US</country>
<doc-number>11020025</doc-number>
</document-id>
</parent-grant-document>
<parent-pct-document>
<document-id>
<country>WO</country>
<doc-number>PCT/US2016/056471</doc-number>
<date>20161011</date>
</document-id>
</parent-pct-document>
</parent-doc>
<child-doc>
<document-id>
<country>US</country>
<doc-number>17241863</doc-number>
</document-id>
</child-doc>
</relation>
</continuation>
<us-provisional-application>
<document-id>
<country>US</country>
<doc-number>62241627</doc-number>
<date>20151014</date>
</document-id>
</us-provisional-application>
</us-related-documents>
<us-parties>
<us-applicants>
<us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee">
<addressbook>
<orgname>President and Fellows of Harvard College</orgname>
<address>
<city>Cambridge</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
<residence>
<country>US</country>
</residence>
</us-applicant>
</us-applicants>
<inventors>
<inventor sequence="00" designation="us-only">
<addressbook>
<last-name>Datta</last-name>
<first-name>Sandeep Robert</first-name>
<address>
<city>Cambridge</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="01" designation="us-only">
<addressbook>
<last-name>Wiltschko</last-name>
<first-name>Alexander B.</first-name>
<address>
<city>Cambridge</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="02" designation="us-only">
<addressbook>
<last-name>Johnson</last-name>
<first-name>Matthew J.</first-name>
<address>
<city>Cambridge</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
<inventor sequence="03" designation="us-only">
<addressbook>
<last-name>Bajaj</last-name>
<first-name>Geetika</first-name>
<address>
<city>Cupertino</city>
<state>CA</state>
<country>US</country>
</address>
</addressbook>
</inventor>
</inventors>
</us-parties>
<assignees>
<assignee>
<addressbook>
<orgname>President and Fellows of Harvard College</orgname>
<role>02</role>
<address>
<city>Cambridge</city>
<state>MA</state>
<country>US</country>
</address>
</addressbook>
</assignee>
</assignees>
</us-bibliographic-data-application>
<abstract id="abstract">
<p id="p-0001" num="0000">Systems and methods are disclosed to objectively identify sub-second behavioral modules in the three-dimensional (3D) video data that represents the motion of a subject. Defining behavioral modules based upon structure in the 3D video data itself&#x2014;rather than using a priori definitions for what should constitute a measurable unit of action&#x2014;identifies a previously-unexplored sub-second regularity that defines a timescale upon which behavior is organized, yields important information about the components and structure of behavior, offers insight into the nature of behavioral change in the subject, and enables objective discovery of subtle alterations in patterned action. The systems and methods of the invention can be applied to drug or gene therapy classification, drug or gene therapy screening, disease study including early detection of the onset of a disease, toxicology research, side-effect study, learning and memory process study, anxiety study, and analysis in consumer behavior.</p>
</abstract>
<drawings id="DRAWINGS">
<figure id="Fig-EMI-D00000" num="00000">
<img id="EMI-D00000" he="84.41mm" wi="158.75mm" file="US20230225636A1-20230720-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00001" num="00001">
<img id="EMI-D00001" he="210.90mm" wi="154.60mm" orientation="landscape" file="US20230225636A1-20230720-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00002" num="00002">
<img id="EMI-D00002" he="209.04mm" wi="149.35mm" orientation="landscape" file="US20230225636A1-20230720-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00003" num="00003">
<img id="EMI-D00003" he="208.79mm" wi="152.57mm" orientation="landscape" file="US20230225636A1-20230720-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00004" num="00004">
<img id="EMI-D00004" he="209.38mm" wi="150.11mm" orientation="landscape" file="US20230225636A1-20230720-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00005" num="00005">
<img id="EMI-D00005" he="208.03mm" wi="155.02mm" orientation="landscape" file="US20230225636A1-20230720-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00006" num="00006">
<img id="EMI-D00006" he="204.55mm" wi="158.50mm" file="US20230225636A1-20230720-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00007" num="00007">
<img id="EMI-D00007" he="164.34mm" wi="161.71mm" file="US20230225636A1-20230720-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00008" num="00008">
<img id="EMI-D00008" he="174.75mm" wi="156.13mm" file="US20230225636A1-20230720-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00009" num="00009">
<img id="EMI-D00009" he="205.74mm" wi="114.05mm" orientation="landscape" file="US20230225636A1-20230720-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00010" num="00010">
<img id="EMI-D00010" he="218.44mm" wi="155.62mm" orientation="landscape" file="US20230225636A1-20230720-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00011" num="00011">
<img id="EMI-D00011" he="170.77mm" wi="154.18mm" orientation="landscape" file="US20230225636A1-20230720-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00012" num="00012">
<img id="EMI-D00012" he="196.43mm" wi="161.04mm" file="US20230225636A1-20230720-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00013" num="00013">
<img id="EMI-D00013" he="209.38mm" wi="130.13mm" orientation="landscape" file="US20230225636A1-20230720-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00014" num="00014">
<img id="EMI-D00014" he="201.08mm" wi="157.56mm" file="US20230225636A1-20230720-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00015" num="00015">
<img id="EMI-D00015" he="194.65mm" wi="158.33mm" file="US20230225636A1-20230720-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00016" num="00016">
<img id="EMI-D00016" he="209.72mm" wi="143.17mm" orientation="landscape" file="US20230225636A1-20230720-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00017" num="00017">
<img id="EMI-D00017" he="212.51mm" wi="158.16mm" orientation="landscape" file="US20230225636A1-20230720-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00018" num="00018">
<img id="EMI-D00018" he="202.61mm" wi="93.64mm" file="US20230225636A1-20230720-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00019" num="00019">
<img id="EMI-D00019" he="187.71mm" wi="111.51mm" file="US20230225636A1-20230720-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00020" num="00020">
<img id="EMI-D00020" he="184.57mm" wi="149.69mm" orientation="landscape" file="US20230225636A1-20230720-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00021" num="00021">
<img id="EMI-D00021" he="201.34mm" wi="161.63mm" file="US20230225636A1-20230720-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00022" num="00022">
<img id="EMI-D00022" he="161.29mm" wi="153.50mm" orientation="landscape" file="US20230225636A1-20230720-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00023" num="00023">
<img id="EMI-D00023" he="148.59mm" wi="154.09mm" orientation="landscape" file="US20230225636A1-20230720-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00024" num="00024">
<img id="EMI-D00024" he="206.84mm" wi="132.93mm" orientation="landscape" file="US20230225636A1-20230720-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00025" num="00025">
<img id="EMI-D00025" he="185.93mm" wi="155.62mm" orientation="landscape" file="US20230225636A1-20230720-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00026" num="00026">
<img id="EMI-D00026" he="202.10mm" wi="144.70mm" file="US20230225636A1-20230720-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00027" num="00027">
<img id="EMI-D00027" he="203.88mm" wi="125.81mm" file="US20230225636A1-20230720-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00028" num="00028">
<img id="EMI-D00028" he="199.31mm" wi="160.19mm" orientation="landscape" file="US20230225636A1-20230720-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00029" num="00029">
<img id="EMI-D00029" he="173.31mm" wi="156.13mm" orientation="landscape" file="US20230225636A1-20230720-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00030" num="00030">
<img id="EMI-D00030" he="203.88mm" wi="155.45mm" file="US20230225636A1-20230720-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00031" num="00031">
<img id="EMI-D00031" he="189.40mm" wi="160.95mm" orientation="landscape" file="US20230225636A1-20230720-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00032" num="00032">
<img id="EMI-D00032" he="210.90mm" wi="136.57mm" orientation="landscape" file="US20230225636A1-20230720-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00033" num="00033">
<img id="EMI-D00033" he="187.03mm" wi="162.73mm" file="US20230225636A1-20230720-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
<figure id="Fig-EMI-D00034" num="00034">
<img id="EMI-D00034" he="208.79mm" wi="148.00mm" orientation="landscape" file="US20230225636A1-20230720-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/>
</figure>
</drawings>
<description id="description">
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?>
<heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading>
<p id="p-0002" num="0001">This application is a continuation under 35 U.S.C. &#xa7; 120 of U.S. application Ser. No. 17/241,863 filed Apr. 27, 2021 which is a continuation under 35 U.S.C. &#xa7; 120 of U.S. application Ser. No. 15/767,544 filed Apr. 11, 2018 which is a 35 U.S.C. &#xa7; 371 National Phase Entry Application of International Application No. PCT/US2016/056471 filed Oct. 11, 2016, which designates the U.S. and claims benefit under 35 U.S.C. &#xa7; 119(e) of U.S. Provisional Application No. 62/241,627, filed Oct. 14, 2015, the contents of each of which are incorporated herein by reference in their entireties.</p>
<?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?>
<?federal-research-statement description="Federal Research Statement" end="lead"?>
<heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH</heading>
<p id="p-0003" num="0002">This invention was made with government support under OD007109 and DC011558 awarded by National Institutes of Health (NIH). The government has certain rights in this invention.</p>
<?federal-research-statement description="Federal Research Statement" end="tail"?>
<?summary-of-invention description="Summary of Invention" end="lead"?>
<heading id="h-0003" level="1">FIELD</heading>
<p id="p-0004" num="0003">The present invention is direct to system and methods for identifying and classifying animal behavior, human behavior or other behavioral metrics.</p>
<heading id="h-0004" level="1">BACKGROUND</heading>
<p id="p-0005" num="0004">The following description includes information that may be useful in understanding the present invention. It is not an admission that any of the information provided herein is prior art or relevant to the presently claimed invention, or that any publication specifically or implicitly referenced is prior art.</p>
<p id="p-0006" num="0005">The quantification of animal behavior is an essential first step in a range of biological studies, from drug discovery to understanding neurodegenerative disorders. It is usually performed by hand; a trained observer watches an animal behave, either live or on videotape, and records the timing of all interesting behaviors.</p>
<p id="p-0007" num="0006">Behavioral data for a single experiment can include hundreds of mice, spanning hundreds of hours of video, necessitating a team of observers, which inevitably decreases the reliability and reproducibility of results. In addition, what constitutes an &#x201c;interesting behavior&#x201d; is essentially left to the human observer: while it is trivial for a human observer to assign an anthropomorphic designation to a particular behavior or series of behaviors (i.e., &#x201c;rearing,&#x201d; &#x201c;sniffing,&#x201d; &#x201c;investigating,&#x201d; &#x201c;walking,&#x201d; &#x201c;freezing,&#x201d; &#x201c;eating,&#x201d; and the like), there are almost certainly behavioral states generated by the mouse that are relevant to the mouse that defy simple human categorization.</p>
<p id="p-0008" num="0007">In more advanced applications, video can be semi-automatically analyzed by a computer program. However, the brain generates behaviors that unfold smoothly over time and yet are composed of distinct patterns of motion. Individual sensory neurons that trigger action can perform behaviorally-relevant computations in as little as a millisecond, and neural populations that mediate behavior exhibit dynamics that evolve on timescales of 10s to 100s of milliseconds [1-8]. This fast neural activity interacts with slower neuromodulator systems to generate behaviors that are organized at multiple timescales simultaneously [9]. Ultimately understanding how neural circuits create complex behaviors&#x2014;particularly spontaneous or innate behaviors expressed by freely-behaving animals&#x2014;requires a clear framework for characterizing how behavior is organized at the timescales relevant to the nervous system.</p>
<heading id="h-0005" level="1">SUMMARY</heading>
<p id="p-0009" num="0008">Although behaviors have been sculpted by evolution to enable animals to accomplish particular goals (such as finding food or a mate), it is not yet clear how these behaviors are organized over time, particularly at fast timescales. However, one powerful approach to characterizing the structure of behavior arises from ethology, which proposes that the brain builds coherent behaviors by expressing stereotyped modules of simpler action in specific sequences [10]. For example, both supervised and unsupervised classification approaches have identified potential behavioral modules expressed during exploration by C. elegans and by both larval and adult D. melanogaster [11-16]. These experiments have revealed an underlying structure to behavior in these organisms, which in turn has uncovered strategies used by invertebrate brains to adapt behavior to changes in the environment. In the case of C. elegans, navigation towards an olfactory cue is mediated at least in part by neural circuits that modulate the transition probabilities that connect behavioral modules into sequences over time; seemingly new sensory-driven behaviors (like positive chemotaxis) can therefore be generated by the worm nervous system through re-sequencing of a core set of behavioral modules [17-19]. Similar observations have been made for sensory-driven behaviors in fly larvae [11].</p>
<p id="p-0010" num="0009">These insights into the underlying time-series structure of behavior arose from the ability to quantify morphological changes in worms and flies, and to use those data to identify behavioral modules [11-16]. However, it has been difficult to gain similar insight into the global organization of behavior in mammals. While innate exploratory, grooming, social approach, aggressive and reproductive behaviors in mice have all been divided by investigators into potential modules, this approach to breaking up mammalian behaviors into parts depends upon human-specified definitions for what constitutes a meaningful behavioral module (e.g. running, mating, fighting) [20-25] and are therefore largely bounded by human perception and intuition. Particularly, human perception has difficulty identifying modules spanning a short timescale.</p>
<p id="p-0011" num="0010">Systematically describing the structure of behavior in animals&#x2014;and understanding how the brain alters that structure to enable adaptation&#x2014;requires addressing three key issues. First, it is not clear which features of behavior are important to measure when attempting to modularize mouse behavior. Although most current methods track two-dimensional parameters such as the position, velocity or shape of the top-down or lateral outline of the mouse [20,22-24,26-28], mice exhibit complex three-dimensional pose dynamics that are difficult to capture but which may afford important insights into the organization of behavior. Second, given that behavior evolves on several timescales in parallel, it is not clear how to objectively identify the relevant spatiotemporal scales at which to modularize behavior. Finally, effectively characterizing behavior requires accommodating the fact that behavior is both stereotyped (a prerequisite for modularity) and variable (an inescapable feature of noisy nervous and motor systems) [29].</p>
<p id="p-0012" num="0011">This variability raises significant challenges for algorithms tasked with identifying the number and content of the behavioral modules that are expressed during a given experiment, or with assigning any given instance of an observed action to a particular behavioral module. Furthermore, identifying the spatiotemporal scales at which naturalistic behaviors are organized has been a defining challenge in ethology, and thus to date most efforts to explore the underlying structure of behavior have relied on ad hoc definitions of what constitutes a behavioral module, and have focused on specific behaviors rather than systematically considering behavior as a whole. It is not clear whether spontaneous behaviors exhibited by animals have a definable underlying structure that can be used to characterize action as it evolves over time.</p>
<p id="p-0013" num="0012">Furthermore, existing computerized systems for classification of animal behavior match parameters describing the observed behavior against hand-annotated and curated parametric databases. Therefore, in both the manual and existing semi-automated cases, subjective evaluation of the animal's behavioral state is built into the system&#x2014;a human observer must decide ahead of time what constitutes a particular behavior. This biases assessment of that behavior and limits the assessment to those particular behaviors the researcher can discriminate with human perception and is therefore limited, especially with respect to behaviors that occur on a short timescale. In addition, video acquisition systems deployed in these semi-supervised forms of behavioral analysis (nearly always acquiring data in two-dimensional) are only optimized for specific behaviors, thereby both limiting throughput and increasing wasted experimental effort through alignment errors.</p>
<heading id="h-0006" level="2">Overview</heading>
<p id="p-0014" num="0013">Despite these challenges, the inventors have discovered systems and methods for automatically identifying and classifying behavior modules of animals by processing video recordings of the animals. In accordance with the principles of the invention, a monitoring method and system uses hardware and custom software that can classify animal behavior. Classification of an animal behavioral state is determined by quantitative measurement of animal posture in three-dimensions using a depth camera. In one embodiment, a three dimensional depth camera is used to obtain a stream of video images of the animal having both area and depth information. The background image (the empty experimental area) is then removed from each of the plurality of images to generate processed images having light and dark areas. The contours of the light areas in the plurality of processed images are found and parameters from both area and depth image information within the contours is extracted to form a plurality of multi-dimensional data points, each data point representing the posture of the animal at a specific time. The posture data points can then be clustered so that point clusters represent animal behaviors.</p>
<p id="p-0015" num="0014">This data may then be fed into a model free algorithm, or fed into computation model to characterize the structure of naturalistic behavior. In some embodiments, the systems fit models for behavior using methods in Bayesian inference, which allows unsupervised identification of the optimal number and identity of behavioral modules from within a given dataset. Defining behavioral modules based upon structure in the three dimensional behavioral data itself&#x2014;rather than using a priori definitions for what should constitute a measurable unit of action&#x2014;identifies a previously-unexplored sub-second regularity that defines a timescale upon which behavior is organized, yields key information about the components and structure of behavior, offers insight into the nature of behavioral change, and enables objective discovery of subtle alterations in patterned action.</p>
<heading id="h-0007" level="2">Example Application to Video of Mouse Exploring Open Field</heading>
<p id="p-0016" num="0015">In one example, the inventors measured how the shape of a mouse's body changes as it freely explores a circular open field. The inventors used depth sensors to capture three-dimensional (&#x201c;3D&#x201d;) pose dynamics of the mouse, and then quantified how the mouse's pose changed over time by centering and aligning the image of the mouse along the inferred axis of its spine.</p>
<p id="p-0017" num="0016">Plotting this three dimensional data over time revealed that mouse behavior is characterized by periods during which pose dynamics evolve slowly, punctuated by fast transitions that separate these periods; this pattern appears to break up the behavioral imaging data into blocks consisting of a small number of frames typically lasting from 200-900 ms. This suggests that mouse behavior may be organized at two distinct timescales, the first defined by the rate at which a mouse's pose can change within a given block, and the second defined by the transition rate between blocks.</p>
<p id="p-0018" num="0017">Characterizing mouse behavior within these blocks&#x2014;and determining how behavior might differ between blocks&#x2014;requires first estimating the timescales at which these blocks are organized. In some embodiments, to identify approximate boundaries between blocks, the behavioral imaging data was submitted to a changepoint algorithm designed to detect abrupt changes in the structure of data over time. In one example, this method automatically identified potential boundaries between blocks, and revealed that the mean block duration was about 350 ms.</p>
<p id="p-0019" num="0018">Additionally, the inventors performed autocorrelation and spectral analysis, which provided complementary information about the timescale of behavior. Temporal autocorrelation in the mouse's pose largely dissipated within 400 ms (tau=340&#xb1;58 ms,) and nearly all of the frequency content that differentiated behaving and dead mice concentrated between 1 and 6 Hz (measured by spectrum ratio, or Wiener filter, mean 3.75&#xb1;0.56 Hz); these results suggest that most of the dynamism in the mouse's behavior occurs within 200-900 ms timescale.</p>
<p id="p-0020" num="0019">Additionally, visual inspection of the block-by-block pattern of behavior exhibited by a mouse reveals that each block appears to encode a brief motif of behavior (e.g. a turn to the right or left, a dart, a pause, the first half of a rear) separated from the subsequent behavioral motif by a fast transition. Taken together, these findings reveal a previously-unappreciated sub-second organization to mouse behavior&#x2014;during normal exploration mice express brief motifs of movement that appear to rapidly switch from one to another in series.</p>
<p id="p-0021" num="0020">The finding that behavior is naturalistically broken up into brief motifs of motion indicates that each of these motifs is a behavioral module: a stereotyped and reused unit of behavior that the brain places into sequences to build more complex patterns of action. Next, systems and method are disclosed for identifying multiple examples of the same stereotyped sub-second motif of behavior.</p>
<heading id="h-0008" level="2">Processing Algorithms and Methods for Identifying Modules in Video Data</heading>
<p id="p-0022" num="0021">To identify similar modules, mouse behavioral data may first be subject to principal components analysis (PCA) or other dimensionality reduction algorithm and the first two principal components may be plotted. Each block in the pose dynamics data corresponds to a continuous trajectory through PCA space; for example, an individual block associated with the mouse's spine being elevated corresponded to a specific sweep through PCA space. Scanning the behavioral data for matching motifs using a template matching method identified several additional examples of this sweep in different animals, suggesting that each of these PCA trajectories may represent individual instances in which a stereotyped behavioral module was reused.</p>
<p id="p-0023" num="0022">Given this evidence for sub-second modularity, the inventors devised a series of computational models&#x2014;each of which describes a different underlying structure for mouse behavior&#x2014;trained these models on 3D behavioral imaging data, and determined which models predicted or identified the underlying structure of mouse behavior. Particularly, the inventors utilized computational inference methods (including Bayesian non-parametric approaches and Gibbs sampling) that are optimized to automatically identify structure within large datasets.</p>
<p id="p-0024" num="0023">Each model differed in whether it considered behavior to be continuous or modular, in the possible contents of the modules, and in the transition structure that governed how modules were placed into sequence over time. To compare model performance, the models were tested to predict the contents and structure of real mouse behavioral data to which the models had not been exposed. Among the alternatives, the best quantitative predictions were made by a model that posits that mouse behavior is composed of modules (each capturing a brief motif of 3D body motion) that switch from one to another at the sub-second timescales identified by our model-free analysis of the pose dynamics data.</p>
<heading id="h-0009" level="2">AR-HMM Model</heading>
<p id="p-0025" num="0024">One model represented each behavioral module as a vector autoregressive (AR) process capturing a stereotyped trajectory through PCA space. Additionally in that model, the switching dynamics between different modules were represented using a Hidden Markov Model (HMM). Together, this model is referred to herein as &#x201c;AR-HMM.&#x201d;</p>
<p id="p-0026" num="0025">In some embodiments, AR-HMM makes predictions about mouse behavior based upon its ability to discover (within training data) the set of behavioral modules and transition patterns that provide the most parsimonious explanation for the overall structure of mouse behavior as it evolves over time. Accordingly, a trained AR-HMM can be used to reveal the identity of behavioral modules and their transition structure from within a behavioral dataset, and thereby expose the underlying organization of mouse behavior. After training, the AR-HMM can assign every frame of the training behavioral data to one of the modules it has discovered, revealing when any given module is expressed by mice during a given experiment.</p>
<p id="p-0027" num="0026">Consistent with the AR-HMM recognizing the inherent block structure in the 3D behavioral data, the module boundaries identified by the AR-HMM respected the inherent block structure embedded within the pose dynamics data. Furthermore, the model-identified module duration distribution was similar to the changepoints-identified block duration distribution; however, the module boundaries identified by the AR-HMM refined the approximate boundaries suggested by the changepoints analysis (78 percent of module boundaries were within 5 frames of a changepoint). Importantly, the ability of the AR-HMM to identify behavioral modules depended upon the inherent sub-second organization of mouse pose data, as shuffling the frames that make up the behavioral data in small chunks (i.e. &#x3c;300 milliseconds) substantially degraded model performance, while shuffling the behavioral data in bigger chunks had little effect. These results demonstrate that the AR-HMM recognizes the inherent sub-second block structure of the behavioral data.</p>
<p id="p-0028" num="0027">Additionally, specific behavioral modules identified by the AR-HMM encoded a set of distinct and reused motifs of motion. For instance, the PCA trajectories assigned by the model to one behavioral module traced similar paths through PCA space. Consistent with each of these trajectories encoding a similar motif of action, collating and inspecting the 3D movies associated with multiple data instances of this specific module confirmed that it encodes a stereotyped motif of behavior, one human observers would refer to as rearing. In contrast, data instances drawn from different behavioral modules traced distinct paths through PCA space. Furthermore, visual inspection of the 3D movies assigned to each of these modules demonstrated that each encodes a repeatedly used and coherent pattern of three-dimensional motion that can be distinguished and labeled with descriptors (e.g., &#x201c;walk,&#x201d; &#x201c;pause,&#x201d; and &#x201c;low rear&#x201d; modules).</p>
<p id="p-0029" num="0028">To quantitatively and comprehensively assess the distinctiveness of each behavioral module identified by the AR-HMM, the inventors performed a cross-likelihood analysis, which revealed that the data instances associated with a given module are best assigned to that module, and not to any of the other behavioral modules in the parse. In contrast, the AR-HMM failed to identify any well-separated modules in a synthetic mouse behavioral dataset that lacks modularity, demonstrating that the discovered modularity within the real behavioral data is a feature of the dataset itself rather than being an artifact of the model. Furthermore, restarting the model training process from random starting points returns the same or a highly similar set of behavioral modules, consistent with the AR-HMM homing in on and identifying an intrinsic modular structure to the behavioral data. Together these data suggest that mouse behavior&#x2014;when viewed through the lens of the AR-HMM&#x2014;is fundamentally organized into distinct sub-second modules.</p>
<p id="p-0030" num="0029">Additionally, if the AR-HMM identifies behavioral modules and transitions that make up mouse behavior, then synthetic behavioral data generated by a trained AR-HMM can provide a reasonable facsimile of real pose dynamics data. The AR-HMM appeared to capture the richness of mouse behavior, as synthetic behavioral data (in the form of spine dynamics, or a 3D movie of a behaving mouse) was qualitatively difficult to distinguish from behavioral data generated by an actual animal. Mouse pose dynamics data therefore appear to have an intrinsic structure organized on sub-second timescales that is well-parsed by the AR-HMM into defined modules; furthermore, optimal identification of these modules and effective prediction of the structure of behavior requires overt modeling of modularity and switching dynamics.</p>
<p id="p-0031" num="0030">The systems and methods of the present invention can be applied to a variety of animal species, such as animals in animal models, humans in clinical trials, humans in need of diagnosis and/or treatment for a particular disease or disorder. Without limitations, these animals include mice, dogs, cats, cows, pigs, goats, sheep, rats, horses, guinea pigs, rabbits, reptiles, zebrafish, birds, fruit flies, worms, amphibians (e.g., frogs), chickens, non-human primates, and humans.</p>
<p id="p-0032" num="0031">The systems and methods of the present invention can be used in a variety of applications including, but not limited to, drug screening, drug classification, genetic classification, disease study including early detection of the onset of a disease, toxicology research, side-effect study, learning and memory process study, anxiety study, and analysis in consumer behavior.</p>
<p id="p-0033" num="0032">The systems and methods of the present invention are particularly useful for diseases that affect the behavior of a subject. These diseases include neurodegenerative diseases such as Parkinson's disease, Huntington's disease, Alzheimer's disease, and Amyotrophic lateral sclerosis, neurodevelopmental psychiatric disorders such as attention deficit hyperactivity disorder, autism, Down syndrome, Mendelsohnn's Syndrome, and Schizophrenia.</p>
<p id="p-0034" num="0033">In some embodiments, the systems and methods of the present invention can be used to study how a known drug or test compound can alter the behavioral state of a subject. This can be done by comparing the behavioral representations obtained before and after the administration of the known drug or test compound to the subject. As used herein, the term &#x201c;behavioral representation&#x201d; refers to a set of sub-second behavioral modules and their transition statistics determined using the systems or methods of the invention. Without limitation, the behavioral representation can be in the form of a matrix, a table, or a heatmap.</p>
<p id="p-0035" num="0034">In some embodiments, the systems and methods of the present invention can be used for drug classification. The systems and methods of the present invention can create a plurality of reference behavioral representations based on existing drugs and the diseases or disorders they treat, wherein each reference behavioral representation represents a class of drugs (e.g., antipsychotic drugs, antidepressants, stimulants, or depressants). A test behavioral representation can be compared to the plurality of reference behavioral representation, and if the test behavioral representation is similar to one of the plurality of reference behavioral representations, the test compound is determined to belong to the same class of drugs that is represented by said particular of reference behavioral representation. Without limitation, the test compound can be a small molecule, an antibody or an antigen-binding fragment thereof, a nucleic acid, a polypeptide, a peptide, a peptidomimetic, a polysaccharide, a monosaccharide, a lipid, a glycosaminoglycan, or combinations thereof.</p>
<p id="p-0036" num="0035">In some embodiments, this may include a system for automatically classifying an animal's behavior as belonging to one class of drugs versus a list of alternatives. For instance, to develop the system, we may provide a training set of many mice under many different drug conditions, and build a linear or non-linear classifier to discover what combinations and ranges of features constitute membership in a particular drug class. This classifier may be then fixed as soon as training is completed, allowing us to apply it to previously unseen mice. Potential classifier algorithms may include logistic regression, support vector machine with linear basis kernel, support vector machine with radial basis function kernel, multi-layer perceptron, random forest classifier, or k-Nearest Neighbors classifier.</p>
<p id="p-0037" num="0036">Similar to drug classification, in some embodiments, the systems and methods of the present invention can be used in gene-function classification.</p>
<p id="p-0038" num="0037">In someone embodiments of drug screening, an existing drug that is known to treat a particular disease or disorder can be administered to a first test subject. The systems and methods of the present invention can then be used on the first test subject to obtain a reference behavioral representation, which includes a set of behavioral modules that can characterize the therapeutic effects of the drug on the first test subject. Subsequently, a test compound can be administered to a second test subject of the same animal type as the first test subject. The systems and methods of the present invention can then be used on the second test subject to obtain a test behavioral representation. If the test behavioral representation is found to be similar to the reference behavioral representation, the test compound is determined to be effective in treating the particular disease or disorder. If the test behavioral representation is found to not be similar to the reference behavioral representation, the test compound is determined to be ineffective in treating the particular disease or disorder. It should be noted that the first and second test subject can each be a group of test subjects, and the behavioral representation obtained can be an average behavioral representation.</p>
<p id="p-0039" num="0038">Similar to drug screening, in some embodiments, the systems and methods of the present invention can be used in gene-therapy screening. Gene therapies can include delivery of a nucleic acid and gene knockout.</p>
<p id="p-0040" num="0039">In some embodiments, the systems and methods of the present invention can be used in the study of disease or disorder. For example, the systems and methods of the invention can be used to discover new behavioral modules in subjects having a particular disease or disorder. For example, the systems and methods of the present invention can permit early diagnosis of a disease or disorder by identifying a reference behavioral representation in subjects having the disease or disorder or subjects that are in the process of developing the disease or disorder. If the reference behavioral representation or a significant portion thereof is also observed in a subject suspected of having the disease or disorder, the subject is diagnosed as having the disease or disorder. Thus early clinical interventions can be administered to the subject.</p>
<p id="p-0041" num="0040">Additionally, in some embodiments, the systems and methods of the present invention can be used in the in the study of consumer behavior, for example, how a consumer responds to a scent (e.g., perfume). The systems and methods of the present invention can be used to identify a reference behavioral representation that represents positive reactions to the scent. In the presence of the scent, a person exhibiting the reference behavioral representation or a significant portion thereof is determined to be reacting positively to the scent. Reference behavioral representation that represents negative reactions to the scent can also be identified and used to gauge a person's reaction.</p>
<?summary-of-invention description="Summary of Invention" end="tail"?>
<?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?>
<description-of-drawings>
<heading id="h-0010" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading>
<p id="p-0042" num="0041">The accompanying drawings, which are incorporated in and constitute a part of this specification, exemplify the embodiments of the present invention and, together with the description, serve to explain and illustrate principles of the invention. The drawings are intended to illustrate major features of the exemplary embodiments in a diagrammatic manner. The drawings are not intended to depict every feature of actual embodiments nor relative dimensions of the depicted elements, and are not drawn to scale.</p>
<p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts, in accordance with various embodiments of the present invention, a diagram of a system designed to capture video data of an animal;</p>
<p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> depicts, in accordance with various embodiments of the present invention, a flow chart showing processing steps performed on video data;</p>
<p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> depicts, in accordance with various embodiments of the present invention, a flow chart showing processing steps performed on video data;</p>
<p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts, in accordance with various embodiments of the present invention, a flow chart showing analysis performed on the video data output from the processing steps;</p>
<p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts, in accordance with various embodiments of the present invention, a flow chart showing the implementation of an AR-HMM algorithm;</p>
<p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> depicts, in accordance with various embodiments of the present invention, a graph showing the proportion of frames explained by each module (Y axis), plotted against the set of modules, sorted by usage (X axis);</p>
<p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> depicts, in accordance with various embodiments of the present invention, a graph showing modules (X axis) sorted by usage (Y axis) with Bayesian credible intervals indicated;</p>
<p id="p-0050" num="0049"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>E</figref> depict, in accordance with various embodiments of the present invention, the influences of the physical environment on module usage and spatial pattern of expression. <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. Modules identified by the AR-HMM sorted by usage (n=25 mice, 500 total minutes, data from circular open field). <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. Hinton diagram of the observed bigram probabilities, depicting the probability that any pair of modules are observed as ordered pairs.</p>
<p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref>. Module usage, sorted by context. Mean usages across animals depicted with dark lines, with bootstrap estimates depicted in fainter lines (n=100). Marked modules discussed in main text and shown in <figref idref="DRAWINGS">FIG. <b>6</b>D</figref>: square=circular thigmotaxis, circle=rosette, diamond=square thigmotaxis, cross=square dart. <figref idref="DRAWINGS">FIG. <b>6</b>D</figref>. Occupancy graph of mice in circular open field (left, n=25, 500 minutes total) indicating average spatial positions across all experiments. Occupancy graph depicting deployment of circular thigmotaxis module (middle, average orientation across the experiment indicated as arrow field) and circle-enriched rosette module (right, orientation of individual animals indicated with arrows). <figref idref="DRAWINGS">FIG. <b>6</b>E</figref>. Occupancy graph of mice in square box (left, n=15, 300 minutes total) indicating cumulative spatial positions across all experiments. Occupancy graph depicting a square-enriched thigmophilic module (middle, average orientation across the experiment indicated as arrow field), and square-specific darting module (right, orientation of individual animals indicated with arrows).</p>
<p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts, in accordance with various embodiments of the present invention, a histogram depicting the average velocity of the modules that were differentially upregulated and interconnected after TMT exposure &#x201c;freezing&#x201d; compared to all other modules in the dataset.</p>
<p id="p-0053" num="0052"><figref idref="DRAWINGS">FIGS. <b>8</b>A-<b>8</b>E</figref> depict, in accordance with various embodiments of the present invention, depict graphs illustrating how odor avoidance alters transition probabilities. <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>. Occupancy plot of mice under control conditions (n=24, 480 total minutes) and exposed to the monomolecular fox-derived odorant trimethylthiazoline (TMT, 5% dilution in carrier DPG, n=15, 300 total minutes) in the lower left quadrant (arrow). <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>. Module usage plot sorted by &#x201c;TMT-ness. Dark lines depict mean usages, bootstrap estimates depicted in fainter lines. Marked modules discussed in this specification and <figref idref="DRAWINGS">FIG. <b>8</b>E</figref>: square=sniff in TMT quadrant, circle=freeze away from TMT. <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, left and middle. Behavioral state maps for mice exploring a square box under control conditions (blank) and after TMT exposure, with modules depicted as nodes (usage proportional to the diameter of each node), and bigram transition probabilities depicted as directional edges. The two-dimensional layout is meant to minimize the overall distance between all connected nodes and is seeded by spectral clustering to emphasize neighborhood structure. <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>. Statemap depiction of the difference between blank and TMT. Usage differences are indicated by the newly sized colored circles (upregulation indicated in blue, downregulation indicated in red, blank usages indicated in black). Altered bigram probabilities are indicated in the same color code. <figref idref="DRAWINGS">FIG. <b>8</b>D</figref>. Mountain plot depicting the joint probability of module expression and spatial position, plotted with respect to the TMT corner (X axis); note that the &#x201c;bump&#x201d; two-thirds of the way across the graph occurs due to the two corners equidistant from the odor source. <figref idref="DRAWINGS">FIG. <b>8</b>E</figref>. Occupancy plot indicating spatial position in which mice after TMT exposure emit an investigatory sniffing module (left) or a pausing module.</p>
<p id="p-0054" num="0053"><figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>C</figref> depict, in accordance with various embodiments of the present invention, graphs illustrating how the AR-HMM disambiguates wild-type, heterozygous and homozygous mice. <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>. Usage plot of modules exhibited by mice (n=6+/+, n=4+/&#x2212;, n=5 &#x2212;/&#x2212;, open field assay, 20 minute trials), sorted by &#x201c;mutant-ness&#x201d;. Mean usages across animals depicted with dark lines, with bootstrap estimates depicted in fainter lines. <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>. State map depiction of baseline OFA behavior for +/+animals as in <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> (left); difference state maps as in <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> between the +/+ and +/&#x2212;genotype (middle), and +/+ and &#x2212;/&#x2212; genotype (right). <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>. Illustration of the &#x201c;waddle&#x201d; module in which the hind limbs of the animal are elevated above the shoulder girdle, and the animal locomotes forward with a wobbly gait.</p>
<p id="p-0055" num="0054"><figref idref="DRAWINGS">FIGS. <b>10</b>A-<b>10</b>B</figref> depict, in accordance with various embodiments of the present invention, graphs illustrating how optogenetic perturbation of the motor cortex yields both neomorphic and physiological modules. <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>. Mountain plot depicting the probability of expression of each behavioral module (each assigned a unique color on the Y axis) as a function of time (X axis), with two seconds of light stimulation initiated at time zero (each plot is the average of 50 trials). Note that because of the trial structure (in which mice were sequentially exposed to increasing light levels) modest variations in the baseline pattern of behavior are captured before light onset across conditions. Stars indicate two modules that are expressed during baseline conditions that are also upregulated at intermediate powers (11 mW) but not high powers (32 mW); cross indicates pausing module upregulated at light offset. <figref idref="DRAWINGS">FIG. <b>10</b>B</figref>. Average position of example mice (with arrows indicating orientation over time) of the two modules induced under the highest stimulation conditions. Note that these plots are taken from one animal and representative of the complete dataset (n=4); because of variability in viral expression the threshold power required to elicit behavioral changes varied from animal to animal, but all expressed the spinning behaviors identified in <figref idref="DRAWINGS">FIG. <b>10</b>A</figref>.</p>
<p id="p-0056" num="0055"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>11</b>C</figref> depict, in accordance with various embodiments of the present invention, graphs illustrating how depth imaging reveals block structure in mouse pose dynamics data. <figref idref="DRAWINGS">FIG. <b>11</b>A</figref> depicts Imaging a mouse in the circular open field with a standard RGB camera (left) and a 3D depth camera (right, mouse height is color mapped, mm=mm above floor) captures the three-dimensional pose of the mouse. <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> depicts an arrow that indicates the inferred axis of the animal's spine; all mouse images are centered and aligned along this axis to enable quantitative measurements of pose dynamics over time during free behavior. Visualization of pose data reveals inherent block structure within 3D pose dynamics. Compression of pre-processed and spine-aligned data through the random projections technique reveals sporadic sharp transitions in the pose data as it evolves over time. Similar data structure was observed in the raw data and in the height of the spine of the animal as it behaves (upper panel, spine height at any given position is colormapped, mm=mm above floor). When the animal is rearing (as it is here at the beginning of the datastream), its cross-sectional profile with respect to the camera becomes smaller; when the animal is on all fours its profile becomes larger.</p>
<p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>11</b>C</figref> shows a changepoints analysis which identifies potential boundaries between these blocks (normalized probability of a changepoint indicated in the trace at the bottom of the behavioral data). Plotting the duration of each block as identified by the changepoints analysis reveals a block duration distribution (n=25, 500 total minutes imaging, mean=358 ms, SD 495 ms). Mean block duration values are plotted in black, with the duration distribution associated with each individual mouse plotted in gray. <figref idref="DRAWINGS">FIG. <b>11</b>C</figref>, middle and right. Autocorrelation analysis reveals that the rate of decorrelation in the mouse's pose slows after about 400 milliseconds (left, mean plotted in dark blue, individual mouse autocorrelations plotted in light blue, tau=340&#xb1;58 ms). Plotting the ratio in spectral power between a behaving and dead mouse (right, mean plotted in black, individual mice plotted in grey) reveals most behavioral frequency content is represented between 1 and 6 Hz (mean=3.75&#xb1;0.56 hz);</p>
<p id="p-0058" num="0057"><figref idref="DRAWINGS">FIGS. <b>12</b>A-<b>12</b>D</figref> depict, in accordance with various embodiments of the present invention, graphs illustrating how mouse pose dynamics data contains reused behavioral modules. <figref idref="DRAWINGS">FIG. <b>12</b>A</figref> depicts how a projection of mouse pose data into Principal Components (PC) space (bottom) reveals that the individual blocks identified in the pose data encode reused trajectories. After subjecting mouse pose data to principal components analysis, the values of the first two PCs at each point in time were plotted in a two-dimensional graph (point density is colormapped). Tracing out the path associated with a block highlighted by changepoints analysis (top) identifies a trajectory through PC space (white). By searching through pose data using a template matching procedure, additional examples of this block were identified that encoded similar trajectories through PC space (time indicated as progression from blue to red), suggesting that the template block represented a reused motif of motion. <figref idref="DRAWINGS">FIG. <b>12</b>B</figref> depicts modeling mouse pose data with the AR-HMM identifies individual behavioral modules. The AR-HMM parses the behavioral data into a limited set of identifiable modules (top&#x2014;marked &#x201c;labels&#x201d;, each module is uniquely color coded). Multiple data instances associated with a single behavioral module each take a stereotyped trajectory through PCA space (bottom left, trajectories in green); multiple trajectories define behavioral sequences (bottom center). Depicting the side-on view of the mouse (inferred from depth data, bottom right) reveals that each trajectory within a behavioral sequence encodes a different elemental action (time within the module is indicated as increasingly darker lines, from module start to end). <figref idref="DRAWINGS">FIG. <b>12</b>C</figref> depicts isometric-view illustrations of the three-dimensional imaging data associated with walk, pause and low rear modules. <figref idref="DRAWINGS">FIG. <b>12</b>D</figref> depicts cross-likelihood analysis depicting the probability that a data instance assigned to a particular module will be effectively modeled by another module. Cross-likelihoods were computed for the open field dataset, and the likelihood that any given data instance assigned to a particular module would be accurately modeled by a different module is heatmapped (units are nats, where enats is the likelihood ratio); note the high-likelihood diagonal, and the low likelihoods associated for all off-diagonal comparisons. Plotting the same metric on a model trained on synthetic data whose autocorrelation structure matches actual mouse data but which lacks any modularity reveals that the AR-HMM fails to identify modules in the absence of underlying modularity in the training data.</p>
<p id="p-0059" num="0058"><figref idref="DRAWINGS">FIGS. <b>13</b>Ai-<b>13</b>B</figref> depict, in accordance with various embodiments of the present invention, graphs illustrate block and autocorrelation structure in Mouse Depth Imaging Data. <figref idref="DRAWINGS">FIG. <b>13</b>Ai</figref> &#x2212;13Aii depict that a block structure is present in random projections data, spine data and raw pixel data derived from aligned mouse pose dynamics. <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrates that live mice exhibit significant block structure in imaging data (left panels), while dead mice do not (right panels). Compression does not significantly affect autocorrelation structure mouse pose dynamics data. Raw pixels, PCA data and random projections representing the same depth dataset (left panel) all decorrelate at approximately the same rate, demonstrating that data compression does not influence fine-timescale correlation structure in the imaging data. This correlation structure is not observed if mice poses evolve as if taking a Levy flight (middle panel) or random walk (right panel), suggesting that live mice express a specific sub-second autocorrelation structure potentially associated with switching dynamics.</p>
<p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts, in accordance with various embodiments of the present invention, a graph illustrating the variance explained after dimensional rejection using Principal Components Analysis. A Plot comparing variance explained (Y axis) with the number of included PCA dimensions (X axis) reveals that 88 percent of the variance is captured by the first 10 principal components; this number of dimensions was used for data analysis by the AR-HMM.</p>
<p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>15</b>A-<b>15</b>B</figref> depicts, in accordance with various embodiments of the present invention, a graph illustrating the comparative modeling of mouse behavior. A series of computational models of behavior were composed, each instantiating a distinct hypothesis about the underlying structure of behavior, and each of these models was trained on mouse behavioral data (in the form of the top 10 principal components extracted from aligned depth data). These models included a Gaussian model (which proposes that mouse behavior is a single Gaussian in pose space), a GMM (a Gaussian Mixture Model, which proposes that mouse behavior is a mixture of Gaussians in pose space), a Gaussian HMM (a Gaussian Hidden Markov Model, which proposes that behavior created from modules, each a Gaussian in pose space, that are interconnected in time with definable transition statistics), a GMM HMM (a Gaussian Mixture Model Hidden Markov Model, which proposes that behavior created from modules, each a mixture of Gaussians in pose space, that are interconnected in time with definable transition statistics), an AR model (which proposes that mouse behavior is a single, continuous autoregressive trajectory through pose space), an AR MM (which proposes that mouse behavior is built from modules, each of which encodes a autoregressive trajectory through pose space, and which transition from one to another randomly), and a AR sHMM (which proposes that mouse behavior is built from modules, each of which encodes a autoregressive trajectory through pose space, and which transition from one to another with definable transition statistics). The performance of these models at predicting the structure of mouse behavioral data these models to which these models had not been exposed is shown on the Y axis (measured in likelihood units, and normalized to the performance of the Gaussian model), and the ability of each model to predict behavior on a frame-by-frame basis is shown on the X axis (upper). Three slices are taken through this plot at different points in time, demonstrating that the optimal AR HMM outperforms alternative models at timescales at which the switching dynamics inherent in the data come into play (e.g. after more than 10 frames, error bars are SEM).</p>
<p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>16</b></figref> depicts, in accordance with various embodiments of the present invention, a graph illustrating duration distributions for blocks and modules that are qualitatively similar. Percentage of blocks/modules of a given duration (Y axis) plotted against block duration (X axis) reveals roughly similar duration distributions for the changepoints algorithm identified blocks, and the model-identified behavioral modules. These distributions are expected to be similar although not identical, as the changepoints algorithm identifies local changes in data structure, while the model identifies modules based upon their contents and their transition statistics; note that the model has no direct access to the &#x201c;local fracture&#x201d; metrics used by the changepoints algorithm.</p>
<p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>17</b></figref> depicts, in accordance with various embodiments of the present invention, a graph illustrating how shuffling behavioral data at fast timescales that lowers AR-HMM performance.</p>
<p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>18</b></figref> depicts, in accordance with various embodiments of the present invention, graphs illustrating a visualization of model-generated mouse behavior, each of the models was trained on behavioral data (left) and then allowed to generate its &#x201c;dream&#x201d; version of mouse behavior (right); here that output is visualized at the shape of the spine of the animal over time. The individual modules identified by each model are indicated as a color code underneath each model (marked &#x201c;labels&#x201d;).</p>
<p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>19</b></figref> depicts, in accordance with various embodiments of the present invention, a graph illustrating how module interconnectivity is sparse. Without thresholding the average module is interconnected with 16.85&#xb1;0.95 other modules; this modest interconnectivity falls sharply with even modest thresholding (X axis, thresholding applied to bigram probabilities), consistent with sparse temporal interconnectivity between individual behavioral modules.</p>
<p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>20</b></figref> depicts, in accordance with various embodiments of the present invention, graphs illustrating the identification filtering parameters. To filter data from the Kinect we used iterative an iterative median filtering approach in which we applied a median filter iteratively both in space and in time; this approach has been shown to effectively maintain data structure while smoothing away noise. To identify optimal filter settings, we imaged dead mice that were differentially posed in rigor mortis; ideal filter settings would distinguish mice that were posed differently, but be unable to distinguish data from the same mouse. Filter setting are indicated as ((pixels), (frames)) with the numbers within each parenthesis referring to the iterative settings for each round of filtering. To assess filter performance, we computed a within/between pose correlation ratio (Y axis), in which the mean spatial correlation for all frames of the same pose was divided by the mean spatial correlation for all frames of different poses. This revealed that light filtering (with settings ((<b>3</b>), (<b>3</b>,<b>5</b>))) optimized discriminability in the data.</p>
<p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>21</b></figref> depicts, in accordance with various embodiments of the present invention, a graph identifying changepoint algorithm parameters. By optimizing against the changepoints ratio (number of changepoints identified in live mice versus dead mice, Y axis), clear optimal values were identified via grid scanning for sigma and H (left two panels). This changepoint ratio was not highly sensitive to K; a setting of 48 (at the observed maximum) was therefore chosen.</p>
<p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>22</b></figref> depicts, in accordance with various embodiments of the present invention, a graphical model for the AR-HMM. The shaded nodes labeled y_t for time indices t=1,2, . . . ,T represent the preprocessed 3D data sequence. Each such data node y_t has a corresponding state node x_t which assigns that data frame to a behavioral mode. The other nodes represent the parameters which govern the transitions between modes (i.e. the transition matrix &#x3c0;) and the autoregressive dynamical parameters for each mode (i.e. the set of parameters &#x3b8;).</p>
</description-of-drawings>
<?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?>
<?detailed-description description="Detailed Description" end="lead"?>
<p id="p-0069" num="0068">In the drawings, the same reference numbers and any acronyms identify elements or acts with the same or similar structure or functionality for ease of understanding and convenience. To easily identify the discussion of any particular element or act, the most significant digit or digits in a reference number refer to the Figure number in which that element is first introduced.</p>
<heading id="h-0011" level="1">DETAILED DESCRIPTION</heading>
<p id="p-0070" num="0069">In some embodiments, properties such as dimensions, shapes, relative positions, and so forth, used to describe and claim certain embodiments of the invention are to be understood as being modified by the term &#x201c;about.&#x201d;</p>
<p id="p-0071" num="0070">Various examples of the invention will now be described. The following description provides specific details for a thorough understanding and enabling description of these examples. One skilled in the relevant art will understand, however, that the invention may be practiced without many of these details. Likewise, one skilled in the relevant art will also understand that the invention can include many other obvious features not described in detail herein. Additionally, some well-known structures or functions may not be shown or described in detail below, so as to avoid unnecessarily obscuring the relevant description.</p>
<p id="p-0072" num="0071">The terminology used below is to be interpreted in its broadest reasonable manner, even though it is being used in conjunction with a detailed description of certain specific examples of the invention. Indeed, certain terms may even be emphasized below; however, any terminology intended to be interpreted in any restricted manner will be overtly and specifically defined as such in this Detailed Description section.</p>
<p id="p-0073" num="0072">While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular implementations of particular inventions. Certain features that are described in this specification in the context of separate implementations can also be implemented in combination in a single implementation. Conversely, various features that are described in the context of a single implementation can also be implemented in multiple implementations separately or in any suitable sub-combination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.</p>
<p id="p-0074" num="0073">Similarly while operations may be depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p>
<heading id="h-0012" level="2">Overview</heading>
<p id="p-0075" num="0074">The inventors have discovered systems and methods for automatically and objectively identifying and classifying behavior modules of animals by processing video data of the animals. These systems may classify animal behavioral state by quantitative measurement, processing, and analysis of an animal posture or posture trajectory in three-dimensions using a depth camera. These system and methods obviate the need for a priori definition for what should constitute a measurable unit of action, thus making the classification of behavioral states objective and unsupervised.</p>
<p id="p-0076" num="0075">In one aspect, the invention relates to a method for analyzing the motion of a subject to separate it into sub-second modules, the method comprising: (i) processing three dimensional video data that represent the motion of the subject using a computational model to partition the video data into at least one set of sub-second modules and at least one set of transition periods between the sub-second modules; and (ii) assigning the at least one set of sub-second modules to a category that represents a type of animal behavior.</p>
<p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an embodiment of the process a system may utilize to automatically classify video frames or sets of frames into behavior modules. For instance, the system may include a video recorder <b>100</b> and tracking system <b>110</b>. In some embodiments, video recorder <b>100</b> may be a 3D depth camera and the tracking system <b>110</b> may project structured infrared light into the experimental field <b>10</b>. Infrared receivers on the tracking system may be able to determine the location of an object based on parallax. In some embodiments, the video recorder <b>100</b> may be connected to the tracking system <b>110</b> or in some embodiments they may be separate components.</p>
<p id="p-0078" num="0077">The video recorder <b>100</b> may output data related to video images and or tracking data from the tracking system <b>110</b> to a computing device <b>113</b>. In some embodiments, the computing device <b>113</b> will perform pre-processing of the data locally before sending over a network <b>120</b> to be analyzed by a server <b>130</b> and to be saved in a database <b>160</b>. In other embodiments, the data may be processed, and fit locally on a computing device <b>113</b>.</p>
<p id="p-0079" num="0078">In one embodiment, a 3D depth camera <b>100</b> is used to obtain a stream of video images of the animal <b>50</b> having both area and depth information. The background image (the empty experimental area) is then removed from each of the plurality of images to generate processed images having light and dark areas. The contours of the light areas in the plurality of processed images can be found and parameters from both area and depth image information within the contours can then be extracted to form a plurality of multi-dimensional data points, each data point representing the posture of the animal at a specific time. The posture data points can then be clustered so that point clusters represent animal behaviors.</p>
<p id="p-0080" num="0079">Then, the preprocessed depth-camera video data may be input into the various models in order to classify the video data into sub-second &#x201c;modules&#x201d; and transition periods that describe repeated units of behavior that are assembled together to form coherent behaviors observable by the human eye. The output of the models that classify the video data into modules may output several key parameters including: (1) the number of behavioral modules observed within a given set of experimental data (i.e. the number of states), (2) the parameters that describe the pattern of motion expressed by the mouse associated with any given module (i.e. state-specific autoregressive dynamical parameters), (3) the parameters that describe how often any particular module transitions to any other module (i.e. the state transition matrix), and (4) for each video frame an assignment of that frame to a behavioral module (i.e. a state sequence associated with each data sequence). In some embodiments, these latent variables were defined by a generative probabilistic process and were simultaneously estimated using Bayesian inference algorithms.</p>
<heading id="h-0013" level="2">Camera Setup and Initialization</heading>
<p id="p-0081" num="0080">Various methods may be utilized to record and track video images of animals <b>50</b> (e.g., mice). In some embodiments, the video recorded may be recorded in three dimensions. Various apparatuses are available for this function, for instance the experiments disclosed herein utilized Microsoft's Kinect for Windows. In other embodiments, the following additional apparatuses may be utilized: (1) stereo-vision cameras (which may include groups of two or more two-dimensional cameras calibrated to produce a depth image, (2) time-of-flight depth cameras (e.g. CamCube, PrimeSense, Microsoft Kinect 2, structured illumination depth cameras (e.g. Microsoft Kinect 1), and x-ray video.</p>
<p id="p-0082" num="0081">The video recorder <b>100</b> and tracking system <b>110</b> may project structured infrared light onto the imaging field <b>10</b>, and compute the three-dimensional position of objects in the imaging field <b>10</b> upon parallax. The Microsoft Kinect for Windows has a minimum working distance (in Near Mode) of 0.5 meters; by quantitating the number of missing depth pixels within an imaged field, the optimal sensor positioned may be determined. For example, the inventors have discovered that the optimal sensor position for a Kinect is between 0.6 and 0.75 meters away from the experimental field depending on ambient light conditions and assay material.</p>
<heading id="h-0014" level="2">Data Acquisition</heading>
<p id="p-0083" num="0082">Data output from the video recorder <b>100</b> and tracking system <b>110</b> may be received by and processed by a computing device <b>113</b> that processes the depth frames and saves them in a suitable format (e.g., binary or other format). In some embodiments, the data from the video recorder <b>100</b> and tracking system <b>110</b> may be directly output over a network <b>120</b> to a server <b>130</b>, or may be temporarily buffered and/or sent over a USB or other connection to an associated computing device <b>113</b> that temporarily stores the data before sending over a network <b>120</b> to a centralized server <b>130</b> for further processing. In other embodiments, the data may be processed by an associated computer <b>113</b> without sending over a network <b>120</b>.</p>
<p id="p-0084" num="0083">For instance, in some embodiments, data output from a Kinect may be sent to a computer over a USB port utilizing custom Matlab or other software to interface the Kinect via the official Microsoft .NET API that retrieves depth frames at a rate of 30 frames per second and saves them in raw binary format (16-bit signed integers) to an external hard-drive or other storage device. Because USB3.0 has sufficient bandwidth to allow streaming of the data to an external hard-drive or computing device with storage in real-time. However, in some embodiments, a network may not have sufficient bandwidth to remotely stream the data in real time.</p>
<heading id="h-0015" level="2">Data Pre-Processing</heading>
<p id="p-0085" num="0084">In some embodiments, after the raw images of the video data are saved and/or stored in a database or other memory, various pre-processing may take place to isolate the animal in the video data and orient the images of the animal along a common axis for further processing. In some embodiments, the orientation of the head may be utilized to orient the images in a common direction. In other embodiments, an inferred direction of the spine may be incorporated.</p>
<p id="p-0086" num="0085">For instance, tracking the evolution of an imaged mouse's pose over time requires identifying the mouse within a given video sequence, segmenting the mouse from the background (in this case the apparatus the mouse is exploring), orienting the isolated image of the mouse along the axis of its spine, correcting the image for perspective distortions, and then compressing the image for processing by the model.</p>
<heading id="h-0016" level="2">Isolating Video Data of the Animal</heading>
<p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates a process the system may perform for isolating a region of interest and subtracting background images to isolate the video data of the animal <b>50</b>. First, to isolate the experimental arena in which the mouse is behaving, the system may first identify a region-of-interest (ROI) <b>210</b> for further analysis. In other embodiments, the region-of-interest <b>210</b> may include the entire field of view <b>10</b> of recorded video data. To isolate the region, one may manually trace along the outside edge of any imaged arena; pixels outside the ROI <b>210</b> may be set to zero to prevent spurious object detection. In other embodiments, the system may automatically define a ROI <b>210</b> using various methods. In some embodiments, the system may filter the raw imaging data with an iterative median filter, which is well suited to removing correlated noise from the sensor, for example, in a Kinect.</p>
<p id="p-0088" num="0087">After selecting the region of interest <b>210</b>, the raw images may be cropped to the region of interest <b>215</b>. Then missing pixel values can be input <b>225</b>, after which an X, Y, and Z position can be calculated <b>230</b> for each pixel, and the pixel position can be resampled. Accordingly, the images can be resampled onto real-world coordinates. Then, the system calculates the median real-world coordinate background image <b>240</b>, and those can be subtracted from the real-world coordinate images <b>245</b>.</p>
<p id="p-0089" num="0088">To subtract the background image of the arena from the video data, various techniques may be performed, including for example, subtracting the median value of a portion of the video data for a set time period (e.g. 30 seconds). For instance, in some embodiments, the first 30 seconds of data from any imaging stream may be subtracted from all video frames and any spurious values less than zero may be reset to zero.</p>
<p id="p-0090" num="0089">To further ensure the analysis focuses on the animal, the system may binarize the image (or perform similar processes using thresholds) and eliminate any objects that did not survive a certain number of iterations of morphological opening. Accordingly, once this is finished, the system may perform the additional processing illustrated in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. Accordingly, the background subtracted images (mouse video data) <b>250</b> may be filtered and the artifacts may be removed <b>255</b>. In some embodiments, this may involve iterative median filtering.</p>
<p id="p-0091" num="0090">The animal in the image data may then be identified by defining it as the largest object within the arena that survived the subtraction and masking procedures, or by blob detection <b>260</b>. Then, the image of the mouse may be extracted <b>265</b>.</p>
<heading id="h-0017" level="2">Identifying the Orientation of the Animal</heading>
<p id="p-0092" num="0091">The centroid of the animal (e.g. mouse) may then be identified <b>270</b> as the center-of-mass of the preprocessed image or by other suitable methods; an ellipse may then be fit to its contour <b>285</b> to detect its overall orientation. In order to properly orient the mouse <b>280</b>, various machine learning algorithms may be trained (e.g. a random forest classifier) on a set of manually-oriented extracted mouse images. Given an image, the orientation algorithm then returns an output indicating whether the mouse's head is oriented correctly or not.</p>
<p id="p-0093" num="0092">Once the position is identified, additional information may be extracted <b>275</b> from the video data including the centroid, head and tail positions of the animal, orientation, length, width, height, and each of their first derivatives with respect to time. Characterization of the animal's pose dynamics required correction of perspective distortion in the X and Y axes. This distortion may be corrected by first generating a tuple of (x,y,z) coordinates for each pixel in real-world coordinates, and then resampling those coordinates to fall on an even grid in the (x,y) plane using Delaunay triangulation.</p>
<heading id="h-0018" level="2">Output to a Model Based or Model-Free Algorithm</heading>
<p id="p-0094" num="0093">As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the output of the orientation corrected images or frames in some embodiments will be to a principle component analysis time series <b>310</b> or other statistical methods for reducing data points. In some embodiments, the data will be run through a model fitting algorithm <b>315</b> such as the AR-HMM algorithm disclosed herein, or may be run through a model free algorithm <b>320</b> as disclosed in order to identify behavior modules <b>300</b> contained within the video data. Additionally, in some embodiments, the PCA time series will not be performed.</p>
<p id="p-0095" num="0094">In embodiments with model-free algorithms <b>320</b>, various combinations of algorithms can be utilized with the goal of isolating sub-second modules of behavior that have similar orientation profile and trajectories. Disclosed herein are some examples of these algorithms, however, additional algorithms could be envisioned that segment the data into behavior modules.</p>
<heading id="h-0019" level="2">Reducing Dimensionality of Image</heading>
<p id="p-0096" num="0095">In some embodiments, both that include model-free algorithms <b>320</b> or the model fitting <b>315</b> algorithm, the information captured in each pixel often is either highly correlated (neighboring pixels) or uninformative (pixels on the border of the image that never represent the mouse's body). To both reduce redundant dimensions and make modeling computationally tractable, various techniques may be employed to dimensionally reduce each image. For example, a five-level wavelet decomposition may be performed, thereby transforming the image into a representation in which each dimension captured and pooled information at a single spatial scale; in this transformation, some dimensions may code explicitly for fine edges on the scale of a few millimeters, while others encoded broad changes over spatial scales of centimeters.</p>
<p id="p-0097" num="0096">This wavelet decomposition however will expand the dimensionality of the image. In order to reduce this dimensionality, principal components analysis may then be applied to these vectors, in order to project the wavelet coefficients into ten dimensions, which the inventors have found still captures &#x3e;95% of total variance. For instance, principle components may be built using a canonical dataset of 25 C57 BL/6 mice, aged 6 weeks, recorded for 20 minutes each, and all datasets were projected into this common pose space. Accordingly, the output of the PCA may then be input into the modeling algorithm for module identification.</p>
<p id="p-0098" num="0097">In some embodiments, random projections technique may be utilized to reduce the dimensionality of the data. Random projections is an approach that produces new dimensions derived from an original signal, with dimensionality D_orig, by randomly weighting each original dimension, and then summing each dimension according to that weighting, producing a single number per data point. This procedure can be repeated several times, with new random weightings, to produce a set of &#x201c;randomly projected&#x201d; dimensions. The Johnson-Lindenstrauss lemma shows that distances between points in the original dataset with dimensionality D_orig is preserved in the randomly projected dimensions, D_proj, where D_proj &#x3c;D_orig.</p>
<heading id="h-0020" level="2">Model-Free Algorithms: Identifying Behavior Module Length</heading>
<p id="p-0099" num="0098">In some embodiments that have a model-free algorithm <b>320</b>, in order to evaluate timescale over which an animal's behavior is self-similar&#x2014;which reflects the rate at which an animal transitions from one pattern of motion to another&#x2014;an autocorrelation analysis may be performed. Because some data smoothing is required to remove sensor-specific noise, computing the auto-correlogram as the statistical correlation between time-lagged versions of a signal will result in a declining auto-correlogram, even for an animal (e.g. mouse) that is posed in rigor mortis. Therefore, correlation distance between all 10 dimensions of the mouse's pose data may be utilized as the comparator between time-lagged versions of the time-series signal in question, resulting in a flat autocorrelation function of value &#x2dc;1.0 for a dead animal, and a declining autocorrelation function for a behaving animal (e.g., mouse). The rate at which this auto-correlogram declines in a behavior mouse is a measure of a fundamental timescale of behavior, which may be characterized as a time-constant, tau, of an exponentially-decaying curve. Tau can be fitted using the Levenberg-Marquardt algorithm (non-linear least squares) using the SciPy optimization package.</p>
<p id="p-0100" num="0099">In some embodiments, a power-spectral density (PSD) analysis may be performed on the mouse behavioral data to further analyze its time domain structure. For instance, a Wiener filter may be utilized to identify the time frequencies that must be boosted in the signal derived from a dead mouse in order to best match a behaving mouse. This can be implemented simply by taking the ratio of the PSD of a behaving mouse over the PSD of a dead mouse. In some embodiments, the PSD may be computed using the Welch periodogram method, which takes the average PSD over a sliding window across the entire signal.</p>
<heading id="h-0021" level="2">Model-Free Algorithms: Locating Change Points for Transition Periods</heading>
<p id="p-0101" num="0100">In some embodiments where a model is not used to identify modules <b>320</b>, various methods may be utilized to identify the changepoints for the transition periods. Plotting the random projections of the mouse depth image over time yields obvious striations, each a potential changepoint over time. To automate the identification of these changepoints, which represent potential boundaries between the block structure apparent in the random projections data, a simple changepoint identification technique called the filtered derivative algorithm may be utilized. For example, an algorithm can be employed that calculates the derivative of the per-frame unit-normalized random projections with a lag of k=4 frames. For each time point, for each dimension, an algorithm may determine whether the signal has crossed some threshold h=0.15 mm. Then, the binary changepoint indicator signal may be summed across each of D=300 random projection dimensions, and then the resulting 1D signal may be smoothed with a Gaussian filter with a kernel standard deviation of sigma=0.43 frames. Change points may then be identified as the local maxima of this smoothed 1D time-series. This procedure depends in part upon the specific values of the parameters k, h and sigma; for example, those values that maximize the number of changepoints in the behaving mouse while yielding no change points in a dead mouse may be utilized.</p>
<heading id="h-0022" level="2">Model-Free Algorithms: Identifying Similar or Repeating Modules</heading>
<p id="p-0102" num="0101">In some embodiments, where data is being analyzed without a model <b>320</b>, certain algorithms may be utilized to identify similar or repeated modules. Accordingly, a set of repeating modules may be identified as the vocabulary or syllables of the animal behavior. Therefore, to determine whether any reasonably long snippet of behavior (greater than just a few frames) was ever &#x201c;repeated&#x201d; (without reliance on a underlying model for behavior), the system may utilize a template matching procedure to identify similar trajectories through PCA space. To identify similar trajectories, for example, the systems and methods may calculate the Euclidean distance between some target snippet, the &#x201c;template&#x201d;, and every possible snippet of equal length (often defined by the approximate block boundaries identified by changepoints analysis). Other similar methods could be employed for identifying modules, including other statistical based methods.</p>
<p id="p-0103" num="0102">In some embodiments, the collection of modules that are similar would be selected as the most similar snippets, ignoring snippets discovered that were shifted less than 1 second from each other (to ensure we select behavioral snippets that occur distanced in time from each other, and also in separate mice).</p>
<heading id="h-0023" level="2">Data Modeling</heading>
<p id="p-0104" num="0103">In other embodiments, systems and methods may be employed that identify behavior modules in video data utilizing data models <b>315</b>. For instance, a data model may implement the well-established paradigm of generative probabilistic modeling, which is often used to model complex dynamical processes. This class of models is generative in the sense that it describes a process by which observed data can be synthetically generated by the model itself, and they are probabilistic because that process is defined mathematically in terms of sampling from probability distributions. In addition, by fitting an interpretable model to data, the data were &#x2018;parsed&#x2019; in a manner that revealed the latent variable structure that the model posits gave rise to the data (including parameters describing the number and identities of the states as well as parameters describing the transitions between the states).</p>
<p id="p-0105" num="0104">In some embodiments, the model <b>315</b> may be expressed utilizing a Bayesian framework. The Bayesian framework provides a natural way to express hierarchical models for the organization of behavior, priors or regularizers that reflect known or observed constraints on the patterns of motion within the 3D dataset, and a coherent representation of uncertainty. This framework also provides significant and well-established computational machinery for inferring key parameters of any model. Within the Bayesian framework, for a particular model structure (e.g. the spatiotemporal nature of the states and their possible transitions) and prior distributions on the latent variables, the data fixes a posterior distribution over the latent variables.</p>
<p id="p-0106" num="0105">Below, the model-based methods used to characterize behavior are defined in two steps: first, a mathematical definition of the generative model and priors used, and second, a description of the inference algorithms.</p>
<heading id="h-0024" level="2">Example Model for Identifying Behavior Modules&#x2014;AR-HMM</heading>
<p id="p-0107" num="0106">In some embodiments, systems may utilize a discrete-time hidden Markov model <b>315</b> (HMM) to identify behavior modules. HMMs encompass a range of stochastic processes for modeling sequential and time series data. The HMM model posits that at each point in time (e.g. for every frame of imaging data), the mouse is within a discrete state (Markov state) that can be given a label. Each Markov state represents a brief three-dimensional motif of motion the animal undertakes while within that state. Because observed three-dimensional behavior of mice appears to depend upon the specific pattern of motion the animal expressed in the immediate past, ideally each Markov state would predict the mouse's future behavior based upon its immediate past pose dynamics. Each Markov state is therefore composed of both a latent discrete component, which identifies the behavioral mode of the animal, and a number of lags of the observation sequence, which are used to predict the short-timescale behavior of the animal based on the behavioral mode. This model structure is often called a switching vector-autoregressive (SVAR) model or autoregressive HMM (AR-HMM).</p>
<p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides an example of how an AR-HMM algorithm can convert input data (spine aligned depth imaging data <b>305</b> that has been dimensionally reduced <b>405</b> using PCA <b>310</b>) into a fit model that describes the number of behavioral modules and the trajectories they encode through PCA space, the module-specific duration distributions that govern how long any trajectory within a given module lasts, and the transition matrix that describes how these individual modules interconnect over time.</p>
<p id="p-0109" num="0108">In addition, the AR-HMM can be configured to assign a label to every frame of the training data associating it with a given behavioral module. After pre-processing and dimensional reduction <b>405</b>, imaging data is broken into training <b>415</b> and test sets <b>410</b>. The training set <b>415</b> is then submitted to the AR-HMM <b>315</b>. After randomly initializing the parameters of the model <b>315</b> (which here refers to the autoregressive parameters that describe each module's trajectory through PCA space, the transition matrix describing the probabilities that governs temporal interconnections between modules, the duration distribution parameters that describe how long any instance of a given module is likely to last, and the labels assigned to each frame of imaging data associating that frame with a particular module) the AR-HMM attempts to fit the model <b>315</b> by varying one parameter while holding the others constant. The AR-HMM alternates between two main updates: the algorithm <b>315</b> first attempts to segment the imaging data into modules given a fixed set of transition statistics and a fixed description of the AR parameters that describe any given module, and then the algorithm switches to fixing the segmentation and updating the transition matrix and the AR parameters <b>455</b>. The AR-HMM <b>315</b> uses a similar approach to assigning any given frame of imaging data to a given module. It first computes the probability of that a given module is the &#x201c;correct&#x201d; module, which is proportional to a measure of how well the state's corresponding autoregressive parameters <b>455</b> describe the data at that time index and how well the resulting state transitions agree with the transition matrix <b>450</b>.</p>
<p id="p-0110" num="0109">In the second step, the AR-HMM <b>315</b> varies the autoregressive parameters <b>455</b> and transition parameters <b>450</b> to better fit the assigned data, thus updating the each of the behavioral modules and the model of the transitions among modes. The product of this process are the parameters described <b>455</b>, the quality of these parameters in terms of describing behavior are then evaluated using likelihood measurements of the data that was held-out from training <b>475</b>.</p>
<p id="p-0111" num="0110">By identifying the discrete latent states <b>445</b> associated with 3D pose sequence data, an HMM model <b>315</b> can identify segments of data that exhibit similar short-timescale motion dynamics and explain such segments in terms of reused autoregressive parameters. For each observation sequence there is an unobserved state sequence: if the discrete state at time index t is x_t=i, then the probability that the discrete state x_(t+1) takes on value j is a deterministic function of i and j and is independent of all previous states. Symbolically,</p>
<p id="p-0112" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>x</i><sub>t+1</sub><i>|x</i><sub>t</sub><i>,x</i><sub>t&#x2212;1</sub><i>,x</i><sub>t&#x2212;2</sub><i>, . . . ,x</i><sub>1</sub>)=<i>p</i>(<i>x</i><sub>t+1</sub><i>|x</i><sub>t</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0113" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>x</i><sub>t+1</sub><i>=j|x</i><sub>t</sub><i>=i</i>)=&#x3c0;<sub>ij </sub><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0114" num="0111">where &#x3c0; is a transition matrix <b>450</b> in which the (i,j) element is the probability of transitioning from state i to state j. In some embodiments, the discrete state's dynamics may be fully parameterized by the transition matrix, which is considered here not to change with time. One of the tasks of the inference algorithm (described below) was to infer probable values for the discrete state sequences and the transition matrix governing their deployment, thus inferring a sequence of reused behavioral modules and transition patterns that govern how these modules are connected over time.</p>
<p id="p-0115" num="0112">Given a discrete state sequence, a corresponding 3D pose data sequence can be modeled as a conditionally vector autoregressive (VAR) process. Each state-specific vector autoregression can capture short-timescale motion dynamics particular to the corresponding discrete state; in other words, each behavioral module can be modeled as its own autoregressive process. More precisely, given the discrete state x_t of the system at any time index t, the value of the observed data vector at that time index y_t is distributed according to a state-specific noisy regression on K previous values of the observation sequence, y_(t&#x2212;1), . . . ,y_(t&#x2212;K). The inference algorithm may also be tasked with inferring the most probable values for each state's autoregressive dynamical parameters as well as the number of lags used in the dynamics.</p>
<p id="p-0116" num="0113">In some embodiments, these switching autoregressive dynamics defined the core of the AR-HMM. However, because different animal populations or experimental conditions are expected to give rise to differences in behavior, when considering two or more such experimental conditions models may be built hierarchically: different experimental conditions may be allowed to share the same library of state-specific VAR dynamics but learned their own transition patterns as well as any unique VAR dynamical modes. This simple extension allows a model to reveal changes in the parameters due to changes in the experiment. Furthermore, the compositional Bayesian inference algorithms employed immediately extends such hierarchical models.</p>
<p id="p-0117" num="0114">To employ Bayesian inference methods, unknown quantities, including the transition matrix <b>450</b> and the autoregressive parameters <b>455</b> that describe each state <b>445</b>, can be treated with a uniform representation as latent random variables. In particular, weak prior distributions <b>465</b> can be placed on these quantities and their posterior distributions <b>465</b> after conditioning on observed 3D imaging data were investigated. For the autoregressive parameters, a prior that included a Lasso-like penalty can be used to encourage uninformative lag indices to have their corresponding regression matrix coefficients tend to zero.</p>
<p id="p-0118" num="0115">For the transition matrix <b>450</b>, a hierarchical Dirichlet process <b>435</b> prior can used, to regularize the number of discrete latent states <b>445</b>. In addition, the transition matrix <b>450</b> prior also included a sticky bias, which is a single nonnegative number that controlled the tendency of the discrete states to self-transition. Because this parameter controls the timescale of the inferred switching dynamics, this parameter can be set such that the output of the model inference algorithms matches (as closely as possible) the model-free duration distribution determined by a changepoint analysis as disclosed herein (or other method of identifying the module length) and the autocorrelogram generated from the preprocessed and unmodeled 3D pose data. In some embodiments, this parameter can be tuned&#x2014;for example to define the prior over the timescale of behavior.</p>
<p id="p-0119" num="0116">In some embodiments, simpler models can be used than the AR-HMM model by removing certain portions of the model structure. For instance, removing the discrete switching dynamics captured in the transition matrix and replacing them with a mixture model may generate an alternative model in which the distribution over each discrete state does not depend on its previous state. This would be the case if animals had a set of behavioral modules from which to choose, and the likelihoods of expressing any given one of them did not depend on the order in which they appear. This simplification resulted in the autoregressive mixture model (AR-MM).</p>
<p id="p-0120" num="0117">Alternatively, replacing the conditionally autoregressive dynamics with simple state-specific Gaussian emissions results in a Gaussian-emission HMM (G-HMM); this model explores the hypothesis that each behavioral module is best described by a simple pose, rather than being a dynamical trajectory. Applying both simplifications yields a Gaussian mixture model (G-MM), in which behavior is simply a sequence of poses over time in which the probability of expressing any given pose does not depend on the prior pose. Removing the switching dynamics yields a pure autoregressive (AR) or linear dynamical system (LDS) model, in which behavior is described as a trajectory through pose space without any reused discrete behavioral modules at all.</p>
<heading id="h-0025" level="2">Analysis of Behavior Modules</heading>
<p id="p-0121" num="0118">In some embodiments, systems may provide the ability to provide an indication of the relationship between behavior modules, describe the most frequently used behavior modules, or perform other useful analysis of behavior modules.</p>
<p id="p-0122" num="0119">For example, in order to represent the grammatical relationship between behavioral syllables, the probability (e.g. bigram) that two syllables were found occurring one after the other (a &#x201c;bigram&#x201d; of modules) can be calculated as a fraction of all observed bigrams. In some embodiments, to calculate this value for each pair (i,j) of modules, for example, a square n&#xd7;n matrix, A, may be utilized where n is the number of total modules in the label sequence. Then, the systems and methods may scan through the label sequences that were saved at the last iteration of Gibbs sampling, incrementing the entry A[i,j] for every time the system identifies a syllable i directly preceding a syllable j. At the end of the label sequence, the system may divide by the number of total bigrams observed.</p>
<p id="p-0123" num="0120">In order to visually organize those modules that were specifically up-regulated or selectively expressed as a result of a manipulation, the system may assign a selectivity index to each module. For example, where p(condition) indicates the percent usage of a module in a condition, the system may sort modules in the circular open field versus square box comparison by (p(circle)-p(square)/(p(circle)+p(square)). In the comparison between blank odor and fox odor (TMT), the system may sort modules by (p(tmt)&#x2212;p(blank))/(p(tmt)+p(blank)).</p>
<heading id="h-0026" level="2">Statemap Visualizations</heading>
<p id="p-0124" num="0121">The system may also output the syllable bigram probabilities and syllable usages on n syllables on a graph G=(V, E) in which each node i&#x2208;V={1,2, . . . , n} corresponds to syllable i and each directed edge (i, j) &#x2208;E={1,2, . . . , n}<sup>2</sup>\ {(i, i): i&#x2208;V} corresponds to a bigram. The graph may be output as a set of circular nodes and directed arcs so that the size of each node is proportional to the corresponding syllable's usage and the width and opacity of each arc is proportional to the corresponding bigram's probability within a minimum and maximum range depicted in the figure legends. To lay out each graph in a reproducible non-(pseudo-)random way (up to global rotation of the figure), the system may initialize the position of the nodes using the spectral layout algorithm and fine-tuned node positions using the Fructherman-Reingold iterative force-directed layout algorithm; we used both algorithms can be used as implemented in the NetworkX software package.</p>
<heading id="h-0027" level="2">Overview of Main Inference Algorithms</heading>
<p id="p-0125" num="0122">In some embodiments, inference algorithms may be applied to the models <b>315</b> to estimate the parameters. For example, an approximate Bayesian inference can be performed using Gibbs sampling, a Markov Chain Monte Carlo (MCMC) inference algorithm. In the MCMC paradigm, the inference algorithm constructs approximate samples from the posterior distribution of interest, and these samples are used to compute averages or as a proxy for posterior modes. The sequence of samples produced by the algorithm dwells in regions of high posterior probability while escaping regions of low posterior probability or bad local optima. In the main AR-HMM model, the latent variables of interest include the vector autoregressive parameters, the hidden discrete state sequence, and the transition matrix (e.g. the autoregressive parameters that define the pose dynamics within any given behavioral module, the sequence of the modules, and the transition probabilities between any given module and any other module). Applying the MCMC inference algorithm to 3D imaging data generate a set of samples of these latent variables for the AR-HMM.</p>
<p id="p-0126" num="0123">The Gibbs sampling algorithm has a natural alternating structure, directly analogous to the alternating structure of expectation-maximization (EM) and variational mean field algorithms. Applied to the AR-HMM, after initialization to a random sample from the prior, the algorithm can be alternated between two main updates: first, the algorithm can resample the hidden discrete state sequences given the transition matrix and autoregressive parameters, and second, the algorithm can resample the parameters given the hidden states.</p>
<p id="p-0127" num="0124">In other words, the algorithm <b>315</b> first attempts to segment the imaging data into modules <b>300</b> given a fixed set of transition statistics and a fixed description of the AR parameters that describe any given module, and then the algorithm switches to fixing the segmentation and updating the transition matrix <b>450</b> and the AR parameters <b>455</b>. To assign each of the 3D pose video frames to one of the behavioral modes <b>300</b> in the first step of this process, the state label <b>445</b> for a particular time index can be sampled randomly from the set of possible discrete states, where the probability of sampling a given state can be proportional to a measure of how well the state's corresponding autoregressive parameters described the data at that time index and how well the resulting state transitions agree with the transition matrix <b>450</b>. In the second step, given the assignment of data subsequences to states, the autoregressive parameters and transition parameters can be resampled to fit the assigned data, thus updating the dynamical model of each of the behavioral modes and the model of the transitions among modes. The procedure implemented by the Gibbs sampling algorithm can be noisy, enabling the algorithm to escape local maxima that may prevent the algorithm from effectively exploring parameter space.</p>
<heading id="h-0028" level="1">EXAMPLES</heading>
<p id="p-0128" num="0125">Below are disclosed examples of the specific implementations of the models described herein for performing the disclosed examples. Variations of these models may be implemented to identify behavior models.</p>
<heading id="h-0029" level="2">Prior on the Transition Matrix</heading>
<p id="p-0129" num="0126">A sticky HDP prior was placed on the transition matrix &#x3c0; with concentration parameters &#x3b1;,&#x3b3;&#x3e;0 and sticky parameter &#x3ba;&#x3e;0</p>
<p id="p-0130" num="0000">
<maths id="MATH-US-00001" num="00001">
<math overflow="scroll">
 <mrow>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>&#x3c1;</mi>
      <mi>j</mi>
     </msub>
     <mover>
      <mo>&#x223c;</mo>
      <mi>iid</mi>
     </mover>
     <mrow>
      <mi>Beta</mi>
      <mo>&#x2062;</mo>
      <mrow>
       <mo>(</mo>
       <mrow>
        <mn>1</mn>
        <mo>,</mo>
        <mi>&#x3b3;</mi>
       </mrow>
       <mo>)</mo>
      </mrow>
      <mo>&#x2062;</mo>
      <msub>
       <mi>&#x3b2;</mi>
       <mi>i</mi>
      </msub>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mn>1</mn>
       <mo>-</mo>
       <msub>
        <mi>&#x3c1;</mi>
        <mi>i</mi>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mo>&#x2062;</mo>
     <mrow>
      <munder>
       <mo>&#x220f;</mo>
       <mrow>
        <mi>j</mi>
        <mo>&#x3c;</mo>
        <mi>i</mi>
       </mrow>
      </munder>
      <msub>
       <mi>&#x3c1;</mi>
       <mi>j</mi>
      </msub>
     </mrow>
    </mrow>
   </mrow>
   <mtext>   </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mrow>
    <mrow>
     <msub>
      <mi>&#x3c0;</mi>
      <mi>i</mi>
     </msub>
     <mover>
      <mo>&#x223c;</mo>
      <mi>iid</mi>
     </mover>
     <mrow>
      <mrow>
       <mi>DP</mi>
       <mo>&#x2061;</mo>
       <mo>(</mo>
       <mrow>
        <mrow>
         <mi>&#x3b1;</mi>
         <mo>&#x2062;</mo>
         <mi>&#x3b2;</mi>
        </mrow>
        <mo>+</mo>
        <mrow>
         <mi>&#x3ba;</mi>
         <mo>&#x2062;</mo>
         <msub>
          <mi>&#x3b4;</mi>
          <mi>i</mi>
         </msub>
        </mrow>
       </mrow>
       <mo>)</mo>
      </mrow>
      <mo>&#x2062;</mo>
      <mi>i</mi>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mn>1</mn>
   </mrow>
   <mo>,</mo>
   <mn>2</mn>
   <mo>,</mo>
   <mo>&#x2026;</mo>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<p id="p-0131" num="0127">where &#x3b4;<sub>ij </sub>is 1 when i=j and is 0 otherwise and &#x3c0;<sub>i </sub>denotes the ith row of &#x3c0;. Gamma priors are placed on &#x3b1; and &#x3b3;, setting &#x3b1;&#x2dc;Gamma(1,1/100) and &#x3b3;&#x2dc;Gamma(1,1/100).</p>
<heading id="h-0030" level="2">Generation of the Discrete State Sequence</heading>
<p id="p-0132" num="0128">Given the transition matrix, the prior on a discrete state sequence x was</p>
<p id="p-0133" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i><sub>t</sub>&#x2dc;&#x3c0;<sub>x</sub><sub><sub2>t&#x2212;1</sub2></sub><i>t=</i>2,3, . . . ,<i>T </i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0134" num="0129">where x<sub>1 </sub>is generated by the stationary distribution under</p>
<heading id="h-0031" level="2">Prior on the Autoregressive Parameters</heading>
<p id="p-0135" num="0130">The autoregressive parameters &#x3b8;={&#x3b8;<sup>(i)</sup>}<sub>i=1</sub><sup>&#x221e;</sup>={A<sup>(i)</sup>, b<sup>(i)</sup>,&#x3a3; (i)} for each state i=1,2, . . . were sampled from a Matrix Normal Inverse-Wishart prior:</p>
<p id="p-0136" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>(<i>A,b</i>),&#x3a3;&#x2dc;MNIW(<i>v</i><sub>0</sub><i>,S</i><sub>0</sub><i>,M</i><sub>0</sub><i>,K</i><sub>0</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0137" num="0131">or equivalently</p>
<p id="p-0138" num="0000">
<maths id="MATH-US-00002" num="00002">
<math overflow="scroll">
 <mrow>
  <mrow>
   <mrow>
    <mo>&#x2211;</mo>
    <mrow>
     <mo>&#x223c;</mo>
     <mrow>
      <mi>InvWishart</mi>
      <mo>&#x2061;</mo>
      <mo>(</mo>
      <mrow>
       <msub>
        <mi>v</mi>
        <mn>0</mn>
       </msub>
       <mo>,</mo>
       <msub>
        <mi>S</mi>
        <mn>0</mn>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mtext>  </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mrow>
    <mi>vec</mi>
    <mo>&#x2061;</mo>
    <mo>(</mo>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mi>A</mi>
      <mo>,</mo>
      <mi>b</mi>
     </mrow>
     <mo>)</mo>
    </mrow>
    <mo>)</mo>
   </mrow>
   <mo>&#x223c;</mo>
   <mrow>
    <mi>Normal</mi>
    <mo>&#x2062;</mo>
    <mtext>  </mtext>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mrow>
       <mi>vec</mi>
       <mo>&#x2061;</mo>
       <mo>(</mo>
       <msub>
        <mi>M</mi>
        <mn>0</mn>
       </msub>
       <mo>)</mo>
      </mrow>
      <mo>,</mo>
      <mtext>&#x205f;</mtext>
      <mrow>
       <mo>&#x2211;</mo>
       <mrow>
        <mo>&#x2297;</mo>
        <msub>
         <mi>K</mi>
         <mn>0</mn>
        </msub>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<p id="p-0139" num="0132">where &#x2297; denotes a Kronecker product and (A, b) denotes the matrix formed by appending b to A as a column. In addition, a block ARD prior on K<sub>0 </sub>is used to encourage uninformative lags to be shrunk to zero:</p>
<p id="p-0140" num="0000">
<maths id="MATH-US-00003" num="00003">
<math overflow="scroll">
 <mrow>
  <msub>
   <mi>K</mi>
   <mn>0</mn>
  </msub>
  <mo>=</mo>
  <mrow>
   <mrow>
    <mrow>
     <mi>diag</mi>
     <mo>&#x2061;</mo>
     <mo>(</mo>
     <mrow>
      <msub>
       <mi>k</mi>
       <mn>1</mn>
      </msub>
      <mo>,</mo>
      <mo>&#x2026;</mo>
      <mtext>   </mtext>
      <mo>,</mo>
      <msub>
       <mi>k</mi>
       <mrow>
        <mi>K</mi>
        <mo>&#x2062;</mo>
        <mi>D</mi>
       </mrow>
      </msub>
     </mrow>
     <mo>)</mo>
    </mrow>
    <mo>&#x2062;</mo>
    <mtext>  </mtext>
    <msub>
     <mi>k</mi>
     <mi>i</mi>
    </msub>
   </mrow>
   <mover>
    <mo>&#x223c;</mo>
    <mi>iid</mi>
   </mover>
   <mrow>
    <mrow>
     <mi>lnvGamma</mi>
     <mo>&#x2061;</mo>
     <mo>(</mo>
     <mrow>
      <mrow>
       <mrow>
        <mn>1</mn>
        <mo>/</mo>
        <mn>2</mn>
       </mrow>
       <mo>&#x2062;</mo>
       <mn>5</mn>
      </mrow>
      <mo>,</mo>
      <mrow>
       <mrow>
        <mn>1</mn>
        <mo>/</mo>
        <mn>2</mn>
       </mrow>
       <mo>&#x2062;</mo>
       <mn>5</mn>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
    <mo>.</mo>
   </mrow>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<heading id="h-0032" level="2">Generation of the 3D Pose Sequence Principle Components</heading>
<p id="p-0141" num="0133">Given the autoregressive parameters and discrete state sequence, the data sequence y was generated according to an affine autoregressiion:</p>
<p id="p-0142" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i><sub>t</sub>&#x2dc;Normal(<i>A</i><sup>(x</sup><sup><sub2>t</sub2></sup><sup>)</sup><i>{tilde over (y)}</i><sub>t&#x2212;1</sub><i>+b</i><sup>(x</sup><sup><sub2>t</sub2></sup><sup>)</sup>,&#x3a3;<sup>(x</sup><sup><sub2>t</sub2></sup><sup>)</sup>)<i>t=K+</i>1,<i>K+</i>2, . . . ,<i>T </i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0143" num="0134">where {tilde over (y)} denotes a vector of K lags:</p>
<p id="p-0144" num="0000">
<maths id="MATH-US-00004" num="00004">
<math overflow="scroll">
 <mrow>
  <msub>
   <mover accent="true">
    <mi>y</mi>
    <mi>&#x2dc;</mi>
   </mover>
   <mi>t</mi>
  </msub>
  <mover>
   <mo>=</mo>
   <mi>def</mi>
  </mover>
  <msup>
   <mrow>
    <mo>[</mo>
    <mrow>
     <msubsup>
      <mi>y</mi>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mi>K</mi>
      </mrow>
      <mi>T</mi>
     </msubsup>
     <mo>&#x2062;</mo>
     <mtext>&#x205f;</mtext>
     <msubsup>
      <mi>y</mi>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mi>K</mi>
       <mo>+</mo>
       <mn>1</mn>
      </mrow>
      <mi>T</mi>
     </msubsup>
     <mo>&#x2062;</mo>
     <mtext>&#x205f;</mtext>
     <mo>&#x2026;</mo>
     <mo>&#x2062;</mo>
     <mtext>   </mtext>
     <msubsup>
      <mi>y</mi>
      <mrow>
       <mi>t</mi>
       <mo>-</mo>
       <mn>1</mn>
      </mrow>
      <mi>T</mi>
     </msubsup>
    </mrow>
    <mo>]</mo>
   </mrow>
   <mi>T</mi>
  </msup>
 </mrow>
</math>
</maths>
</p>
<p id="p-0145" num="0135">The alternative models are special cases of the AR-HMM and were constructed by adding constraints. In particular, the Gaussian-emission HMM (G-HMM) corresponds to constraining A<sup>(i)</sup>=0 for each state index i. Similarly, the autoregressive mixture (AR-MM) and Gaussian mixture (GMM) correspond to constraining the transition matrix to be constant across rows, &#x3c0;<sub>i,j</sub>=&#x3c0;<sub>i&#x2032;,j</sub>=&#x3c0;<sub>j </sub>for each i and i&#x2032;, in the AR-HMM and G-HMM, respectively.</p>
<heading id="h-0033" level="2">Specific Implementation of Inference Algorithms to Examples</heading>
<p id="p-0146" num="0136">As discussed above, the Gibbs sampling inference algorithm alternated between two principal stages: updating the segmentation of the data into modules given a fixed transition matrix and autoregressive parameters, and updating the transition matrix and autoregressive parameters given a fixed segmentation. Mathematically, updating the segmentation sampled the label sequence x conditioned on the values of the data y, the autoregressive parameters &#x3b8;, and the transition matrix &#x3c0;; that is, sampling the conditional random variable x|&#x3b8;,&#x3c0;,y. Similarly, updating the transition matrix and autoregressive parameters given the segmentation sampled &#x3c0;|x and &#x3b8;|x,y, respectively.</p>
<p id="p-0147" num="0137">For inference in the AR-HMM the weak limit approximation to the Dirichlet process was used, in which the infinite model was approximated by a finite one. That is, choosing some finite approximation parameter L, &#x3b2; and &#x3c0; were modeled using finite Dirichlet distributions of size L</p>
<p id="p-0148" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;&#x2dc;<i>Dir</i>(&#x3b3;/<i>L, . . . ,&#x3b3;/L</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0149" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3c0;<sub>k</sub><i>&#x2dc;Dir</i>(&#x3b1;&#x3b2;<sub>1</sub>, . . . ,&#x3b1;&#x3b2;<sub>j</sub>+&#x3ba;&#x3b4;<sub>kj</sub>, . . . &#x3b1;&#x3b2;<sub>L</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0150" num="0138">where &#x3c0;<sub>k </sub>denotes the ith row of the transition matrix. This finite representation of the transition matrix allowed the state sequence x to be resampled as a block and for large L provides an arbitrarily good approximation to the infinite Dirichlet process.</p>
<p id="p-0151" num="0139">Using a weak limit approximation, the Gibbs sampler for the AR-HMM iterated resampling the conditional random variables</p>
<p id="p-0152" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>x|&#x3c0;,&#x3b8;,y&#x3b8;|x,y </i>and &#x3b2;,&#x3c0;|<i>x </i><?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0153" num="0140">For simplicity, throughout this section notation for conditioning on hyperparameters and the superscript notation for multiple observation sequences is suppressed.</p>
<p id="p-0154" num="0000">Sampling x|&#x3c0;, &#x3b8;, y</p>
<p id="p-0155" num="0141">Sampling the state labels x given the dynamical parameters, &#x3c0; and &#x3b8;, and the data y corresponds to segmenting the 3D video sequence and assigning each segment to a behavioral mode that describes its statistics.</p>
<p id="p-0156" num="0142">Given the observation parameters &#x3b8; and the transition parameters &#x3c0;, the hidden state sequence x is Markov with respect to a chain graph. The standard HMM backward message passing recursions are</p>
<p id="p-0157" num="0000">
<maths id="MATH-US-00005" num="00005">
<math overflow="scroll">
 <mtable>
  <mtr>
   <mtd>
    <mrow>
     <mrow>
      <msub>
       <mi>B</mi>
       <mi>t</mi>
      </msub>
      <mo>(</mo>
      <mi>k</mi>
      <mo>)</mo>
     </mrow>
     <mo>=</mo>
     <malignmark/>
     <mrow>
      <mi>p</mi>
      <mo>&#x2061;</mo>
      <mo>(</mo>
      <mrow>
       <mrow>
        <msub>
         <mi>y</mi>
         <mrow>
          <mrow>
           <mi>t</mi>
           <mo>+</mo>
           <mn>1</mn>
          </mrow>
          <mo>:</mo>
          <mi>T</mi>
         </mrow>
        </msub>
        <mo>|</mo>
        <mi>&#x3b8;</mi>
       </mrow>
       <mo>,</mo>
       <mi>&#x3c0;</mi>
       <mo>,</mo>
       <mrow>
        <msub>
         <mi>x</mi>
         <mi>t</mi>
        </msub>
        <mo>=</mo>
        <mi>k</mi>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mtd>
  </mtr>
  <mtr>
   <mtd>
    <mrow>
     <mo>=</mo>
     <malignmark/>
     <mrow>
      <munderover>
       <mo>&#x2211;</mo>
       <mrow>
        <mi>j</mi>
        <mo>=</mo>
        <mn>1</mn>
       </mrow>
       <mi>K</mi>
      </munderover>
      <mrow>
       <mrow>
        <mi>p</mi>
        <mo>&#x2061;</mo>
        <mo>(</mo>
        <mrow>
         <mrow>
          <msub>
           <mi>x</mi>
           <mrow>
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
           </mrow>
          </msub>
          <mo>=</mo>
          <mrow>
           <mrow>
            <mi>j</mi>
            <mo>|</mo>
            <msub>
             <mi>x</mi>
             <mi>t</mi>
            </msub>
           </mrow>
           <mo>=</mo>
           <mi>k</mi>
          </mrow>
         </mrow>
         <mo>,</mo>
         <mtext>&#x205f;</mtext>
         <mi>&#x3c0;</mi>
        </mrow>
        <mo>)</mo>
       </mrow>
       <mo>&#x2062;</mo>
       <mrow>
        <mi>p</mi>
        <mo>&#x2061;</mo>
        <mo>(</mo>
        <mrow>
         <mrow>
          <mrow>
           <msub>
            <mi>y</mi>
            <mrow>
             <mi>t</mi>
             <mo>+</mo>
             <mn>1</mn>
            </mrow>
           </msub>
           <mo>|</mo>
           <msub>
            <mi>x</mi>
            <mrow>
             <mi>t</mi>
             <mo>+</mo>
             <mn>1</mn>
            </mrow>
           </msub>
          </mrow>
          <mo>=</mo>
          <mi>j</mi>
         </mrow>
         <mo>,</mo>
         <mtext>&#x205f;</mtext>
         <mi>&#x3b8;</mi>
        </mrow>
        <mo>)</mo>
       </mrow>
       <mo>&#x2062;</mo>
       <mrow>
        <msub>
         <mi>B</mi>
         <mrow>
          <mi>t</mi>
          <mo>+</mo>
          <mn>1</mn>
         </mrow>
        </msub>
        <mo>(</mo>
        <mi>j</mi>
        <mo>)</mo>
       </mrow>
      </mrow>
     </mrow>
    </mrow>
   </mtd>
  </mtr>
 </mtable>
</math>
</maths>
</p>
<p id="p-0158" num="0143">for t=1,2, . . . , T&#x2212;1 and k=1,2, . . . , K, where B<sub>T </sub>(k)=1 and where y<sub>t&#x2212;1:T</sub>=(y<sub>t+1</sub>, y<sub>t+2</sub>, . . . , t<sub>T</sub>) Using these messages, the conditional distribution of the first state x<sub>1</sub>, marginalizing over all the future states x<sub>2:T </sub>is</p>
<p id="p-0159" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>x</i><sub>1</sub><i>=k|&#x3c0;,&#x3b8;,y</i>)&#x221d;<i>p</i>(<i>x</i><sub>1</sub><i>=k</i>|&#x3c0;)<i>p</i>(<i>y</i><sub>1</sub><i>|x</i><sub>1</sub><i>=k</i>,&#x3b8;)<i>B</i><sub>1</sub>(<i>k</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0160" num="0144">which can be sampled efficiently. Given a sampled value <o ostyle="single">z</o><sub>1</sub>, the conditional distribution of the second state x<sub>2 </sub>is</p>
<p id="p-0161" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>x</i><sub>2</sub><i>=k|&#x3c0;,&#x3b8;,y,x</i><sub>1</sub><i>=<o ostyle="single">z</o></i>)&#x221d;<i>p</i>(<i>x</i><sub>2</sub><i>=k|x</i><sub>1</sub><i>=<o ostyle="single">z</o></i><sub>1</sub>,&#x3c0;)<i>p</i>(<i>y</i><sub>2</sub><i>|x</i><sub>2</sub><i>=k</i>,&#x3b8;)<i>B</i><sub>2</sub>(<i>k</i>).<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0162" num="0145">Therefore after passing HMM messages backward the state sequence can be recursively sampled forwards.</p>
<heading id="h-0034" level="2">Sampling &#x3b8;|x, y</heading>
<p id="p-0163" num="0146">Sampling the autoregressive parameters &#x3b8; given the state sequence x and the data sequence y corresponds to updating each mode's dynamical parameters to describe the 3D video data segments assigned to it.</p>
<p id="p-0164" num="0147">To resample the observation parameters &#x3b8; conditioned on a fixed sample of the state sequence x and the observations y one can exploit conjugacy between the autoregressive likelihood and the MNIW prior. That is, the conditional also follows the MNIW distribution:</p>
<p id="p-0165" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>p</i>(<i>A</i><sup>(k)</sup>,&#x3a3;<sup>(k)</sup><i>|x,y,S</i><sub>0</sub><i>,v</i><sub>0</sub><i>,M</i><sub>0</sub><i>,K</i><sub>0</sub>)=<i>p</i>(<i>A</i><sup>(k)</sup>),&#x3a3;<sup>(k)</sup><i>|S</i><sub>n</sub><i>,v</i><sub>n</sub><i>,M</i><sub>n</sub><i>,K</i>_<i>n</i>)<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0166" num="0148">where (S<sub>n</sub>, v<sub>n</sub>, M<sub>n</sub>, K<sub>n</sub>) are posterior hyperparameters that are functions of the elements of y assigned to state k as well as the preceding lagged observations:</p>
<p id="p-0167" num="0000">
<maths id="MATH-US-00006" num="00006">
<math overflow="scroll">
 <mrow>
  <mrow>
   <mrow>
    <msub>
     <mi>S</mi>
     <mi>n</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <msub>
      <mi>S</mi>
      <mn>0</mn>
     </msub>
     <mo>+</mo>
     <msub>
      <mi>S</mi>
      <mrow>
       <mi>y</mi>
       <mo>&#x2062;</mo>
       <msup>
        <mi>y</mi>
        <mi>T</mi>
       </msup>
      </mrow>
     </msub>
     <mo>+</mo>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mrow>
        <msub>
         <mi>M</mi>
         <mn>0</mn>
        </msub>
        <mo>&#x2062;</mo>
        <msubsup>
         <mi>K</mi>
         <mn>0</mn>
         <mrow>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
        </msubsup>
        <mo>&#x2062;</mo>
        <msubsup>
         <mi>M</mi>
         <mn>0</mn>
         <mi>T</mi>
        </msubsup>
       </mrow>
       <mo>-</mo>
       <mrow>
        <msub>
         <mi>M</mi>
         <mi>n</mi>
        </msub>
        <mo>&#x2062;</mo>
        <msubsup>
         <mi>K</mi>
         <mi>n</mi>
         <mrow>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
        </msubsup>
        <mo>&#x2062;</mo>
        <msubsup>
         <mi>M</mi>
         <mi>N</mi>
         <mi>T</mi>
        </msubsup>
       </mrow>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mtext>    </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mrow>
    <msub>
     <mi>M</mi>
     <mi>n</mi>
    </msub>
    <mo>=</mo>
    <mrow>
     <mrow>
      <mo>(</mo>
      <mrow>
       <mrow>
        <msub>
         <mi>M</mi>
         <mn>0</mn>
        </msub>
        <mo>&#x2062;</mo>
        <msubsup>
         <mi>K</mi>
         <mn>0</mn>
         <mrow>
          <mo>-</mo>
          <mn>1</mn>
         </mrow>
        </msubsup>
       </mrow>
       <mo>+</mo>
       <msub>
        <mi>S</mi>
        <mrow>
         <mover accent="true">
          <mi>y</mi>
          <mo>&#x223c;</mo>
         </mover>
         <mo>&#x2062;</mo>
         <msup>
          <mover accent="true">
           <mi>y</mi>
           <mo>&#x223c;</mo>
          </mover>
          <mi>T</mi>
         </msup>
        </mrow>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mo>&#x2062;</mo>
     <msub>
      <mi>K</mi>
      <mi>n</mi>
     </msub>
    </mrow>
   </mrow>
   <mtext>   </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mrow>
    <msub>
     <mi>K</mi>
     <mi>n</mi>
    </msub>
    <mo>=</mo>
    <msup>
     <mrow>
      <mo>(</mo>
      <mrow>
       <msubsup>
        <mi>K</mi>
        <mn>0</mn>
        <mrow>
         <mo>-</mo>
         <mn>1</mn>
        </mrow>
       </msubsup>
       <mo>+</mo>
       <msub>
        <mi>S</mi>
        <mrow>
         <mover accent="true">
          <mi>y</mi>
          <mo>&#x223c;</mo>
         </mover>
         <mo>&#x2062;</mo>
         <msup>
          <mover accent="true">
           <mi>y</mi>
           <mo>&#x223c;</mo>
          </mover>
          <mi>T</mi>
         </msup>
        </mrow>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mrow>
      <mo>-</mo>
      <mn>1</mn>
     </mrow>
    </msup>
   </mrow>
   <mtext>   </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <msub>
    <mi>v</mi>
    <mi>n</mi>
   </msub>
   <mo>=</mo>
   <mrow>
    <msub>
     <mi>v</mi>
     <mn>0</mn>
    </msub>
    <mo>+</mo>
    <mi>n</mi>
   </mrow>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mrow>
    <mrow>
     <mi>where</mi>
     <mo>&#x2062;</mo>
     <mtext>    </mtext>
     <msub>
      <mi>S</mi>
      <mrow>
       <mi>y</mi>
       <mo>&#x2062;</mo>
       <msup>
        <mi>y</mi>
        <mi>T</mi>
       </msup>
      </mrow>
     </msub>
    </mrow>
    <mtext>&#x205f;</mtext>
    <mo>=</mo>
    <mrow>
     <mrow>
      <munder>
       <mo>&#x2211;</mo>
       <mrow>
        <mrow>
         <mi>t</mi>
         <mo>:</mo>
         <msub>
          <mi>x</mi>
          <mi>t</mi>
         </msub>
        </mrow>
        <mo>=</mo>
        <mi>k</mi>
       </mrow>
      </munder>
      <mrow>
       <msub>
        <mi>y</mi>
        <mi>t</mi>
       </msub>
       <mo>&#x2062;</mo>
       <msubsup>
        <mi>y</mi>
        <mi>t</mi>
        <mi>T</mi>
       </msubsup>
       <mo>&#x2062;</mo>
       <mtext>&#x205f;</mtext>
       <msub>
        <mi>S</mi>
        <mrow>
         <mover accent="true">
          <mi>y</mi>
          <mo>&#x223c;</mo>
         </mover>
         <mo>&#x2062;</mo>
         <msup>
          <mover accent="true">
           <mi>y</mi>
           <mo>&#x223c;</mo>
          </mover>
          <mi>T</mi>
         </msup>
        </mrow>
       </msub>
      </mrow>
     </mrow>
     <mo>=</mo>
     <mrow>
      <munder>
       <mo>&#x2211;</mo>
       <mrow>
        <mrow>
         <mi>t</mi>
         <mo>:</mo>
         <msub>
          <mi>x</mi>
          <mi>t</mi>
         </msub>
        </mrow>
        <mo>=</mo>
        <mi>k</mi>
       </mrow>
      </munder>
      <mrow>
       <msub>
        <mover accent="true">
         <mi>y</mi>
         <mi>&#x2dc;</mi>
        </mover>
        <mi>t</mi>
       </msub>
       <mo>&#x2062;</mo>
       <msubsup>
        <mover accent="true">
         <mi>y</mi>
         <mi>&#x2dc;</mi>
        </mover>
        <mi>t</mi>
        <mi>T</mi>
       </msubsup>
      </mrow>
     </mrow>
    </mrow>
   </mrow>
   <mtext>   </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <msub>
    <mi>S</mi>
    <mrow>
     <mi>y</mi>
     <mo>&#x2062;</mo>
     <msup>
      <mover accent="true">
       <mi>y</mi>
       <mo>&#x223c;</mo>
      </mover>
      <mi>T</mi>
     </msup>
    </mrow>
   </msub>
   <mtext>&#x205f;</mtext>
   <mo>=</mo>
   <mrow>
    <mrow>
     <munder>
      <mo>&#x2211;</mo>
      <mrow>
       <mrow>
        <mi>t</mi>
        <mo>:</mo>
        <msub>
         <mi>x</mi>
         <mi>t</mi>
        </msub>
       </mrow>
       <mo>=</mo>
       <mi>k</mi>
      </mrow>
     </munder>
     <mrow>
      <msub>
       <mi>y</mi>
       <mi>t</mi>
      </msub>
      <mo>&#x2062;</mo>
      <msubsup>
       <mover accent="true">
        <mi>y</mi>
        <mi>&#x2dc;</mi>
       </mover>
       <mi>t</mi>
       <mi>T</mi>
      </msubsup>
      <mo>&#x2062;</mo>
      <mtext>&#x205f;</mtext>
      <mi>n</mi>
     </mrow>
    </mrow>
    <mo>=</mo>
    <mrow>
     <mi>#</mi>
     <mo>&#x2062;</mo>
     <mrow>
      <mrow>
       <mo>{</mo>
       <mrow>
        <mrow>
         <mi>t</mi>
         <mo>:</mo>
         <msub>
          <mi>x</mi>
          <mi>t</mi>
         </msub>
        </mrow>
        <mo>=</mo>
        <mi>k</mi>
       </mrow>
       <mo>}</mo>
      </mrow>
      <mo>.</mo>
     </mrow>
    </mrow>
   </mrow>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<p id="p-0168" num="0149">Therefore resampling &#x3b8;|x,y includes three steps: collecting statistics from the data assigned to each state, forming each state's posterior hyperparameters, and updating each state's observation parameter by simulating a draw from the appropriate MNIW. Simulating (A,&#x3a3;) &#x2dc;MNIW(S<sub>n</sub>, v<sub>n</sub>, M<sub>n</sub>, K<sub>n</sub>) proceeds as</p>
<p id="p-0169" num="0000">
<maths id="MATH-US-00007" num="00007">
<math overflow="scroll">
 <mrow>
  <mrow>
   <mrow>
    <mo>&#x2211;</mo>
    <mrow>
     <mo>&#x223c;</mo>
     <mrow>
      <mi>InvWishart</mi>
      <mo>&#x2061;</mo>
      <mo>(</mo>
      <mrow>
       <msub>
        <mi>S</mi>
        <mi>n</mi>
       </msub>
       <mo>,</mo>
       <msub>
        <mi>v</mi>
        <mi>n</mi>
       </msub>
      </mrow>
      <mo>)</mo>
     </mrow>
    </mrow>
   </mrow>
   <mtext>   </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mi>A</mi>
   <mo>=</mo>
   <mrow>
    <mrow>
     <msub>
      <mi>M</mi>
      <mi>n</mi>
     </msub>
     <mo>+</mo>
     <mrow>
      <msup>
       <mo>&#x2211;</mo>
       <mfrac>
        <mn>1</mn>
        <mn>2</mn>
       </mfrac>
      </msup>
      <mrow>
       <mi>G</mi>
       <mo>&#x2062;</mo>
       <msubsup>
        <mi>K</mi>
        <mi>n</mi>
        <mrow>
         <mo>-</mo>
         <mfrac>
          <mn>1</mn>
          <mn>2</mn>
         </mfrac>
        </mrow>
       </msubsup>
       <mo>&#x2062;</mo>
       <mtext>   </mtext>
       <mi>where</mi>
       <mo>&#x2062;</mo>
       <mtext>   </mtext>
       <msub>
        <mi>G</mi>
        <mrow>
         <mi>i</mi>
         <mo>&#x2062;</mo>
         <mi>j</mi>
        </mrow>
       </msub>
      </mrow>
     </mrow>
    </mrow>
    <mover>
     <mo>&#x223c;</mo>
     <mi>iid</mi>
    </mover>
    <mrow>
     <mrow>
      <mi>N</mi>
      <mo>&#x2061;</mo>
      <mo>(</mo>
      <mrow>
       <mn>0</mn>
       <mo>,</mo>
       <mn>1</mn>
      </mrow>
      <mo>)</mo>
     </mrow>
     <mo>.</mo>
    </mrow>
   </mrow>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<heading id="h-0035" level="2">Sampling &#x3b2;, &#x3c0;|x</heading>
<p id="p-0170" num="0150">Sampling the transition parameters &#x3c0; and &#x3b2; given the state sequence x corresponds to updating the probabilities of transitions among behavioral modules to reflect the transition patterns observed in the state sequence. Updating &#x3b2; encouraged redundant behavioral modes to be pruned from the model, while updating each &#x3c0;<sub>ij </sub>fit the transitions observed from state i to state j.</p>
<p id="p-0171" num="0151">Resampling the transition parameters &#x3b2; and &#x3c0;, which are draws from the weak limit approximation to the (sticky) HDP, was performed using an auxiliary variable sampling scheme. That is, &#x3b2;,\pi|x was generated by first sampling auxiliary variables m|&#x3b2;,x. Then &#x3b2;,\pi|x, m was generated by first sampling from the marginal &#x3b2;|m and then the conditional &#x3c0;|&#x3b2;,x.</p>
<p id="p-0172" num="0152">The matrix of transition counts in the sampled state sequence x is</p>
<p id="p-0173" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?><i>n</i><sub>kj</sub><i>=#{t:x</i><sub>t</sub><i>=k,x</i><sub>t+1</sub><i>=j,t=</i>1,2, . . . ,<i>T&#x2212;</i>1}.<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<p id="p-0174" num="0153">Suppressing conditioning notation for simplicity, the auxiliary variables m={m<sub>kj</sub>:k,j=1,2, . . . , K} are sampled via</p>
<p id="p-0175" num="0000">
<maths id="MATH-US-00008" num="00008">
<math overflow="scroll">
 <mrow>
  <mrow>
   <mrow>
    <msub>
     <mi>m</mi>
     <mrow>
      <mi>k</mi>
      <mo>&#x2062;</mo>
      <mi>j</mi>
     </mrow>
    </msub>
    <mo>=</mo>
    <mrow>
     <munderover>
      <mo>&#x2211;</mo>
      <mrow>
       <mi>l</mi>
       <mo>=</mo>
       <mn>1</mn>
      </mrow>
      <msub>
       <mi>n</mi>
       <mrow>
        <mi>k</mi>
        <mo>&#x2062;</mo>
        <mi>j</mi>
       </mrow>
      </msub>
     </munderover>
     <msub>
      <mi>b</mi>
      <mrow>
       <mi>k</mi>
       <mo>&#x2062;</mo>
       <mi>j</mi>
       <mo>&#x2062;</mo>
       <mi>l</mi>
      </mrow>
     </msub>
    </mrow>
   </mrow>
   <mtext>   </mtext>
  </mrow>
  <mo>&#x2062;</mo>
  <mtext>
</mtext>
  <mrow>
   <mrow>
    <mi>where</mi>
    <mo>&#x2062;</mo>
    <mtext>   </mtext>
    <msub>
     <mi>b</mi>
     <mrow>
      <mi>k</mi>
      <mo>&#x2062;</mo>
      <mi>j</mi>
      <mo>&#x2062;</mo>
      <mi>l</mi>
     </mrow>
    </msub>
   </mrow>
   <mover>
    <mo>&#x223c;</mo>
    <mi>iid</mi>
   </mover>
   <mrow>
    <mi>Bernoulli</mi>
    <mo>&#x2062;</mo>
    <mtext>  </mtext>
    <mrow>
     <mo>(</mo>
     <mrow>
      <mfrac>
       <mrow>
        <mi>&#x3b1;</mi>
        <mo>&#x2062;</mo>
        <msub>
         <mi>&#x3b2;</mi>
         <mi>j</mi>
        </msub>
       </mrow>
       <mrow>
        <mrow>
         <mi>&#x3b1;</mi>
         <mo>&#x2062;</mo>
         <msub>
          <mi>&#x3b2;</mi>
          <mi>j</mi>
         </msub>
        </mrow>
        <mo>+</mo>
        <mi>&#x3ba;</mi>
       </mrow>
      </mfrac>
      <mo>&#x2062;</mo>
      <mfrac>
       <mrow>
        <mrow>
         <mi>&#x3b1;</mi>
         <mo>&#x2062;</mo>
         <msub>
          <mi>&#x3b2;</mi>
          <mi>j</mi>
         </msub>
        </mrow>
        <mo>+</mo>
        <mrow>
         <mi>&#x3ba;</mi>
         <mo>&#x2062;</mo>
         <msub>
          <mi>&#x3b4;</mi>
          <mrow>
           <mi>k</mi>
           <mo>&#x2062;</mo>
           <mi>j</mi>
          </mrow>
         </msub>
        </mrow>
       </mrow>
       <mrow>
        <mrow>
         <mi>&#x3b1;</mi>
         <mo>&#x2062;</mo>
         <msub>
          <mi>&#x3b2;</mi>
          <mi>j</mi>
         </msub>
        </mrow>
        <mo>+</mo>
        <mi>l</mi>
        <mo>+</mo>
        <mrow>
         <mi>&#x3ba;</mi>
         <mo>&#x2062;</mo>
         <msub>
          <mi>&#x3b4;</mi>
          <mrow>
           <mi>k</mi>
           <mo>&#x2062;</mo>
           <mi>j</mi>
          </mrow>
         </msub>
        </mrow>
       </mrow>
      </mfrac>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<p id="p-0176" num="0154">where Bernoulli(p) denotes a Bernoulli random variable that takes value 1 with probability p and takes value 0 otherwise. Note that the update for the HDP-HMM without a sticky bias corresponds to setting &#x3ba;=0 in these updates.</p>
<p id="p-0177" num="0155">Given the auxiliary variables, the update to is a Dirichlet-multinomial conjugate one, where</p>
<p id="p-0178" num="0000">
<maths id="MATH-US-00009" num="00009">
<math overflow="scroll">
 <mrow>
  <mi>&#x3b2;</mi>
  <mo>|</mo>
  <mrow>
   <mi>m</mi>
   <mo>&#x223c;</mo>
   <mrow>
    <mi>D</mi>
    <mo>&#x2062;</mo>
    <mi>i</mi>
    <mo>&#x2062;</mo>
    <mrow>
     <mi>r</mi>
     <mo>&#x2061;</mo>
     <mo>(</mo>
     <mrow>
      <mrow>
       <mfrac>
        <mi>&#x3b3;</mi>
        <mi>K</mi>
       </mfrac>
       <mo>+</mo>
       <mrow>
        <mi>m</mi>
        <msub>
         <mo>.</mo>
         <mn>1</mn>
        </msub>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mrow>
       <mfrac>
        <mi>&#x3b3;</mi>
        <mi>K</mi>
       </mfrac>
       <mo>+</mo>
       <mrow>
        <mi>m</mi>
        <msub>
         <mo>.</mo>
         <mn>2</mn>
        </msub>
       </mrow>
      </mrow>
      <mo>,</mo>
      <mo>&#x2026;</mo>
      <mtext>   </mtext>
      <mo>,</mo>
      <mrow>
       <mrow>
        <mi>&#x3b3;</mi>
        <mo>/</mo>
        <mi>K</mi>
       </mrow>
       <mo>+</mo>
       <mrow>
        <mi>m</mi>
        <msub>
         <mo>.</mo>
         <mi>K</mi>
        </msub>
       </mrow>
      </mrow>
     </mrow>
     <mo>)</mo>
    </mrow>
   </mrow>
  </mrow>
 </mrow>
</math>
</maths>
</p>
<p id="p-0179" num="0156">where m.<sub>j</sub>=&#x3a3;<sub>k=1</sub><sup>K</sup>m<sub>kj </sub>for j=1,2, . . . , K. The update to &#x3c0;|&#x3b2;,x is similar, with</p>
<p id="p-0180" num="0000">
<br/>
<?in-line-formulae description="In-line Formulae" end="lead"?>&#x3c0;<sub>k</sub><i>|&#x3b2;,x&#x2dc;Dir</i>(&#x3b1;&#x3b2;<sub>1</sub><i>+n</i><sub>k1</sub>, . . . ,&#x3b1;&#x3b2;<sub>j</sub><i>+n</i><sub>kj</sub>+&#x3ba;&#x3b4;<sub>kj</sub>, . . . ,\alpha&#x3b2;<sub>K</sub><i>+n</i><sub>kK</sub>).<?in-line-formulae description="In-line Formulae" end="tail"?>
</p>
<heading id="h-0036" level="2">Application of the Models to the Examples</heading>
<p id="p-0181" num="0157">Datasets from the open-field, odor, and genetic manipulation experiments were modeled jointly to increase statistical power. Because the neural implants associated with the optogenetics experiment modestly altered the profile of the animal, these data were modeled separately. In all experiments, the first 10 principal components for each frame of each imaged mouse were gathered. Data were then subdivided and assigned either a &#x201c;train&#x201d; or a &#x201c;test&#x201d; label, in a 3:1 train:test ratio. The mice labeled &#x201c;test&#x201d; were held-out from the training process, and used to test generalization performance via measurement held-out likelihood. This approach allowed us to directly compare algorithms whose composition reflected different underlying structures for behavior.</p>
<p id="p-0182" num="0158">We trained models on data using the procedures described herein; modeling was robust to both initialization settings and to parameter and hyperparameter settings (with the exception of kappa, see below). Specifically, the number of lags used in our AR observation distributions and the number of used states in our transition matrix with an HDP prior was found to be robust to the particular hyperparameter settings on both priors. We varied the hyperparameters of our sparsifying ARD prior by several orders of magnitude, and held-out likelihood, the number of used lags, and the number of used states varied negligibly. We also varied the hyperparameters of our HDP prior by several orders of magnitude and again observed no change to the number of used states or held-out likelihood. All jointly-trained data shared observation distributions, but each treatment class was allowed its own transition matrix. Each model was updated through <b>1000</b> iterations of Gibbs sampling; upon the last iteration of Gibbs sampling the model output was saved; all further analysis was performed on this final update.</p>
<p id="p-0183" num="0159">The &#x201c;stickiness&#x201d; of the duration distribution of our behavioral modules&#x2014;defined by the kappa setting of the model&#x2014;influenced the average duration of behavioral modules discovered by the AR-HMM; this allowed us to control the temporal scale at which behavior was modeled. As discussed in the main text, autocorrelation, power spectral density, and the changepoint algorithm identified switching dynamics at a specific sub-second temporal scale (as encapsulated by the changepoints duration distribution and reflected by the spectrogram and autocorrelogram). We therefore empirically set the kappa stickiness parameter of the time-series model to best match the duration distribution discovered by changepoint detection. To find the kappa setting at which these distributions were best matched, we minimized the Kolmogorov-Smirnov distance between the inter-changepoint interval distribution and the posterior behavioral module duration distribution through a dense grid search.</p>
<heading id="h-0037" level="2">Mouse Strains, Housing and Habituation</heading>
<p id="p-0184" num="0160">Unless otherwise noted, all experiments were performed on 6-8 week old C57/BL6 males (Jackson Laboratories). Mice from the ror&#x3b2; and rbp4 strains were habituated and tested identically to the reference C57/BL6 mice. Mice were brought into our colony at 4 weeks of age, where they were group-housed for two weeks in a reverse 12 hours light/12 hours dark cycle. On the day of testing, mice were brought into the laboratory in a light-tight container, where they were habituated in darkness for 30 minutes before testing.</p>
<heading id="h-0038" level="1">Example 1. Behavioral Assays: Innate Exploration</heading>
<p id="p-0185" num="0161">To address these possibilities, we first used the AR-HMM to define the baseline architecture of mouse exploratory behavior in the open field, and then asked how this template for behavior was modified through distinct manipulations of the external world.</p>
<p id="p-0186" num="0162">For the open field assay (OFA), mice were habituated as noted above, and then placed in the middle of a circular 18&#x2033; diameter enclosure with 15&#x2033;-high walls (US Plastics), immediately after which 3D video recording was begun. The animal was allowed to freely explore the enclosure for the 30 minute experimental period. Mice whose behavior was assessed in a square box were handled and measured identically to the OFA, except in the odor box described below.</p>
<p id="p-0187" num="0163">The AR-HMM identified &#x2dc;60 reliably-used behavioral modules (51 modules explained 95 percent of imaging frames, and 65 modules explained 99 percent of imaging frames, <figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B</figref>) from the circular open field dataset, which is representative of normal mouse exploratory behavior in the laboratory (<figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, n=25 animals, 20 minute trials). <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> shows the proportion of frames explained by each module (Y axis), plotted against the set of modules, sorted by usage (X axis). Ninety-five percent of frames were explained by 51 behavioral modules; ninety-nine percent of frames were explained by 62 behavioral modules in the open field dataset.</p>
<p id="p-0188" num="0164"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> shows modules (X axis) sorted by usage (Y axis) with Bayesian credible intervals indicated. Note that all the credible intervals are smaller than the SEs computed based upon the bootstrap estimates (<figref idref="DRAWINGS">FIG. <b>5</b>B</figref>). As noted above, many of these modules encode human-describable components of behavior (e.g. rears, walking, pauses, turns).</p>
<p id="p-0189" num="0165">The AR-HMM also measures the probability that any given module precedes or follows any other module; in other words, after model training each module is assigned a pairwise transition probability with every other module in the set; these probabilities summarize the sequences of modules that were expressed by the mouse during behavior. Plotting these transition probabilities as a matrix revealed that they were highly non-uniform, with each module preferentially connected in time to some modules and not others (<figref idref="DRAWINGS">FIG. <b>6</b>B</figref>; average node degree without thresholding 16.82&#xb1;0.95, after thresholding bigram probabilities lower than 5 percent. 4.08&#xb1;0.10). This specific connectivity between pairs of modules restricted the module sequences that were observed in the dataset (8900/&#x2dc;125,000 possible trigrams) demonstrating that certain module sequences were favored; this observation suggests that mouse behavior is predictable, as knowing what the mouse is doing at any given moment in time informs an observer about what the mouse is likely to do next. Information theoretic analysis of the transition matrix confirmed that mouse behavior is significantly predictable, as the average per-frame entropy rate was low relative to a uniform transition matrix (without self-transitions 3.78&#xb1;0.03 bits, with self-transitions 0.72&#xb1;0.01 bits, entropy rate in uniform matrix 6.022 bits), and the average mutual information between interconnected modules was significantly above zero (without self-transitions 1.92&#xb1;0.02 bits, with self-transitions 4.84 bits&#xb1;0.03 bits). This deterministic quality to behavior likely serves to ensure that the mouse emits coherent patterns of motion; consistent with this possibility, upon inspection frequently-observed module sequences were found to encode different aspects of exploratory behavior.</p>
<p id="p-0190" num="0166">The behavior expressed by mice in the circular open field reflects a context-specific pattern of locomotor exploration. We hypothesized that mice would adapt to changes in apparatus shape by focally altering the structure of behavior to generate new pose dynamics to interact with specific physical features of the environment; to test this hypothesis, we imaged mice within a smaller square box and then co-trained our model with both the circular open field data and square data, thereby enabling direct comparisons of modules and transition under both conditions (n=25 mice in each condition). Although mice tended to explore the corners of the square box and the walls of the circular open field, the overall usage of most modules was similar between these apparatuses, consistent with exploratory behavior sharing many common features across arenas (<figref idref="DRAWINGS">FIG. <b>6</b>C</figref>). The AR-HMM also identified a small number of behavioral modules that were deployed extensively in one context, but negligibly or not at all in the other, consistent with the idea that different physical environments drive expression of new behavioral modules (<figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, for all usage differences discussed below p&#x3c;10<sup>&#x2212;3 </sup>based upon bootstrap estimation).</p>
<p id="p-0191" num="0167">Interestingly, these &#x201c;new&#x201d; modules are not only deployed during physical interactions with specific features of the apparatus&#x2014;which would be predicted to elicit new pose dynamics&#x2014;but also during unconstrained periods of exploration. For example, one circular arena-specific module encoded a thigmotactic behavior in which the mouse locomotes near the arena wall with a body posture that matches the curvature of the wall. This module was also expressed when the mouse is closer to the center of the circular arena and not in physical contact with the wall, demonstrating that expression of this module is not simply the direct consequence of physical interactions with the wall but rather reflects the behavioral state of the mouse in a curved arena; while thigmotaxis also occurred in the square box, the associated behavioral module encodes locomotion with a straight body and was used during straight trajectories in both square and circular apparatuses (<figref idref="DRAWINGS">FIGS. <b>6</b>D-E</figref>, middle panels). Similarly, within the square box mice expressed a context-specific module that encodes a dart from the center of the square to one of the adjacent corners; this pattern of motion likely was a consequence of the square having a small central open field, and was not the specific product of a physical constraint placed upon the mouse.</p>
<p id="p-0192" num="0168">A number of additional modules were found to be preferentially expressed in one context or the other; these upregulated modules appeared to encode behaviors that were deployed in allocentric patterns specified by the shape of the arena. In the circular arena, for example the mouse preferentially expressed a rear in which the mouse's body points outwards while it pauses near the center of the open field, while in the smaller square box mice preferentially executed a high rear in the corners of the box (<figref idref="DRAWINGS">FIG. <b>6</b>E</figref>, data not shown). These results suggest that what the mouse does (i.e. its egocentric behavior) is modulated based upon where in space the mouse is (i.e. its allocentric position). Taken together, these data demonstrate that mice adapt to new physical environments, at least in part, through recruitment of a limited set of context-specific behavioral modules (that encode context-appropriate pose dynamics) into baseline patterns of action; these new modules&#x2014;along with other modules whose expression is enriched in one context or the other&#x2014;are differentially deployed in space to respond to changes in the environment.</p>
<heading id="h-0039" level="1">Example 2. Behavioral Assays: Stimulus-Driven Innate Behaviors&#x2014;Response to Odorants</heading>
<p id="p-0193" num="0169">Because mice express the same underlying behavioral state&#x2014;locomotor exploration&#x2014;in both the circle and the square one might predict that the observed changes to behavioral modules in this case would be focal and limited in extent. Therefore, the inventors asked how the underlying structure of behavior is altered when mice are exposed to a sensory cue&#x2014;within an otherwise-constant physical environment&#x2014;that drives a global change in behavioral state that includes the expression of new and motivated actions.</p>
<p id="p-0194" num="0170">To assess innate behavioral responses to volatile odorants, the inventors developed an odor delivery system that spatially isolates odors in specific quadrants of a square box. Each 12&#x2033;&#xd7;12&#x2033; box was constructed of &#xbe;&#x2033; black matte acrylic (Altech Plastics), with &#xbe;&#x2033; holes patterning the bottom of the box in a cross formation, and a 1/16&#x2033; thick glass cover (Tru Vue). These holes were tapped and connected via PTFE tubing to a vacuum manifold (Sigma Aldrich) that provides negative pressure to isolate odors within quadrants. Odor was injected into the box through &#xbd;&#x2033; NPT-&#x215c;%&#x2033; pipe-fittings (Cole-Parmer). Filtered air (1.0 L/min) was blown over odorant-soaked blotting paper (VWR) placed at the bottom of Vacutainer syringe vials (Covidien). The odorized airstream was then passed through corrugated PTFE tubing (Zeus) into one of the four pipe-fittings in a corner of the odor box.</p>
<p id="p-0195" num="0171">The inventors verified the ability of the odor box to isolate odors within specified quadrants by visualizing vaporized odor or smoke through sheet illumination of the box with a low-power handheld HeNe laser. This approach allowed us to tune the vacuum flow and odor flow rates to achieve odor isolation, which was verified using a photoionization device (Aurora Scientific). To eliminate the possibility of cross contamination between experiments, the odor boxes were soaked in a 1% Alconox solution overnight, then thoroughly cleaned with a 70% ethanol solution. Mice were habituated to the experimental room for 30 minutes before the initiation of the experiment. Under control conditions, dipropylene glycol with air (1.0 L/min) was delivered to each of the four corners of the apparatus before a single mouse was placed in the center of the box and allowed to freely explore while 3D video records were acquired for 20 minutes. The same cohort of animals was tested for odor responses by subsequently repeating the experiment with odorized air delivered to one of the four quadrants. All 3D video recordings are performed in total darkness. TMT was obtained from Pherotech, and used at 5% concentration.</p>
<p id="p-0196" num="0172">Mice exploring the square box were therefore exposed to the aversive fox odor trimethylthiazoline (TMT), which was delivered to one quadrant of the box via olfactometer. This odorant initiates a complex and profound behavioral state change including odor investigation, and escape and freezing behaviors that are accompanied by increases in corticosteroid and endogenous opioid levels. Consistent with these known effects, mice sniffed the odor-containing quadrant, and then avoided the quadrant containing the predator cue and displayed prolonged periods of immobility traditionally described as freezing behavior (<figref idref="DRAWINGS">FIG. <b>7</b></figref>). <figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a histogram depicting the average velocity of the modules that were differentially upregulated and interconnected after TMT exposure &#x201c;freezing&#x201d; compared to all other modules in the dataset.</p>
<p id="p-0197" num="0173">Surprisingly, this suite of new behaviors was encoded by the same set of behavioral modules that were expressed during normal exploration; several modules were up- or down-regulated after TMT exposure, but no new modules were introduced or eliminated relative to control (n=25 animals in control conditions, n=15 in TMT, model was co-trained on both datasets simultaneously). Instead, TMT altered the usage of and transition probabilities between specific modules, leading to newly-favored behavioral sequences that encode TMT-regulated behaviors (for all usage and transition differences discussed below p&#x3c;10<sup>&#x2212;3 </sup>based upon bootstrap estimation).</p>
<p id="p-0198" num="0174">Plotting the module transitions altered after exposure to TMT defined two neighborhoods within the behavioral statemap; the first included an expansive set of modules and interconnections that were modestly downregulated by TMT, and the second included a focused set of modules and transitions that were upregulated by TMT). During normal behavior these newly-interconnected modules were temporally dispersed and individually appear to encode for different morphological forms of pausing or balling up. In contrast, under the influence of TMT these modules were concatenated into new sequences that, upon inspection and quantification, were found to encode freezing behavior (average during-sequence velocity &#x2212;0.14&#xb1;0.54 mm/sec, for other modules 34.7&#xb1;53 mm/sec). For example, the most commonly-expressed freezing trigram was expressed 716 times after TMT exposure (in 300 minutes of imaging), as opposed to just 17 times under control conditions (in 480 minutes of imaging). The TMT-induced neighborhood structure imposed upon these pausing modules to create freezing demonstrates that behavior can be altered through focal changes in transition probabilities. This local rewriting of transition probabilities was accompanied by an increase in the overall determinism of mouse behavior&#x2014;its global pattern of action became more predictable as a consequence of TMT exposure (per frame entropy rate fell from 3.92&#xb1;0.02 bits to 3.66&#xb1;0.08 bits without self-transitions, and from 0.82&#xb1;0.01 bits to 0.64&#xb1;0.02 bits with self-transitions) consistent with the mouse enacting an deterministic avoidance strategy.</p>
<p id="p-0199" num="0175">Proximity to the odor source also governed the pattern of expression of specific behavioral modules (<figref idref="DRAWINGS">FIGS. <b>8</b>D-<b>8</b>E</figref>). For example, a set of freezing-related modules tended to be expressed in the quadrant most distal from the odor source, while the expression of an investigatory rearing module (whose overall usage was not altered by TMT) was specifically enriched within the odor quadrant (<figref idref="DRAWINGS">FIGS. <b>8</b>D-<b>8</b>E</figref>). Together, these findings suggest two additional mechanisms through which the mouse nervous system can generate new and adaptive behaviors. First, the transition structure between individual modules that are otherwise normally associated with a different behavioral state, such as locomotor exploration, can be altered to generate new behaviors such as freezing. Second, the spatial patterns of deployment of pre-existing modules and sequences can be regulated to support motivated behaviors such as odor investigation and avoidance. Behavioral modules are not, therefore, simply reused over time, but instead act as flexibly interlinked components of behavioral sequences whose expression is dynamically regulated both in time and space.</p>
<heading id="h-0040" level="1">Example 3. The Effect of Genes and Neural Circuits on Modules</heading>
<p id="p-0200" num="0176">As described above, the fine-timescale structure of behavior is selectively vulnerable to changes in the physical or sensory environment that influence action over timescales of minutes. Furthermore, the AR-HMM appears to comprehensively encapsulate the pattern of behavior expressed by the mouse (within the limits of our imaging). These observations suggest that the AR-HMM&#x2014;which affords a systematic window into mouse behavior at the sub-second timescale&#x2014;may be able to both quantify obvious behavioral phenotypes and to reveal new or subtle phenotypes induced after experimental manipulations that influence behavior across a range of spatiotemporal scales.</p>
<p id="p-0201" num="0177">To explore how changes in individual genes&#x2014;which act on timescales of the lifetime of the mouse&#x2014;might impact fast behavioral modules and transitions, we characterized the phenotype of mice mutant for the retinoid-related orphan receptor 1&#x3b2; (Ror1&#x3b2;) gene, which is expressed in neurons in the brain and spinal cord; we selected this mouse for analysis because homozygous mutant animals exhibit abnormal gait<sup>37-40</sup>, which we would expect to be detected by the AR-HMM. After imaging and modeling, littermate control mice were found to be nearly indistinguishable from fully inbred C57/B16 mice, whereas mutant mice expressed a unique behavioral module that encoded a waddling gait (<figref idref="DRAWINGS">FIG. <b>9</b>A, <b>9</b>C</figref>). This alteration in behavior was accompanied by its converse: the expression of five behavioral modules encoding normal forward locomotion at different speeds in wild-type and C57 mice was downregulated in the Ror1&#x3b2; mutant (<figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, average during-module velocity=114.6&#xb1;76.3 mm/sec). In addition, expression of a set of four modules that encoded brief pauses and headbobs were also upregulated (<figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, average during-module velocity=8.8&#xb1;25.3 mm/sec); this pausing phenotype had not previously been reported in the literature. Interestingly, heterozygous mice&#x2014;which have no reported phenotype<sup>37-40</sup>, appear normal by eye, and exhibit wild-type running wheel behavior<sup>40</sup>&#x2014;also were found to express a fully-penetrant mutant phenotype: they overexpressed the same set of pausing modules that were upregulated in the full Ror1&#x3b2; mutants, while failing to express the more dramatic waddling phenotype (<figref idref="DRAWINGS">FIG. <b>9</b>A</figref>).</p>
<p id="p-0202" num="0178">The AR-HMM therefore describes the pathological behavior of Ror1&#x3b2; mice as the combination of a single neomorphic waddling module and the increased expression of a small group of physiological modules encoding pausing behaviors; heterozygote mice express a defined subset of these behavioral abnormalities, whose penetrance is not intermediate but equals that observed in the mutant. These results suggest that the sensitivity of the AR-HMM enables fractionation of severe and subtle behavioral abnormalities within the same litter of animals, enables discovery of new phenotypes, and facilitates comparisons amongst genotypes. These experiments also demonstrate that genotype-dependent variations in behavior, the consequence of the indelible and lifetime alteration of a specific gene in the genome, can influence module expression and transition statistics that operate on timescales of milliseconds.</p>
<heading id="h-0041" level="1">Example 4. Behavioral Assays: Optogenetics&#x2014;Effect of Neural Activity on Modules</heading>
<p id="p-0203" num="0179">Finally, the inventors wished to ask whether the behavioral structure captured by the AR-HMM would offer insight into fleeting or unreliable changes in behavior. The inventors therefore briefly triggered neural activity in motor circuits, and asked how stimulation at different levels of intensity influenced the moment-to-moment organization of behavior. The inventors unilaterally expressed the light-gated ion channel Channelrhodopsin-2 in corticostriatal neurons<sup>41,42 </sup>and assessed behavioral responses before, during and after two seconds of light-mediated activation of motor cortex (n=4 mice, model was trained separately from previous experiments).</p>
<p id="p-0204" num="0180">Four adult male Rbp4-Cre (The Jackson Laboratory) mice were anesthetized with 1.5% isoflurane and placed in a stereotaxic frame (Leica). Microinjection pipettes (O.D. 10-15 &#x3bc;m) were inserted into the left motor cortex (coordinates from Bregma: 0.5 AP, &#x2212;1 ML, 0.60 DV). 0.5 &#x3bc;l of AAV5.EF1a.DIO.hChR2(H134R)-eYFP.WPRE.hGH (&#x2dc;10<sup>12 </sup>infectious units/mL, Penn Vector Core) were injected in each mouse over 10 minutes followed by an additional 10 minutes to allow diffusion of viral particles away from the injection site. After the injection, a bare optic fiber with a zirconia ferrule (O.D. 200 &#x3bc;m, 0.37 numerical aperture) was inserted 100 &#x3bc;m above the injection site and secured to the skull with acrylic cement (Lang). Twenty-eight days following the viral injection, mice were placed in a circular arena and the optical implant was coupled to a laser pump (488 nm, CrystaLaser) via a patch-chord and a rotary joint (Doric Lenses). The laser was directly controlled from a PC. After 20 minutes of familiarization to the arena, the optostimulation was started. The laser power, the pulse width, the inter-pulse interval and the inter-train interval were controlled by custom-made software (NI Labview). Each train of laser pulses consisted of 30 pulses (pulse width: 50 ms) at 15 Hz. The interval between successive trains was set to 18 seconds. 50 trains were delivered for each laser intensity. The animal was progressively exposed to higher laser intensities over the course of the experiment.</p>
<p id="p-0205" num="0181">At the lowest power levels no light-induced changes in behavior were observed, while at the highest power levels the AR-HMM identified two behavioral modules whose expression was reliably induced by the light (<figref idref="DRAWINGS">FIG. <b>10</b>A</figref>). Neither of these modules were expressed during normal mouse locomotion; inspection revealed them to encode two forms of spinning behavior (differing in their length and the angle of turn), in which the mouse traces out semi-circles or donuts in space (<figref idref="DRAWINGS">FIG. <b>10</b>B</figref>). The induction of neomorphic behaviors after strong unilateral motor cortex stimulation is not surprising, although it is important to note that the AR-HMM both recognized these behaviors as new and encapsulated them as two unique behavioral modules. However, we noted that approximately 40 percent of the time, the overall pattern of behavior did not return to baseline for several seconds after light offset. This deviation from baseline was not due to continued expression of the modules triggered at light onset; instead, mice often expressed a pausing module (average during-module velocity=0.8&#xb1;7 mm/sec) at light offset as if &#x201c;resetting&#x201d; after a non-volitional movement.</p>
<p id="p-0206" num="0182">The behavioral changes induced by high intensity optogenetic stimulation were reliable, as on essentially every trial the animal emitted one of the two spinning modules. We then asked whether the sensitivity of the AR-HMM would enable quantitative analysis of more subtle changes in behavior, as occurs in intermediate regimes of motor cortex stimulation that elicit unreliable emission of specific behavioral modules. The inventors therefore titrated the levels of light stimulation down until one of the two neomorphic behavioral modules was no longer detected, and the other was expressed on only 25 percent of trials. Surprisingly, we were then could detect the upregulation of a second set of behavioral modules, each of which was expressed about 25 percent of the time (<figref idref="DRAWINGS">FIG. <b>10</b>A</figref>). These modules were not neomorphic, but rather were normally expressed during physiological exploration, and encoded a turn and a head-bobbing behavior (data not shown). While each of these individual light-regulated modules was emitted unreliably, taken in aggregate the behavioral changes across all modules suggested that lower-level neural activation reliably influenced behavior, but largely through inducing physiological rather than neomorphic actions (<figref idref="DRAWINGS">FIG. <b>10</b>A</figref>). Taken together, the detection of both stimulus-locked induction of behavioral modules and the lingering effects of stimulation of module usage demonstrates that neurally-induced changes in behavior can influence the sub-second structure of behavior. Furthermore, the identification of a physiologically-expressed set of light-regulated behavioral modules&#x2014;whose induction would not have been apparent under strong stimulation conditions&#x2014;also suggests that the AR-HMM can reveal subtle relationships between neural circuits and the time-series structure of behavior.</p>
<heading id="h-0042" level="2">Computer &#x26; Hardware Implementation of Disclosure</heading>
<p id="p-0207" num="0183">It should initially be understood that the disclosure herein may be implemented with any type of hardware and/or software, and may be a pre-programmed general purpose computing device. For example, the system may be implemented using a server, a personal computer, a portable computer, a thin client, or any suitable device or devices. The disclosure and/or components thereof may be a single device at a single location, or multiple devices at a single, or multiple, locations that are connected together using any appropriate communication protocols over any communication medium such as electric cable, fiber optic cable, or in a wireless manner.</p>
<p id="p-0208" num="0184">It should also be noted that the disclosure is illustrated and discussed herein as having a plurality of modules which perform particular functions. It should be understood that these modules are merely schematically illustrated based on their function for clarity purposes only, and do not necessary represent specific hardware or software. In this regard, these modules may be hardware and/or software implemented to substantially perform the particular functions discussed. Moreover, the modules may be combined together within the disclosure, or divided into additional modules based on the particular function desired. Thus, the disclosure should not be construed to limit the present invention, but merely be understood to illustrate one example implementation thereof.</p>
<p id="p-0209" num="0185">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.</p>
<p id="p-0210" num="0186">Implementations of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;) and a wide area network (&#x201c;WAN&#x201d;), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).</p>
<p id="p-0211" num="0187">Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Implementations of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. A computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Moreover, while a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal. The computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).</p>
<p id="p-0212" num="0188">The operations described in this specification can be implemented as operations performed by a &#x201c;data processing apparatus&#x201d; on data stored on one or more computer-readable storage devices or received from other sources.</p>
<p id="p-0213" num="0189">The term &#x201c;data processing apparatus&#x201d; encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.</p>
<p id="p-0214" num="0190">A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p>
<p id="p-0215" num="0191">The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).</p>
<p id="p-0216" num="0192">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p>
<heading id="h-0043" level="1">REFERENCES</heading>
<p id="p-0217" num="0000">
<ul id="ul0001" list-style="none">
    <li id="ul0001-0001" num="0193">1 Fettiplace, R. &#x26; Fuchs, P. A. Mechanisms of hair cell tuning. <i>Annual review of physiology </i>61, 809-834, (1999).</li>
    <li id="ul0001-0002" num="0194">2 Fettiplace, R. &#x26; Kim, K. X. The Physiology of Mechanoelectrical Transduction Channels in Hearing. <i>Physiological reviews </i>94, 951-986, (2014).</li>
    <li id="ul0001-0003" num="0195">3 Gollisch, T. &#x26; Herz, A. M. V. Disentangling Sub-Millisecond Processes within an Auditory Transduction Chain. <i>PLoS Biology </i>3, e8, (2005).</li>
    <li id="ul0001-0004" num="0196">4 Kawasaki, M., Rose, G. &#x26; Heiligenberg, W. Temporal hyperacuity in single neurons of electric fish. <i>Nature </i>336, 173-176, (1988).</li>
    <li id="ul0001-0005" num="0197">5 Nemenman, I., Lewen, G. D., Bialek, W. &#x26; de Ruyter van Steveninck, R. R. Neural Coding of Natural Stimuli: Information at Sub-Millisecond Resolution. <i>PLoS computational biology </i>4, e1000025, (2008).</li>
    <li id="ul0001-0006" num="0198">6 Peters, A. J., Chen, S. X. &#x26; Komiyama, T. Emergence of reproducible spatiotemporal activity during motor learning. <i>Nature </i>510, 263-267, (2014).</li>
    <li id="ul0001-0007" num="0199">7 Ritzau-Jost, A., Delvendahl, I., Rings, A., Byczkowicz, N., Harada, H., Shigemoto, R., Hirrlinger, J., Eilers, J. &#x26; Hallermann, S. Ultrafast Action Potentials Mediate Kilohertz Signaling at a Central Synapse. <i>Neuron </i>84, 152-163, (2014).</li>
    <li id="ul0001-0008" num="0200">8 Shenoy, K. V., Sahani, M. &#x26; Churchland, M. M. Cortical Control of Arm Movements: A Dynamical Systems Perspective. <i>Annual review of neuroscience </i>36, 337-359, (2013).</li>
    <li id="ul0001-0009" num="0201">9 Bargmann, C. I. Beyond the connectome: How neuromodulators shape neural circuits. <i>BioEssays </i>34, 458-465, (2012).</li>
    <li id="ul0001-0010" num="0202">10 Tinbergen, N. <i>The study of instinct</i>. (Clarendon Press, 1951).</li>
    <li id="ul0001-0011" num="0203">11 Garrity, P. A., Goodman, M. B., Samuel, A. D. &#x26; Sengupta, P. Running hot and cold: behavioral strategies, neural circuits, and the molecular machinery for thermotaxis in C. elegans and Drosophila. <i>Genes </i>&#x26;<i>amp; Developmen</i>t 24, 2365-2382, (2010).</li>
    <li id="ul0001-0012" num="0204">12 Stephens, G. J., Johnson-Kerner, B., Bialek, W. &#x26; Ryu, W. S. Dimensionality and Dynamics in the Behavior of C. elegans. <i>PLoS computational biology </i>4, e1000028, (2008).</li>
    <li id="ul0001-0013" num="0205">13 Stephens, G. J., Johnson-Kerner, B., Bialek, W. &#x26; Ryu, W. S. From Modes to Movement in the Behavior of Caenorhabditis elegans. <i>PLoS ONE </i>5, e13914, (2010).</li>
    <li id="ul0001-0014" num="0206">14 Vogelstein, J. T., Vogelstein, J. T., Park, Y., Park, Y., Ohyama, T., Kerr, R. A., Kerr, R. A., Truman, J. W., Truman, J. W., Priebe, C. E., Priebe, C. E., Zlatic, M. &#x26; Zlatic, M. Discovery of brainwide neural-behavioral maps via multiscale unsupervised structure learning. <i>Science </i>(<i>New York, N.Y</i>.) 344, 386-392, (2014).</li>
    <li id="ul0001-0015" num="0207">15 Berman, G. J., Choi, D. M., Bialek, W. &#x26; Shaevitz, J. W. Mapping the structure of drosophilid behavior. (2013).</li>
    <li id="ul0001-0016" num="0208">16 Croll, N. A. Components and patterns in the behaviour of the nematode Caenorhabditis elegans. <i>Journal of zoology </i>176, 159-176, (1975).</li>
    <li id="ul0001-0017" num="0209">17 Pierce-Shimomura, J. T., Morse, T. M. &#x26; Lockery, S. R. The fundamental role of pirouettes in Caenorhabditis elegans chemotaxis. <i>Journal of Neuroscience </i>19, 9557-9569,</li>
    <li id="ul0001-0018" num="0210">18 Gray, J. M., Hill, J. J. &#x26; Bargmann, C. I. A circuit for navigation in Caenorhabditis elegans. <i>Proceedings of the National Academy of Sciences of the United States of America </i>102, 3184-3191, (2005).</li>
    <li id="ul0001-0019" num="0211">19 Miller, A. C., Thiele, T. R., Faumont, S., Moravec, M. L. &#x26; Lockery, S. R. Step-response analysis of chemotaxis in Caenorhabditis elegans. <i>Journal of Neuroscience </i>25, 3369-3378, (2005).</li>
    <li id="ul0001-0020" num="0212">20 Jhuang, H., Garrote, E., Yu, X., Khilnani, V., Poggio, T., Steele, A. D. &#x26; Serre, T. Automated home-cage behavioural phenotyping of mice. <i>Nature Communications </i>1, 68, (2010).</li>
    <li id="ul0001-0021" num="0213">21 Stewart, A., Liang, Y., Kobla, V. &#x26; Kalueff, A. V. Towards high-throughput phenotyping of complex patterned behaviors in rodents: Focus on mouse self-grooming and its sequencing. <i>Behavioural brain </i>. . . , (2011).</li>
    <li id="ul0001-0022" num="0214">22 Ohayon, S., Avni, O., Taylor, A. L., Perona, P. &#x26; Egnor, S. E. R. Automated multi-day tracking of marked mice for the analysis of social behavior. <i>Journal of neuroscience methods, </i>1-25, (2013).</li>
    <li id="ul0001-0023" num="0215">23 de Chaumont, F., Coura, R. D.-S., Serreau, P., Cressant, A., Chabout, J., Granon, S. &#x26; Olivo-Marin, J.-C. Computerized video analysis of social interactions in mice. <i>Nature Methods </i>9, 410-417, (2012).</li>
    <li id="ul0001-0024" num="0216">24 Kabra, M., Robie, A. A., Rivera-Alba, M., Branson, S. &#x26; Branson, K. JAABA: interactive machine learning for automatic annotation of animal behavior. <i>Nature Methods </i>10, 64-67, (2013).</li>
    <li id="ul0001-0025" num="0217">25 Weissbrod, A., Shapiro, A., Vasserman, G., Edry, L., Dayan, M., Yitzhaky, A., Hertzberg, L., Feinerman, O. &#x26; Kimchi, T. Automated long-term tracking and social behavioural phenotyping of animal colonies within a semi-natural environment. <i>Nature Communications </i>4, 2018, (2013).</li>
    <li id="ul0001-0026" num="0218">26 Spink, A. J., Tegelenbosch, R. A., Buma, M. O. &#x26; Noldus, L. P. The EthoVision video tracking system&#x2014;a tool for behavioral phenotyping of transgenic mice. <i>Physiology </i>&#x26;<i>amp; behavior </i>73, 731-744, (2001).</li>
    <li id="ul0001-0027" num="0219">27 Tort, A. B. L., Neto, W. P., Amaral, O. B., Kazlauckas, V., Souza, D. O. &#x26; Lara, D. R. A simple webcam-based approach for the measurement of rodent locomotion and other behavioural parameters. <i>Journal of neuroscience methods </i>157, 91-97, (2006).</li>
    <li id="ul0001-0028" num="0220">28 Gomez-Marin, A., Partoune, N., Stephens, G. J., Louis, M. &#x26; Brembs, B. Automated tracking of animal posture and movement during exploration and sensory orientation behaviors. <i>PLoS ONE </i>7, e41642, (2012).</li>
    <li id="ul0001-0029" num="0221">29 Colgan, P. W. <i>Quantitative ethology</i>. (John Wiley &#x26;amp; Sons, 1978).</li>
    <li id="ul0001-0030" num="0222">30 Fox, E. B., Sudderth, E. B., Jordan, M. I. &#x26; Willsky, A. S. in Proc. <i>International Conference on Machine Learning </i>(2008).</li>
    <li id="ul0001-0031" num="0223">31 Fox, E. B., Sudderth, E. B., Jordan, M. I. &#x26; Willsky, A. S. Bayesian Nonparametric Inference of Switching Dynamic Linear Models. <i>IEEE Transactions on Signal Processing </i>59, (2011).</li>
    <li id="ul0001-0032" num="0224">32 Johnson, M. J. &#x26; Willsky, A. S. The Hierarchical Dirichlet Process Hidden Semi-Markov Model. <i>Arxiv </i>abs/1203.3485, (2012).</li>
    <li id="ul0001-0033" num="0225">33 Teh, Y. W., Jordan, M. I. &#x26; Beal, M. J. Hierarchical dirichlet processes. <i>Journal of the american </i>. . . , (2006).</li>
    <li id="ul0001-0034" num="0226">34 Geman, S. &#x26; Geman, D. Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. <i>IEEE Trans. Pattern Anal. Mach. Intell. </i>6, 721-741, (1984).</li>
    <li id="ul0001-0035" num="0227">35 Wallace, K. J. &#x26; Rosen, J. B. Predator odor as an unconditioned fear stimulus in rats: elicitation of freezing by trimethylthiazoline, a component of fox feces. <i>Behav Neurosci </i>114, 912-922, (2000).</li>
    <li id="ul0001-0036" num="0228">36 Fendt, M., Endres, T., Lowry, C. A., Apfelbach, R. &#x26; McGregor, I. S. TMT-induced autonomic and behavioral changes and the neural basis of its processing. <i>Neurosci Biobehav Rev </i>29, 1145-1156, (2005).</li>
    <li id="ul0001-0037" num="0229">37 Andr&#xe9;, E., Conquet, F., Steinmayr, M., Stratton, S. C., Porciatti, V. &#x26; Becker-Andr&#xe9;, M. Disruption of retinoid-related orphan receptor beta changes circadian behavior, causes retinal degeneration and leads to vacillans phenotype in mice. <i>The EMBO journal </i>17, 3867-3877, (1998).</li>
    <li id="ul0001-0038" num="0230">38 Liu, H., Kim, S.-Y., Fu, Y., Wu, X., Ng, L., Swaroop, A. &#x26; Forrest, D. An isoform of retinoid-related orphan receptor &#x3b2; directs differentiation of retinal amacrine and horizontal interneurons. <i>Nature Communications </i>4, 1813, (2013).</li>
    <li id="ul0001-0039" num="0231">39 Eppig, J. T., Blake, J. A., Bult, C. J., Kadin, J. A., Richardson, J. E. &#x26; Group, M. G. D. The Mouse Genome Database (MGD): facilitating mouse as a model for human biology and disease. <i>Nucleic Acids Research </i>43, D726-736, (2015).</li>
    <li id="ul0001-0040" num="0232">40 Masana, M. I., Sumaya, I. C., Becker-Andre, M. &#x26; Dubocovich, M. L. Behavioral characterization and modulation of circadian rhythms by light and melatonin in C3H/HeN mice homozygous for the RORbeta knockout. <i>American journal of physiology. Regulatory, integrative and comparative physiology </i>292, R2357-2367, (2007).</li>
    <li id="ul0001-0041" num="0233">41 Glickfeld, L. L., Andermann, M. L., Bonin, V. &#x26; Reid, R. C. Cortico-cortical projections in mouse visual cortex are functionally target specific. <i>Nature Neuroscience </i>16, 219-226, (2013).</li>
    <li id="ul0001-0042" num="0234">42 Mei, Y. &#x26; Zhang, F. Molecular tools and approaches for optogenetics. <i>Biological psychiatry </i>71, 1033-1038, (2012).</li>
    <li id="ul0001-0043" num="0235">43 Lashley, K. S. (ed Lloyd A Jeffress) (Psycholinguistics: A book of readings, 1967).</li>
    <li id="ul0001-0044" num="0236">44 Sherrington, C. The Integrative Action of the Nervous System. <i>The Journal of Nervous and Mental Disease</i>, (1907).</li>
    <li id="ul0001-0045" num="0237">45 Bizzi, E., Tresch, M. C., Saltiel, P. &#x26; d&#x26;apos; Avella, A. New perspectives on spinal motor systems. <i>Nature Reviews Neuroscience </i>1, 101-108, (2000).</li>
    <li id="ul0001-0046" num="0238">46 Drai, D., Benjamini, Y. &#x26; Golani, I. Statistical discrimination of natural modes of motion in rat exploratory behavior. <i>Journal of neuroscience methods </i>96, 119-131, (2000).</li>
    <li id="ul0001-0047" num="0239">47 Brown, T. G. in <i>Proceedings of the Royal Society of London Series B </i>(1911).</li>
    <li id="ul0001-0048" num="0240">48 Crawley, J. N. Behavioral phenotyping of rodents. <i>Comparative medicine </i>53, 140-146, (2003).</li>
    <li id="ul0001-0049" num="0241">49 Anderson, D. J. &#x26; Perona, P. Toward a science of computational ethology. <i>Neuron </i>84, 18-31, (2014).</li>
    <li id="ul0001-0050" num="0242">50 Berg, H. C. &#x26; Brown, D. A. Chemotaxis in Escherichia coli analysed by three-dimensional tracking. <i>Nature </i>239, 500-504, (1972).</li>
    <li id="ul0001-0051" num="0243">51 Berg, H. C. Chemotaxis in bacteria. <i>Annual review of biophysics and bioengineering </i>4, 119-136, (1975).</li>
    <li id="ul0001-0052" num="0244">52 Berg, H. C. Bacterial behaviour. <i>Nature </i>254, 389-392, (1975).</li>
    <li id="ul0001-0053" num="0245">53 Hong, W., Kim, D.-W. &#x26; Anderson, D. J. Antagonistic Control of Social versus Repetitive Self-Grooming Behaviors by Separable Amygdala Neuronal Subsets. <i>Cell </i>158, 1348-1361, (2014).</li>
    <li id="ul0001-0054" num="0246">54 Lin, D., Boyle, M. P., Dollar, P., Lee, H., Lein, E. S., Perona, P. &#x26; Anderson, D. J. Functional identification of an aggression locus in the mouse hypothalamus. <i>Nature </i>470, 221-226, (2011).</li>
    <li id="ul0001-0055" num="0247">55 Swanson, L. W. Cerebral hemisphere regulation of motivated behavior. <i>Brain research </i>886, 113-164, (2000).</li>
    <li id="ul0001-0056" num="0248">56 Aldridge, J. W. &#x26; Berridge, K. C. Coding of serial order by neostriatal neurons: a &#x26;quot; natural action&#x26;quot; approach to movement sequence. <i>The Journal of neuroscience: the official journal of the Society for Neuroscience </i>18, 2777-2787, (1998).</li>
    <li id="ul0001-0057" num="0249">57 Aldridge, J. W., Berridge, K. C. &#x26; Rosen, A. R. Basal ganglia neural mechanisms of natural movement sequences. <i>Canadian Journal of Physiology and Pharmacology </i>82, 732-739, (2004).</li>
    <li id="ul0001-0058" num="0250">58 Jin, X., Tecuapetla, F. &#x26; Costa, R. M. Basal ganglia subcircuits distinctively encode the parsing and concatenation of action sequences. <i>Nature Publishing Group </i>17, 423-430, (2014).</li>
    <li id="ul0001-0059" num="0251">59 Tresch, M. C. &#x26; Jarc, A. The case for and against muscle synergies. <i>Current opinion in neurobiology </i>19, 601-607, (2009).</li>
    <li id="ul0001-0060" num="0252">60 Flash, T. &#x26; Hochner, B. Motor primitives in vertebrates and invertebrates. <i>Current opinion in neurobiology </i>15, 660-666, (2005).</li>
    <li id="ul0001-0061" num="0253">61 Bizzi, E., Cheung, V. C. K., d&#x26;apos; Avella, A., Saltiel, P. &#x26; Tresch, M. Combining modules for movement. <i>Brain Research Reviews </i>57, 125-133, (2008).</li>
    <li id="ul0001-0062" num="0254">62 Tresch, M. C., Saltiel, P. &#x26; Bizzi, E. The construction of movement by the spinal cord. <i>Nature Neuroscience </i>2, 162-167, (1999).</li>
    <li id="ul0001-0063" num="0255">63 Berwick, R. C., Okanoya, K., Beckers, G. J. L. &#x26; Bolhuis, J. J. Songs to syntax: the linguistics of birdsong. <i>Trends in cognitive sciences </i>15, 113-121, (2011).</li>
    <li id="ul0001-0064" num="0256">64 Wohlgemuth, M. J., Sober, S. J. &#x26; Brainard, M. S. Linked control of syllable sequence and phonology in birdsong. <i>Journal of Neuroscience </i>30, 12936-12949, (2010).</li>
    <li id="ul0001-0065" num="0257">65 Markowitz, J. E., Ivie, E., Kligler, L. &#x26; Gardner, T. J. Long-range Order in Canary Song. <i>PLoS computational biology </i>9, e1003052, (2013).</li>
    <li id="ul0001-0066" num="0258">66 Fentress, J. C. &#x26; Stilwell, F. P. Letter: Grammar of a movement sequence in inbred mice. <i>Nature </i>244, 52-53, (1973).</li>
</ul>
</p>
<heading id="h-0044" level="1">Selected Embodiments</heading>
<p id="p-0218" num="0259">Although the above description and the attached claims disclose a number of embodiments of the present invention, other alternative aspects of the invention are disclosed in the following further embodiments.</p>
<p id="p-0219" num="0000">Embodiment 1. A method for analyzing the motion of a subject to separate it into modules, the method comprising:
<br/>
processing three dimensional video data frames that represent the motion of the subject using a computational model to partition the frames into at least one set of frames that represent modules and at least one set frames that represent transitions between the modules; and storing, in a memory, the at least one set of frames that represent modules referenced to a data identifier that represents a type of animal behavior;
<br/>
Embodiment 2. The method of embodiment 1, said processing comprises a step of isolating the subject from the background in the video data.
<br/>
Embodiment 3. The method of embodiment 2, said processing further comprises a step of identifying an orientation of a feature of the subject on a set of frames of the video data with respect to a coordinate system common to each frame.
<br/>
Embodiment 4. The method of embodiment 3, said processing further comprises a step of modifying the orientation of the subject in at least a subset of the set of frames so that the feature is oriented in the same direction with respect to the coordinate system to output a set of aligned frames.
<br/>
Embodiment 5. The method of embodiment 4, said processing further comprises a step of processing the aligned frames using a principal component analysis (PCA) to output pose dynamics data, wherein the pose dynamics data represents a pose of the subject for each aligned frame through principal component space.
<br/>
Embodiment 6. The method of embodiment 5, said processing further comprises a step of processing the aligned frames with a computational model to temporally segment the pose dynamics data into separate sets of modules wherein each of the sub-second module in a set of modules exhibits similar pose dynamics.
<br/>
Embodiment 7. The method of embodiment 6, further comprising a step of displaying a representation of each of the sets of modules that occur with a frequency above a threshold in the three dimensional video data.
<br/>
Embodiment 8. The method of embodiment 1, wherein the computational model comprises modeling the sub-second modules as a vector autoregressive process representing a stereotyped trajectory through PCA space.
<br/>
Embodiment 9. The method of embodiment 1, wherein the computational model comprises modeling transition periods between sub-second modules using a Hidden Markov Model.
<br/>
Embodiment 10. The method of embodiment 1, wherein the three dimensional video data is first processed to output a series of points in a multidimensional vector space, wherein each point represents the three dimensional pose dynamics of the subject.
<br/>
Embodiment 11. The method of any one of claims <b>1</b>-<b>10</b>, wherein the subject is an animal in an animal study.
<br/>
Embodiment 12. The method of any one of claims <b>1</b>-<b>10</b>, wherein the subject is a human.
<br/>
Embodiment 13. A method for analyzing the motion of a subject to separate it into modules, the method comprising:
</p>
<p id="p-0220" num="0260">pre-processing three dimensional video data that represents the motion of the subject to isolate the subject from the background;</p>
<p id="p-0221" num="0261">identifying an orientation of a feature of the subject on a set of frames of the video data with respect to a coordinate system common to each frame;</p>
<p id="p-0222" num="0262">modifying the orientation of the subject in at least a subset of the set of frames so that the feature is oriented in the same direction with respect to the coordinate system to output a set of aligned frames;</p>
<p id="p-0223" num="0263">processing the aligned frames using a principal component analysis to output pose dynamics data, wherein the pose dynamics data represents a pose of the subject for each aligned frame through principal component space;</p>
<p id="p-0224" num="0264">processing the aligned frames to temporally segment the pose dynamics data into separate sets of sub-second modules wherein each of the sub-second module in a set of modules exhibits similar pose dynamics; and</p>
<p id="p-0225" num="0265">displaying a representation of each of the sets of modules that occur with a frequency above a threshold in the three dimensional video data.</p>
<p id="p-0226" num="0000">Embodiment 14. The method of embodiment 13, wherein the processing the aligned frames step is performed using a model free algorithm.
<br/>
Embodiment 15. The method of embodiment 14, wherein the model free algorithm comprises computing an auto-correlogram.
<br/>
Embodiment 16. The method of claim <b>13</b>, wherein the processing the aligned frames step is performed using a model based algorithm.
<br/>
Embodiment 17. The method of embodiment 16, wherein the model based algorithm is an AR-HMM algorithm.
<br/>
Embodiment 18. The method of any one of claims <b>13</b>-<b>17</b>, wherein the subject is an animal in an animal study.
<br/>
Embodiment 19. The method of any one of claims <b>13</b>-<b>17</b>, wherein the subject is a human.
<br/>
Embodiment 20. A method of classifying a test compound, the method comprising:
</p>
<p id="p-0227" num="0266">identifying a test behavioral representation that includes a set of modules in a test subject after the test compound is administered to the test subject;</p>
<p id="p-0228" num="0000">comparing the test behavioral representation to a plurality of reference behavioral representations, wherein each reference behavioral representation represents each class of drugs; and
<br/>
determining that the test compound belongs to a class of drugs if the test behavioral representation is identified by a classifier as matching the reference behavioral representation representing said class of drugs.
<br/>
Embodiment 21. The method of embodiment 20, wherein the test behavioral representation is identified by
<br/>
receiving three dimensional video data representing the motion of the test subject;
</p>
<p id="p-0229" num="0267">processing the three dimensional data using a computational model to partition the data into at least one set of modules and at least one set of transition periods between the modules; and</p>
<p id="p-0230" num="0268">assigning the at least one set of modules to a category that represents a type of animal behavior.</p>
<p id="p-0231" num="0000">Embodiment 22. The method of embodiment 21, wherein the computational model comprises modeling the sub-second modules as a vector autoregressive process representing a stereotyped trajectory through principal component analysis (PCA) space.
<br/>
Embodiment 23. The method of embodiment 21, wherein the computational model comprises modeling the transition periods using a Hidden Markov Model.
<br/>
Embodiment 24. The method of any one of claims <b>20</b>-<b>23</b>, wherein the three dimensional video data is first processed to output a series of points in a multidimensional vector space, wherein each point represents the 3D pose dynamics of the test subject.
<br/>
Embodiment 25. The method of any one of claims <b>20</b>-<b>24</b>, wherein the test compound is selected from the group consisting of a small molecule, an antibody or an antigen-binding fragment thereof, a nucleic acid, a polypeptide, a peptide, a peptidomimetic, a polysaccharide, a monosaccharide, a lipid, a glycosaminoglycan, and a combination thereof.
<br/>
Embodiment 26. The method of any one of claims <b>20</b>-<b>25</b>, wherein the test subject is an animal in an animal study.
<br/>
Embodiment 27. A method for analyzing the motion of a subject to separate it into modules, the method comprising:
</p>
<p id="p-0232" num="0269">receiving three dimensional video data representing the motion of the subject before and after administration of an agent to the subject;</p>
<p id="p-0233" num="0270">pre-processing the three dimensional video data to isolate the subject from the background;</p>
<p id="p-0234" num="0271">identifying an orientation of a feature of the subject on a set of frames of the video data with respect to a coordinate system common to each frame;</p>
<p id="p-0235" num="0272">modifying the orientation of the subject in at least a subset of the set of frames so that the feature is oriented in the same direction with respect to the coordinate system to output a set of aligned frames;</p>
<p id="p-0236" num="0273">processing the aligned frames using a principal component analysis to output pose dynamics data, wherein the pose dynamics data represents a pose of the subject for each aligned frame through principal component space;</p>
<p id="p-0237" num="0274">processing the aligned frames with a computational model to temporally segment the pose dynamics data into separate sets of sub-second modules wherein each of the sub-second module in a set of sub-second modules exhibits similar pose dynamics;</p>
<p id="p-0238" num="0275">determining the quantity of modules in each set of sub-second modules before administration of the agent to the subject;</p>
<p id="p-0239" num="0276">determining the quantity of modules in each set of sub-second modules after administration of the agent to the subject;</p>
<p id="p-0240" num="0277">comparing the quantity of modules in each set of sub-second modules before and after administration of the agent to the subject; and</p>
<p id="p-0241" num="0278">outputting an indication of the change in frequency of expression of the quantity of modules in each set of modules before and after administration of the agent to the subject.</p>
<p id="p-0242" num="0000">Embodiment 28. The method of embodiment 27, wherein each set of sub-second modules is classified into a predetermined behavior module based on comparison to reference data representing behavior modules.
<br/>
Embodiment 29. The method of embodiment 27 or 28, wherein the change in frequency of expression of the quantity of modules in each set of modules before and after administration of the agent to the subject is compared to the reference data representing a change in frequency of expression of modules after exposure to known categories of agents.
<br/>
Embodiment 30. The method of embodiment 29, comprising the further step of classifying the agent as one of the plurality of known categories of agents based on the comparison to reference data representing the change in frequency after exposure to known categories of agents.
<br/>
Embodiment 31. The method of any one of claims <b>27</b>-<b>30</b>, wherein the agent is a pharmaceutically active compound.
<br/>
Embodiment 32. The method of any one of claims <b>27</b>-<b>30</b>, wherein the agent is visual or auditory stimulus.
<br/>
Embodiment 33. The method of any one of claims <b>27</b>-<b>30</b>, wherein the agent is an odorant.
<br/>
Embodiment 34. The method of any one of claims <b>27</b>-<b>33</b>, wherein the subject is an animal in an animal study.
<br/>
Embodiment 35. The method of any one of claims <b>27</b>-<b>33</b>, wherein the subject is a human.
<br/>
Embodiment 36. A system for recording motion of a subject with a three dimensional video camera and parsing three dimensional video data output from the three dimensional video camera into sets of frames the represent different behaviors, the system comprising:
<br/>
a three dimensional video camera configured to output video data representing the motion of a subject;
</p>
<p id="p-0243" num="0279">a memory in communication with the three dimensional video camera containing machine readable medium comprising machine executable code having stored thereon;</p>
<p id="p-0244" num="0000">a control system comprising one or more processors coupled to the memory, the control system configured to execute the machine executable code to cause the control system to:</p>
<p id="p-0245" num="0280">pre-process, using the control system, the video data to isolate the subject from the background;</p>
<p id="p-0246" num="0281">identify, using the control system, an orientation of a feature of the subject on a set of frames of the video data with respect to a coordinate system common to each frame;</p>
<p id="p-0247" num="0282">modify, using the control system, the orientation of the subject in at least a subset of the set of frames so that the feature is oriented in the same direction with respect to the coordinate system to output a set of aligned frames;</p>
<p id="p-0248" num="0283">process, using the control system, the set of aligned frames using a principal component analysis to output pose dynamics data for each frame of the set of aligned frames, wherein the pose dynamics data represents a pose of the subject for each aligned frame through principal component space;</p>
<p id="p-0249" num="0284">process, using the control system, the set of aligned frames to temporally segment the set of aligned frames into separate sets of sub-second modules wherein each set of sub-second modules includes only sub-second modules with similar pose dynamics; and</p>
<p id="p-0250" num="0285">store, in a database, each frame in the set of aligned frames referenced to its sub-second module.</p>
<p id="p-0251" num="0000">Embodiment 37. The system of embodiment 36, wherein the control system is further configured to: send, to a display, a representation of a sub-set of the sets of sub-second modules that occur above a threshold in the separate sets of sub-second modules.
<br/>
Embodiment 38. The system of embodiment 36, wherein the control system receives user input regarding a behavior tag for each of the sub-set of the sets of sub-second modules that occur above a threshold.
</p>
<heading id="h-0045" level="1">CONCLUSION</heading>
<p id="p-0252" num="0286">The various methods and techniques described above provide a number of ways to carry out the invention. Of course, it is to be understood that not necessarily all objectives or advantages described can be achieved in accordance with any particular embodiment described herein. Thus, for example, those skilled in the art will recognize that the methods can be performed in a manner that achieves or optimizes one advantage or group of advantages as taught herein without necessarily achieving other objectives or advantages as taught or suggested herein. A variety of alternatives are mentioned herein. It is to be understood that some embodiments specifically include one, another, or several features, while others specifically exclude one, another, or several features, while still others mitigate a particular feature by inclusion of one, another, or several advantageous features.</p>
<p id="p-0253" num="0287">Furthermore, the skilled artisan will recognize the applicability of various features from different embodiments. Similarly, the various elements, features and steps discussed above, as well as other known equivalents for each such element, feature or step, can be employed in various combinations by one of ordinary skill in this art to perform methods in accordance with the principles described herein. Among the various elements, features, and steps some will be specifically included and others specifically excluded in diverse embodiments.</p>
<p id="p-0254" num="0288">Although the application has been disclosed in the context of certain embodiments and examples, it will be understood by those skilled in the art that the embodiments of the application extend beyond the specifically disclosed embodiments to other alternative embodiments and/or uses and modifications and equivalents thereof.</p>
<p id="p-0255" num="0289">In some embodiments, the terms &#x201c;a&#x201d; and &#x201c;an&#x201d; and &#x201c;the&#x201d; and similar references used in the context of describing a particular embodiment of the application (especially in the context of certain of the following claims) can be construed to cover both the singular and the plural. The recitation of ranges of values herein is merely intended to serve as a shorthand method of referring individually to each separate value falling within the range. Unless otherwise indicated herein, each individual value is incorporated into the specification as if it were individually recited herein. All methods described herein can be performed in any suitable order unless otherwise indicated herein or otherwise clearly contradicted by context. The use of any and all examples, or exemplary language (for example, &#x201c;such as&#x201d;) provided with respect to certain embodiments herein is intended merely to better illuminate the application and does not pose a limitation on the scope of the application otherwise claimed. No language in the specification should be construed as indicating any non-claimed element essential to the practice of the application.</p>
<p id="p-0256" num="0290">Certain embodiments of this application are described herein. Variations on those embodiments will become apparent to those of ordinary skill in the art upon reading the foregoing description. It is contemplated that skilled artisans can employ such variations as appropriate, and the application can be practiced otherwise than specifically described herein. Accordingly, many embodiments of this application include all modifications and equivalents of the subject matter recited in the claims appended hereto as permitted by applicable law. Moreover, any combination of the above-described elements in all possible variations thereof is encompassed by the application unless otherwise indicated herein or otherwise clearly contradicted by context.</p>
<p id="p-0257" num="0291">Particular implementations of the subject matter have been described. Other implementations are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results.</p>
<p id="p-0258" num="0292">All patents, patent applications, publications of patent applications, and other material, such as articles, books, specifications, publications, documents, things, and/or the like, referenced herein are hereby incorporated herein by this reference in their entirety for all purposes, excepting any prosecution file history associated with same, any of same that is inconsistent with or in conflict with the present document, or any of same that may have a limiting affect as to the broadest scope of the claims now or later associated with the present document. By way of example, should there be any inconsistency or conflict between the description, definition, and/or the use of a term associated with any of the incorporated material and that associated with the present document, the description, definition, and/or the use of the term in the present document shall prevail.</p>
<p id="p-0259" num="0293">In closing, it is to be understood that the embodiments of the application disclosed herein are illustrative of the principles of the embodiments of the application. Other modifications that can be employed can be within the scope of the application. Thus, by way of example, but not of limitation, alternative configurations of the embodiments of the application can be utilized in accordance with the teachings herein. Accordingly, embodiments of the present application are not limited to that precisely as shown and described.</p>
<?detailed-description description="Detailed Description" end="tail"?>
</description>
<us-math idrefs="MATH-US-00001" nb-file="US20230225636A1-20230720-M00001.NB">
<img id="EMI-M00001" he="11.60mm" wi="76.20mm" file="US20230225636A1-20230720-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00002" nb-file="US20230225636A1-20230720-M00002.NB">
<img id="EMI-M00002" he="8.81mm" wi="76.20mm" file="US20230225636A1-20230720-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00003" nb-file="US20230225636A1-20230720-M00003.NB">
<img id="EMI-M00003" he="3.89mm" wi="76.20mm" file="US20230225636A1-20230720-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00004" nb-file="US20230225636A1-20230720-M00004.NB">
<img id="EMI-M00004" he="4.23mm" wi="76.20mm" file="US20230225636A1-20230720-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00005" nb-file="US20230225636A1-20230720-M00005.NB">
<img id="EMI-M00005" he="12.02mm" wi="76.20mm" file="US20230225636A1-20230720-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00006" nb-file="US20230225636A1-20230720-M00006.NB">
<img id="EMI-M00006" he="31.75mm" wi="76.20mm" file="US20230225636A1-20230720-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00007" nb-file="US20230225636A1-20230720-M00007.NB">
<img id="EMI-M00007" he="10.24mm" wi="76.20mm" file="US20230225636A1-20230720-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00008" nb-file="US20230225636A1-20230720-M00008.NB">
<img id="EMI-M00008" he="16.26mm" wi="76.20mm" file="US20230225636A1-20230720-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<us-math idrefs="MATH-US-00009" nb-file="US20230225636A1-20230720-M00009.NB">
<img id="EMI-M00009" he="4.91mm" wi="76.20mm" file="US20230225636A1-20230720-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/>
</us-math>
<claims id="claims">
<claim id="CLM-00001" num="00001">
<claim-text><b>1</b>. A method for analyzing motion of a subject, the method comprising:
<claim-text>processing three-dimensional video image data frames representing the motion of the subject using a model-free algorithm to output a first and a second set of modules, wherein each module in the first set of modules exhibits pose dynamics for 200-900 milliseconds, and each module in the second set of modules exhibits pose dynamics for 200-900 milliseconds; and</claim-text>
<claim-text>identifying repeated modules of the subject's behavior using a template matching procedure for matching motifs of the subject's motion.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00002" num="00002">
<claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each module in the first set of modules and the second set of modules exhibits pose dynamics satisfying a predetermined similarity threshold.</claim-text>
</claim>
<claim id="CLM-00003" num="00003">
<claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the processing further comprises isolating the subject from a background in each frame of the three-dimensional video image data.</claim-text>
</claim>
<claim id="CLM-00004" num="00004">
<claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the processing further comprises identifying an orientation of a feature of the subject on a set of frames of the three-dimensional video image data with respect to a coordinate system common to each frame.</claim-text>
</claim>
<claim id="CLM-00005" num="00005">
<claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the processing further comprises modifying the orientation of the subject in at least a subset of the set of frames so that the feature is oriented in a same direction with respect to the coordinate system to output a set of aligned frames.</claim-text>
</claim>
<claim id="CLM-00006" num="00006">
<claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the processing further comprises reducing dimensionality of the three-dimensional video image data frames using a principal component analysis (PCA) to output pose dynamics data representing a pose of the subject through principal component space.</claim-text>
</claim>
<claim id="CLM-00007" num="00007">
<claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the processing further comprises reducing dimensionality of the three-dimensional video image data frames using a random projections technique.</claim-text>
</claim>
<claim id="CLM-00008" num="00008">
<claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing further comprises identifying a time period of a behavior module of the subject by computing an auto-correlogram to evaluate timescale over which the subject's behavior is self-repeating.</claim-text>
</claim>
<claim id="CLM-00009" num="00009">
<claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing further comprises performing a power-spectral density (PSD) analysis on the subject's behavioral data to further analyze its time domain structure.</claim-text>
</claim>
<claim id="CLM-00010" num="00010">
<claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processing further comprises automatically locating changepoints for transition periods between modules using a filtered derivative algorithm.</claim-text>
</claim>
<claim id="CLM-00011" num="00011">
<claim-text><b>11</b>. A non-transitory computer-readable medium storing processor-executable instructions that, when executed by at least one processor, cause the at least one processor to perform a method for analyzing motion of a subject, the method comprising:
<claim-text>processing three-dimensional video image data frames representing the motion of the subject using a model-free algorithm to output a first and a second set of modules, wherein each module in the first set of modules exhibits pose dynamics for 200-900 milliseconds, and each module in the second set of modules exhibits pose dynamics for 200-900 milliseconds; and</claim-text>
<claim-text>identifying repeated modules of the subject's behavior using a template matching procedure for matching motifs of the subject's motion.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00012" num="00012">
<claim-text><b>12</b>. A system for recording motion of a subject and parsing the recorded image data into sets of frames the represent different behaviors of the subject, the system comprising:
<claim-text>a three-dimensional video camera configured to output video image data representing the motion of the subject;</claim-text>
<claim-text>a memory in communication with the three-dimensional video camera and having a non-transitory machine-readable storage medium with a machine-executable instruction set stored thereon; and</claim-text>
<claim-text>a control system comprising one or more processors coupled to the memory, the one or more processors configured to execute the machine-executable instruction set to cause the control system to:</claim-text>
<claim-text>process frames obtained using the video image data, the frames representing the motion of the subject, using a model-free algorithm to output a first and a second set of modules, wherein each module in the first set of modules exhibits pose dynamics for 200-900 milliseconds, and each module in the second set of modules exhibits pose dynamics for 200-900 milliseconds; and</claim-text>
<claim-text>identify repeated modules of the subject's behavior using a template matching procedure for matching motifs of the subject's motion.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00013" num="00013">
<claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the control system is further configured to:
<claim-text>pre-process the video image data to isolate the subject from the background; and</claim-text>
<claim-text>identify an orientation of a feature of the subject on a set of frames of the video image data with respect to a coordinate system common to each frame.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00014" num="00014">
<claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the control system is further configured to modify the orientation of the subject in at least a subset of the set of frames so that the feature is oriented in the same direction with respect to the coordinate system to output a set of aligned frames.</claim-text>
</claim>
<claim id="CLM-00015" num="00015">
<claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the control system is further configured to:
<claim-text>process the set of aligned frames using a principal component analysis to output pose dynamics data for each frame of the set of aligned frames, wherein the pose dynamics data represents a pose of the subject for each aligned frame through principal component space; and</claim-text>
<claim-text>process, using the control system, the set of aligned frames to temporally segment the pose dynamics data into a plurality of sets of modules, wherein each module within a set of modules exhibits similar pose dynamics satisfying a predetermined similarity threshold and comprises 200-900 milliseconds.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00016" num="00016">
<claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the control system is further configured to store, in a database, each frame in the set of aligned frames referenced to its module.</claim-text>
</claim>
<claim id="CLM-00017" num="00017">
<claim-text><b>17</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the control system is further configured to send, to a display, a representation of a sub-set of the plurality of sets of modules that occur above the predetermined similarity threshold.</claim-text>
</claim>
<claim id="CLM-00018" num="00018">
<claim-text><b>18</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the control system is further configured to automatically apply the behavior tag to video image data outputted from the three-dimensional video camera representing motion of a second subject.</claim-text>
</claim>
<claim id="CLM-00019" num="00019">
<claim-text><b>19</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the video image data represents motion of the subject before and after administration of an agent to the subject, and wherein the control system is further configured to:
<claim-text>determine a quantity of modules in the first and second sets of modules before administration of the agent to the subject;</claim-text>
<claim-text>determine a quantity of modules in the first and second sets of modules after administration of the agent to the subject; and</claim-text>
<claim-text>output an indication of a change in frequency of expression of the quantity of modules in each of the first and second sets of modules before and after administration of the agent to the subject.</claim-text>
</claim-text>
</claim>
<claim id="CLM-00020" num="00020">
<claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the agent is (i) a pharmaceutically-active compound, (ii) a visual stimulus, (iii) an auditory stimulus, or (iv) an odorant.</claim-text>
</claim>
</claims>
</us-patent-application>
